# -*- coding: utf-8 -*-
"""
Optunaè¶…å‚æ•°ä¼˜åŒ–å™¨
è‡ªåŠ¨æœç´¢æœ€ä¼˜æ¨¡å‹è¶…å‚æ•°
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, Callable, List
import logging
import warnings

# Optuna imports
try:
    import optuna
    from optuna.samplers import TPESampler, RandomSampler, CmaEsSampler
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    warnings.warn("Optuna not installed. Install with: pip install optuna")

# Model imports
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score
import xgboost as xgb
import lightgbm as lgb

logger = logging.getLogger(__name__)


class OptunaOptimizer:
    """
    Optunaè¶…å‚æ•°ä¼˜åŒ–å™¨
    
    æ”¯æŒXGBoostå’ŒLightGBMçš„åˆ†ç±»/å›å½’ä»»åŠ¡
    ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ç¡®ä¿æ— æ•°æ®æ³„æ¼
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Parameters
        ----------
        config : dict, optional
            é…ç½®å‚æ•°:
            - n_trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•° (default: 100)
            - timeout: è¶…æ—¶æ—¶é—´ç§’æ•° (default: 3600)
            - sampler: é‡‡æ ·å™¨ç±»å‹ ('tpe'/'random'/'cmaes', default: 'tpe')
            - cv_folds: äº¤å‰éªŒè¯æŠ˜æ•° (default: 5)
            - n_jobs: å¹¶è¡Œä»»åŠ¡æ•° (default: 1)
            - direction: ä¼˜åŒ–æ–¹å‘ ('maximize'/'minimize', default: 'maximize')
        """
        if not OPTUNA_AVAILABLE:
            raise ImportError("Optuna is required for optimization. Install with: pip install optuna")
        
        config = config or {}
        self.n_trials = config.get('n_trials', 100)
        self.timeout = config.get('timeout', 3600)
        self.sampler_type = config.get('sampler', 'tpe')
        self.cv_folds = config.get('cv_folds', 5)
        self.n_jobs = config.get('n_jobs', 1)
        self.direction = config.get('direction', 'maximize')
        
        # åˆ›å»ºé‡‡æ ·å™¨
        if self.sampler_type == 'tpe':
            self.sampler = TPESampler(seed=42)
        elif self.sampler_type == 'random':
            self.sampler = RandomSampler(seed=42)
        elif self.sampler_type == 'cmaes':
            self.sampler = CmaEsSampler(seed=42)
        else:
            raise ValueError(f"æœªçŸ¥é‡‡æ ·å™¨: {self.sampler_type}")
        
        # åˆ›å»ºpruner (æå‰ç»ˆæ­¢è¡¨ç°å·®çš„è¯•éªŒ)
        self.pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=5)
        
        logger.info(f"Optunaä¼˜åŒ–å™¨åˆå§‹åŒ–: {self.n_trials} trials, {self.sampler_type} sampler")
    
    def _analyze_data_complexity(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        åˆ†ææ•°æ®å¤æ‚åº¦ï¼Œä¸ºå‚æ•°è°ƒæ•´æä¾›ä¾æ®
        
        Parameters
        ----------
        X : DataFrame
            ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
            
        Returns
        -------
        complexity_metrics : dict
            æ•°æ®å¤æ‚åº¦æŒ‡æ ‡:
            - n_samples: æ ·æœ¬æ•°é‡
            - n_features: ç‰¹å¾æ•°é‡
            - feature_density: ç‰¹å¾å¯†åº¦ (éé›¶ç‰¹å¾æ¯”ä¾‹)
            - target_variance: ç›®æ ‡å˜é‡æ–¹å·®
            - complexity_score: ç»¼åˆå¤æ‚åº¦è¯„åˆ†
        """
        n_samples, n_features = X.shape
        
        # è®¡ç®—ç‰¹å¾å¯†åº¦ (éé›¶ç‰¹å¾æ¯”ä¾‹)
        if hasattr(X, 'sparse'):
            feature_density = (X != 0).sum().sum() / (n_samples * n_features)
        else:
            feature_density = 1.0  # ç¨ å¯†çŸ©é˜µ
        
        # ç›®æ ‡å˜é‡æ–¹å·®
        target_variance = y.var() if len(y) > 1 else 0.0
        
        # ç»¼åˆå¤æ‚åº¦è¯„åˆ† (0-1èŒƒå›´)
        complexity_score = min(1.0, 
            (n_samples / 10000) * 0.3 +  # æ ·æœ¬è§„æ¨¡å½±å“
            (n_features / 100) * 0.3 +   # ç‰¹å¾è§„æ¨¡å½±å“
            feature_density * 0.2 +       # ç‰¹å¾å¯†åº¦å½±å“
            (target_variance / 10) * 0.2  # ç›®æ ‡å¤æ‚åº¦å½±å“
        )
        
        metrics = {
            'n_samples': n_samples,
            'n_features': n_features,
            'feature_density': feature_density,
            'target_variance': target_variance,
            'complexity_score': complexity_score
        }
        
        logger.info(f"ğŸ“Š æ•°æ®å¤æ‚åº¦åˆ†æ: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, "
                   f"å¯†åº¦{feature_density:.3f}, å¤æ‚åº¦è¯„åˆ†{complexity_score:.3f}")
        
        return metrics
    
    def _adjust_parameter_ranges(self, base_params: Dict[str, Any], 
                                complexity_metrics: Dict[str, float],
                                model_type: str) -> Dict[str, Any]:
        """
        åŸºäºæ•°æ®å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å‚æ•°æœç´¢èŒƒå›´
        
        Parameters
        ----------
        base_params : dict
            åŸºç¡€å‚æ•°æœç´¢èŒƒå›´
        complexity_metrics : dict
            æ•°æ®å¤æ‚åº¦æŒ‡æ ‡
        model_type : str
            æ¨¡å‹ç±»å‹ ('xgboost_classification', 'xgboost_regression', 'lightgbm_classification')
            
        Returns
        -------
        adjusted_params : dict
            è°ƒæ•´åçš„å‚æ•°æœç´¢èŒƒå›´
        """
        complexity_score = complexity_metrics['complexity_score']
        n_samples = complexity_metrics['n_samples']
        n_features = complexity_metrics['n_features']
        
        adjusted_params = base_params.copy()
        
        # åŸºäºæ ·æœ¬é‡è°ƒæ•´å‚æ•°
        if n_samples < 1000:
            # å°æ ·æœ¬æ•°æ®ï¼šå‡å°‘æ¨¡å‹å¤æ‚åº¦
            if 'n_estimators' in adjusted_params:
                adjusted_params['n_estimators'] = (50, 300)  # å‡å°‘æ ‘çš„æ•°é‡
            if 'max_depth' in adjusted_params:
                adjusted_params['max_depth'] = (2, 6)  # é™ä½æ ‘æ·±åº¦
            if 'learning_rate' in adjusted_params:
                adjusted_params['learning_rate'] = (0.01, 0.3)  # æé«˜å­¦ä¹ ç‡
        elif n_samples > 10000:
            # å¤§æ ·æœ¬æ•°æ®ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦
            if 'n_estimators' in adjusted_params:
                adjusted_params['n_estimators'] = (200, 1000)  # å¢åŠ æ ‘çš„æ•°é‡
            if 'max_depth' in adjusted_params:
                adjusted_params['max_depth'] = (5, 12)  # å¢åŠ æ ‘æ·±åº¦
            if 'learning_rate' in adjusted_params:
                adjusted_params['learning_rate'] = (0.001, 0.2)  # é™ä½å­¦ä¹ ç‡
        
        # åŸºäºç‰¹å¾æ•°é‡è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°
        if n_features >= 50:
            # é«˜ç»´ç‰¹å¾ï¼šåŠ å¼ºæ­£åˆ™åŒ–
            if 'reg_alpha' in adjusted_params:
                adjusted_params['reg_alpha'] = (0.1, 20.0)  # å¢åŠ L1æ­£åˆ™åŒ–
            if 'reg_lambda' in adjusted_params:
                adjusted_params['reg_lambda'] = (0.1, 20.0)  # å¢åŠ L2æ­£åˆ™åŒ–
            if 'colsample_bytree' in adjusted_params:
                adjusted_params['colsample_bytree'] = (0.3, 0.8)  # é™ä½ç‰¹å¾é‡‡æ ·ç‡
        
        # åŸºäºå¤æ‚åº¦è¯„åˆ†å¾®è°ƒ
        if complexity_score > 0.7:
            # é«˜å¤æ‚åº¦æ•°æ®ï¼šå¢åŠ æ­£åˆ™åŒ–
            if 'gamma' in adjusted_params:
                adjusted_params['gamma'] = (1.0, 20.0)  # å¢åŠ åˆ†è£‚é˜ˆå€¼
            if 'min_child_weight' in adjusted_params:
                adjusted_params['min_child_weight'] = (5, 30)  # å¢åŠ å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬
        
        logger.info(f"ğŸ”§ åŸºäºæ•°æ®å¤æ‚åº¦è°ƒæ•´å‚æ•°èŒƒå›´: {model_type}")
        logger.info(f"   æ ·æœ¬é‡: {n_samples}, ç‰¹å¾æ•°: {n_features}, å¤æ‚åº¦: {complexity_score:.3f}")
        
        return adjusted_params
    
    def optimize_xgboost_classification(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        dates: Optional[pd.Series] = None,
        metric: str = 'auc'
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–XGBooståˆ†ç±»å™¨è¶…å‚æ•°
        
        Parameters
        ----------
        X : DataFrame
            ç‰¹å¾æ•°æ®
        y : Series
            åˆ†ç±»æ ‡ç­¾
        dates : Series, optional
            æ—¥æœŸåºåˆ—ï¼ˆç”¨äºæ—¶é—´åºåˆ—åˆ‡åˆ†ï¼‰
        metric : str, default='auc'
            ä¼˜åŒ–æŒ‡æ ‡ ('auc', 'f1')
        
        Returns
        -------
        result : dict
            åŒ…å« best_params, best_score, n_trials, study
        """
        logger.info("="*80)
        logger.info("å¼€å§‹XGBooståˆ†ç±»å™¨è¶…å‚æ•°ä¼˜åŒ–")
        logger.info("="*80)
        
        # åˆ†ææ•°æ®å¤æ‚åº¦
        complexity_metrics = self._analyze_data_complexity(X, y)
        
        # åŸºç¡€å‚æ•°æœç´¢èŒƒå›´
        base_params = {
            'n_estimators': (100, 800),  # ğŸ”§ æ‰©å±•åˆ°100-800
            'max_depth': (3, 5),
            'learning_rate': (0.01, 0.2),
            'subsample': (0.5, 0.75),
            'colsample_bytree': (0.5, 0.8),
            'min_child_weight': (5, 40),
            'gamma': (0.5, 10.0),
            'reg_alpha': (2, 20.0),
            'reg_lambda': (2, 20.0),
        }
        
        # åŸºäºæ•°æ®å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å‚æ•°èŒƒå›´
        adjusted_params = self._adjust_parameter_ranges(base_params, complexity_metrics, 'xgboost_classification')
        
        def objective(trial):
            # ä½¿ç”¨è°ƒæ•´åçš„å‚æ•°èŒƒå›´ - ğŸ”§ ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–: æ•°æ®é©±åŠ¨å‚æ•°è°ƒæ•´
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *adjusted_params['n_estimators']),
                'max_depth': trial.suggest_int('max_depth', *adjusted_params['max_depth']),
                'learning_rate': trial.suggest_float('learning_rate', *adjusted_params['learning_rate'], log=True),
                'subsample': trial.suggest_float('subsample', *adjusted_params['subsample']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *adjusted_params['colsample_bytree']),
                'min_child_weight': trial.suggest_int('min_child_weight', *adjusted_params['min_child_weight']),
                'gamma': trial.suggest_float('gamma', *adjusted_params['gamma']),
                'reg_alpha': trial.suggest_float('reg_alpha', *adjusted_params['reg_alpha']),
                'reg_lambda': trial.suggest_float('reg_lambda', *adjusted_params['reg_lambda']),
                'random_state': 42,
                'verbosity': 0,
                'n_jobs': 1  # é¿å…åµŒå¥—å¹¶è¡Œ
            }
            
            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            tscv = TimeSeriesSplit(n_splits=self.cv_folds)
            cv_scores = []
            
            # å¤„ç†numpyæ•°ç»„å’Œpandas DataFrame
            if hasattr(X, 'iloc'):  # pandas DataFrame
                for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            else:  # numpyæ•°ç»„
                for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
                    X_train, X_val = X[train_idx], X[val_idx]
                    y_train, y_val = y[train_idx], y[val_idx]
                
                # è®­ç»ƒæ¨¡å‹
                model = xgb.XGBClassifier(**params)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )
                
                # è¯„ä¼°
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                if metric == 'auc':
                    score = roc_auc_score(y_val, y_pred_proba)
                else:
                    from sklearn.metrics import f1_score
                    y_pred = (y_pred_proba >= 0.5).astype(int)
                    score = f1_score(y_val, y_pred)
                
                cv_scores.append(score)
                
                # æŠ¥å‘Šä¸­é—´ç»“æœï¼ˆç”¨äºpruningï¼‰
                trial.report(score, fold_idx)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥æå‰ç»ˆæ­¢
                if trial.should_prune():
                    raise optuna.TrialPruned()
            
            return np.mean(cv_scores)
        
        # åˆ›å»ºstudyå¹¶ä¼˜åŒ–
        study = optuna.create_study(
            direction=self.direction,
            sampler=self.sampler,
            pruner=self.pruner
        )
        
        study.optimize(
            objective,
            n_trials=self.n_trials,
            timeout=self.timeout,
            n_jobs=self.n_jobs,
            show_progress_bar=True
        )
        
        logger.info(f"âœ… XGBooståˆ†ç±»ä¼˜åŒ–å®Œæˆ:")
        logger.info(f"  æœ€ä¼˜{metric.upper()}: {study.best_value:.4f}")
        logger.info(f"  æœ€ä¼˜å‚æ•°:")
        for key, value in study.best_params.items():
            logger.info(f"    {key}: {value}")
        logger.info(f"  æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        logger.info(f"  å‰ªæè¯•éªŒæ•°: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
        
        return {
            'best_params': study.best_params,
            'best_score': study.best_value,
            'n_trials': len(study.trials),
            'n_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
            'study': study
        }
    
    def optimize_xgboost_regression(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        dates: Optional[pd.Series] = None,
        metric: str = 'r2'
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–XGBoostå›å½’å™¨è¶…å‚æ•°
        
        Parameters
        ----------
        X : DataFrame
            ç‰¹å¾æ•°æ®
        y : Series
            å›å½’ç›®æ ‡
        dates : Series, optional
            æ—¥æœŸåºåˆ—
        metric : str, default='r2'
            ä¼˜åŒ–æŒ‡æ ‡ ('r2', 'mse', 'mae')
        
        Returns
        -------
        result : dict
            åŒ…å« best_params, best_score, n_trials, study
        """
        logger.info("="*80)
        logger.info("å¼€å§‹XGBoostå›å½’å™¨è¶…å‚æ•°ä¼˜åŒ–")
        logger.info("="*80)
        
        # åˆ†ææ•°æ®å¤æ‚åº¦
        complexity_metrics = self._analyze_data_complexity(X, y)
        
        # åŸºç¡€å‚æ•°æœç´¢èŒƒå›´
        base_params = {
            'n_estimators': (100, 800),  # ğŸ”§ æ‰©å±•åˆ°100-800
            'max_depth': (3, 8),  # ğŸ”§ ä»3-4æ‰©å±•åˆ°3-8
            'learning_rate': (0.005, 0.5),  # ğŸ”§ æ‰©å±•åˆ°0.005-0.5
            'subsample': (0.5, 0.9),  # ğŸ”§ ä»0.6-0.8æ‰©å±•åˆ°0.5-0.9
            'colsample_bytree': (0.5, 0.9),  # ğŸ”§ ä»0.6-0.8æ‰©å±•åˆ°0.5-0.9
            'min_child_weight': (1, 20),  # ğŸ”§ æ‰©å±•åˆ°1-20
            'gamma': (0.0, 15.0),  # ğŸ”§ æ‰©å±•åˆ°0-15
            'reg_alpha': (0.0, 15.0),  # ğŸ”§ ä»3-10æ‰©å±•åˆ°0-15
            'reg_lambda': (0.0, 15.0),  # ğŸ”§ ä»5-10æ‰©å±•åˆ°0-15
        }
        
        # åŸºäºæ•°æ®å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å‚æ•°èŒƒå›´
        adjusted_params = self._adjust_parameter_ranges(base_params, complexity_metrics, 'xgboost_regression')
        
        def objective(trial):
            # ä½¿ç”¨è°ƒæ•´åçš„å‚æ•°èŒƒå›´ - ğŸ”§ ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–: æ•°æ®é©±åŠ¨å‚æ•°è°ƒæ•´
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *adjusted_params['n_estimators']),
                'max_depth': trial.suggest_int('max_depth', *adjusted_params['max_depth']),
                'learning_rate': trial.suggest_float('learning_rate', *adjusted_params['learning_rate'], log=True),
                'subsample': trial.suggest_float('subsample', *adjusted_params['subsample']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *adjusted_params['colsample_bytree']),
                'min_child_weight': trial.suggest_int('min_child_weight', *adjusted_params['min_child_weight']),
                'gamma': trial.suggest_float('gamma', *adjusted_params['gamma']),
                'reg_alpha': trial.suggest_float('reg_alpha', *adjusted_params['reg_alpha']),
                'reg_lambda': trial.suggest_float('reg_lambda', *adjusted_params['reg_lambda']),
                'random_state': 42,
                'verbosity': 0,
                'n_jobs': 1
            }
            
            tscv = TimeSeriesSplit(n_splits=self.cv_folds)
            cv_scores = []
            
            # å¤„ç†numpyæ•°ç»„å’Œpandas DataFrame
            if hasattr(X, 'iloc'):  # pandas DataFrame
                for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            else:  # numpyæ•°ç»„
                for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
                    X_train, X_val = X[train_idx], X[val_idx]
                    y_train, y_val = y[train_idx], y[val_idx]
                
                model = xgb.XGBRegressor(**params)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )
                
                y_pred = model.predict(X_val)
                
                if metric == 'r2':
                    score = r2_score(y_val, y_pred)
                elif metric == 'mse':
                    score = -mean_squared_error(y_val, y_pred)  # è´Ÿå€¼å› ä¸ºè¦æœ€å¤§åŒ–
                else:  # mae
                    from sklearn.metrics import mean_absolute_error
                    score = -mean_absolute_error(y_val, y_pred)
                
                cv_scores.append(score)
                trial.report(score, fold_idx)
                
                if trial.should_prune():
                    raise optuna.TrialPruned()
            
            return np.mean(cv_scores)
        
        study = optuna.create_study(
            direction=self.direction,
            sampler=self.sampler,
            pruner=self.pruner
        )
        
        study.optimize(
            objective,
            n_trials=self.n_trials,
            timeout=self.timeout,
            n_jobs=self.n_jobs,
            show_progress_bar=True
        )
        
        logger.info(f"âœ… XGBoostå›å½’ä¼˜åŒ–å®Œæˆ:")
        logger.info(f"  æœ€ä¼˜{metric.upper()}: {study.best_value:.4f}")
        logger.info(f"  æœ€ä¼˜å‚æ•°:")
        for key, value in study.best_params.items():
            logger.info(f"    {key}: {value}")
        logger.info(f"  æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        
        return {
            'best_params': study.best_params,
            'best_score': study.best_value,
            'n_trials': len(study.trials),
            'n_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
            'study': study
        }
    
    def optimize_lightgbm_classification(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        dates: Optional[pd.Series] = None,
        metric: str = 'auc'
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–LightGBMåˆ†ç±»å™¨è¶…å‚æ•°
        """
        logger.info("="*80)
        logger.info("å¼€å§‹LightGBMåˆ†ç±»å™¨è¶…å‚æ•°ä¼˜åŒ–")
        logger.info("="*80)
        
        # åˆ†ææ•°æ®å¤æ‚åº¦
        complexity_metrics = self._analyze_data_complexity(X, y)
        
        # åŸºç¡€å‚æ•°æœç´¢èŒƒå›´
        base_params = {
            'n_estimators': (100, 800),  # ğŸ”§ æ‰©å±•åˆ°100-800
            'max_depth': (3, 8),  # ğŸ”§ ä»3-4æ‰©å±•åˆ°3-8
            'learning_rate': (0.005, 0.5),  # ğŸ”§ æ‰©å±•åˆ°0.005-0.5
            'subsample': (0.5, 0.9),  # ğŸ”§ ä»0.6-0.8æ‰©å±•åˆ°0.5-0.9
            'colsample_bytree': (0.5, 0.9),  # ğŸ”§ ä»0.6-0.8æ‰©å±•åˆ°0.5-0.9
            'min_child_samples': (10, 150),  # ğŸ”§ æ‰©å±•åˆ°10-150
            'reg_alpha': (0.0, 15.0),  # ğŸ”§ ä»3-10æ‰©å±•åˆ°0-15
            'reg_lambda': (0.0, 15.0),  # ğŸ”§ ä»5-10æ‰©å±•åˆ°0-15
        }
        
        # åŸºäºæ•°æ®å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å‚æ•°èŒƒå›´
        adjusted_params = self._adjust_parameter_ranges(base_params, complexity_metrics, 'lightgbm_classification')
        
        def objective(trial):
            # ä½¿ç”¨è°ƒæ•´åçš„å‚æ•°èŒƒå›´ - ğŸ”§ ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–: æ•°æ®é©±åŠ¨å‚æ•°è°ƒæ•´
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *adjusted_params['n_estimators']),
                'max_depth': trial.suggest_int('max_depth', *adjusted_params['max_depth']),
                'learning_rate': trial.suggest_float('learning_rate', *adjusted_params['learning_rate'], log=True),
                'subsample': trial.suggest_float('subsample', *adjusted_params['subsample']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *adjusted_params['colsample_bytree']),
                'min_child_samples': trial.suggest_int('min_child_samples', *adjusted_params['min_child_samples']),
                'reg_alpha': trial.suggest_float('reg_alpha', *adjusted_params['reg_alpha']),
                'reg_lambda': trial.suggest_float('reg_lambda', *adjusted_params['reg_lambda']),
                'random_state': 42,
                'verbosity': -1,
                'n_jobs': 1
            }
            
            tscv = TimeSeriesSplit(n_splits=self.cv_folds)
            cv_scores = []
            
            for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                model = lgb.LGBMClassifier(**params)
                model.fit(X_train, y_train)
                
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                
                if metric == 'auc':
                    score = roc_auc_score(y_val, y_pred_proba)
                else:
                    from sklearn.metrics import f1_score
                    y_pred = (y_pred_proba >= 0.5).astype(int)
                    score = f1_score(y_val, y_pred)
                
                cv_scores.append(score)
                trial.report(score, fold_idx)
                
                if trial.should_prune():
                    raise optuna.TrialPruned()
            
            return np.mean(cv_scores)
        
        study = optuna.create_study(
            direction=self.direction,
            sampler=self.sampler,
            pruner=self.pruner
        )
        
        study.optimize(
            objective,
            n_trials=self.n_trials,
            timeout=self.timeout,
            n_jobs=self.n_jobs,
            show_progress_bar=True
        )
        
        logger.info(f"âœ… LightGBMåˆ†ç±»ä¼˜åŒ–å®Œæˆ:")
        logger.info(f"  æœ€ä¼˜{metric.upper()}: {study.best_value:.4f}")
        logger.info(f"  æœ€ä¼˜å‚æ•°: {study.best_params}")
        logger.info(f"  æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        
        return {
            'best_params': study.best_params,
            'best_score': study.best_value,
            'n_trials': len(study.trials),
            'n_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
            'study': study
        }
    
    def visualize_optimization(self, study: optuna.Study, save_path: Optional[str] = None):
        """
        å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
        
        Parameters
        ----------
        study : optuna.Study
            å®Œæˆçš„studyå¯¹è±¡
        save_path : str, optional
            ä¿å­˜è·¯å¾„å‰ç¼€
        """
        try:
            import matplotlib.pyplot as plt
            from optuna.visualization import (
                plot_optimization_history,
                plot_param_importances,
                plot_parallel_coordinate,
                plot_slice
            )
            
            logger.info("ç”ŸæˆOptunaå¯è§†åŒ–å›¾è¡¨...")
            
            # ä¼˜åŒ–å†å²
            fig1 = plot_optimization_history(study)
            if save_path:
                fig1.write_html(f"{save_path}_history.html")
            
            # å‚æ•°é‡è¦æ€§
            fig2 = plot_param_importances(study)
            if save_path:
                fig2.write_html(f"{save_path}_importance.html")
            
            # å¹³è¡Œåæ ‡å›¾
            fig3 = plot_parallel_coordinate(study)
            if save_path:
                fig3.write_html(f"{save_path}_parallel.html")
            
            logger.info(f"âœ… å¯è§†åŒ–å®Œæˆï¼Œä¿å­˜è‡³: {save_path}_*.html")
            
        except Exception as e:
            logger.warning(f"å¯è§†åŒ–å¤±è´¥: {e}")
