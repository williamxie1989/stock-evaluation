# -*- coding: utf-8 -*-
"""æ ‡ç­¾æ„å»ºç›¸å…³å·¥å…·ã€‚"""

import logging
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


def add_labels_corrected(
    features_df: pd.DataFrame,
    price_data: Optional[pd.DataFrame] = None,
    prediction_period: int = 30,
    threshold: float = 0.05,
    price_data_raw: Optional[pd.DataFrame] = None,
    classification_strategy: str = 'absolute',
    quantile: float = 0.7,
    min_samples_per_date: int = 30,
    negative_quantile: Optional[float] = 0.3,
    enable_neutral_band: bool = False,
    neutral_quantile: Optional[float] = 0.5,
    market_returns: Optional[pd.DataFrame] = None,
    use_market_baseline: bool = False,
    market_column: str = 'MKT',
    use_industry_neutral: bool = False,
    industry_column: str = 'industry',
    stock_data: Optional[pd.DataFrame] = None
) -> pd.DataFrame:
    """ä½¿ç”¨å‰å¤æƒä»·æ ¼æ„å»ºæ ‡ç­¾ï¼Œå¹¶æä¾›åŸå§‹ä»·æ ¼å¯¹ç…§ã€‚

    Parameters
    ----------
    features_df : DataFrame
        ç‰¹å¾æ•°æ®ï¼ˆéœ€åŒ…å« symbol, date åˆ—ï¼‰ã€‚
    price_data : DataFrame
        å‰å¤æƒä»·æ ¼æ•°æ®ï¼Œè‡³å°‘åŒ…å« symbol, date, closeã€‚
        è‹¥ä¸ºç©ºå°†å›é€€è‡³ ``stock_data``ï¼ˆå…¼å®¹æ—§å‚æ•°ï¼‰ã€‚
    prediction_period : int
        é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰ã€‚
    threshold : float
        ç»å¯¹æ¶¨å¹…é˜ˆå€¼ï¼Œä½œä¸ºé‡åŒ–ç­–ç•¥çš„å…œåº•æˆ– absolute ç­–ç•¥é˜ˆå€¼ã€‚
    price_data_raw : DataFrame, optional
        ä¸å¤æƒä»·æ ¼æ•°æ®ï¼Œç”¨äºé¢å¤–è¯Šæ–­ã€‚
    classification_strategy : {'absolute', 'quantile'}
        åˆ†ç±»æ ‡ç­¾ç­–ç•¥ï¼Œabsolute è¡¨ç¤ºä½¿ç”¨å›ºå®šé˜ˆå€¼ï¼Œquantile è¡¨ç¤ºæŒ‰æ—¥æœŸæˆªé¢åˆ†ä½æ•°ã€‚
    quantile : float
        å½“ç­–ç•¥ä¸º quantile æ—¶çš„ä¸Šåˆ†ä½æ•°ï¼ˆå¦‚ 0.7 è¡¨ç¤ºå–å½“æ—¥æ’è¡Œå‰ 30% ä¸ºæ­£ç±»ï¼‰ã€‚
    min_samples_per_date : int
        ä½¿ç”¨ quantile ç­–ç•¥æ—¶å½“æ—¥æœ€å°æ ·æœ¬æ•°ï¼Œå°äºè¯¥é˜ˆå€¼å›é€€è‡³ absolute ç­–ç•¥ã€‚
    negative_quantile : float, optional
        quantile ç­–ç•¥ä¸‹çš„ä¸‹åˆ†ä½æ•°ï¼Œç”¨äºæ˜ç¡®è´Ÿç±»æˆ–ä¸­æ€§åŒºã€‚
    enable_neutral_band : bool
        æ˜¯å¦å¯ç”¨ä¸­æ€§åŒºé—´ï¼Œå¯ç”¨æ—¶ä¼šç§»é™¤ä½äºä¸Š/ä¸‹åˆ†ä½æ•°ä¹‹é—´çš„æ ·æœ¬ã€‚
    neutral_quantile : float, optional
        ä¸­æ€§åŒºé—´çš„ä¸Šç•Œåˆ†ä½ï¼ˆä¾‹å¦‚ 0.5 è¡¨ç¤ºä¿ç•™ä½äºä¸­ä½æ•°çš„è´Ÿç±»ï¼‰ã€‚
    market_returns : DataFrame, optional
        å¸‚åœºåŸºå‡†æ”¶ç›Šæ•°æ®ï¼Œéœ€åŒ…å« date ä¸å¸‚åœºæ”¶ç›Šåˆ—ï¼ˆé»˜è®¤ MKTï¼‰ã€‚
    use_market_baseline : bool
        æ˜¯å¦ä½¿ç”¨å¸‚åœºåŸºå‡†æ„å»ºè¶…é¢æ”¶ç›Šã€‚
    market_column : str
        market_returns ä¸­çš„å¸‚åœºæ”¶ç›Šåˆ—åã€‚
    use_industry_neutral : bool
        æ˜¯å¦æŒ‰è¡Œä¸šæˆªé¢å»å‡å€¼ï¼Œä¾èµ– features_df ä¸­çš„è¡Œä¸šåˆ—ã€‚
    industry_column : str
        è¡Œä¸šåˆ—åˆ—åï¼ˆé»˜è®¤ industryï¼‰ã€‚
    stock_data : DataFrame, optional
        å…¼å®¹æ—§æ¥å£ï¼Œç­‰ä»·äº ``price_data``ã€‚
    """
    if price_data is None and stock_data is not None:
        price_data = stock_data

    if price_data is None:
        raise ValueError("price_data ç¼ºå¤±ï¼Œè¯·æä¾›å‰å¤æƒä»·æ ¼æ•°æ®æˆ–ä½¿ç”¨ stock_data å‚æ•°")

    features_df = features_df.copy()
    price_data = price_data.copy()
    price_data_raw = price_data_raw.copy() if price_data_raw is not None else None
    market_returns = market_returns.copy() if market_returns is not None else None

    required_cols = {'symbol', 'date', 'close'}
    missing_cols = required_cols - set(price_data.columns)
    if missing_cols:
        raise ValueError(f"price_data ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")

    features_df['date'] = pd.to_datetime(features_df['date'])
    price_data['date'] = pd.to_datetime(price_data['date'])
    price_data['close'] = pd.to_numeric(price_data['close'], errors='coerce')

    if price_data_raw is not None:
        price_data_raw['date'] = pd.to_datetime(price_data_raw['date'])
        price_data_raw['close'] = pd.to_numeric(price_data_raw['close'], errors='coerce')

    strategy = classification_strategy.lower().strip()
    if strategy not in {'absolute', 'quantile'}:
        raise ValueError(f"classification_strategy ä»…æ”¯æŒ 'absolute' æˆ– 'quantile'ï¼Œå½“å‰ä¸º {classification_strategy}")

    logger.info(
        "å¼€å§‹è®¡ç®—æ ‡ç­¾ (prediction_period=%d, strategy=%s, threshold=%.3f, quantile=%.2f)",
        prediction_period,
        strategy,
        threshold,
        quantile
    )

    symbol_results: List[pd.DataFrame] = []
    stats: Dict[str, float] = {
        'total_rows': 0,
        'valid_rows': 0,
        'missing_future': 0,
        'extreme_returns': 0,
        'extreme_raw': 0,
        'quantile_fallback_rows': 0,
        'neutral_dropped': 0,
        'market_baseline_rows': 0,
        'industry_residual_rows': 0
    }

    if not 0 < quantile < 1:
        raise ValueError(f"quantile å¿…é¡»åœ¨ (0, 1) èŒƒå›´å†…ï¼Œå½“å‰ä¸º {quantile}")

    if negative_quantile is not None and not 0 < negative_quantile < 1:
        logger.warning("negative_quantile åº”ä½äº (0, 1)ï¼Œå½“å‰å€¼æ— æ•ˆï¼Œå°†å¿½ç•¥è´Ÿåˆ†ä½æ•°è®¾ç½®")
        negative_quantile = None

    if neutral_quantile is not None and not 0 < neutral_quantile < 1:
        logger.warning("neutral_quantile åº”ä½äº (0, 1)ï¼Œå½“å‰å€¼æ— æ•ˆï¼Œå°†å¿½ç•¥ä¸­æ€§åŒºé—´")
        neutral_quantile = None
 
    def _prepare_market_baseline(df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """æ ‡å‡†åŒ–å¸‚åœºæ”¶ç›Šæ•°æ®å¹¶è®¡ç®—æœªæ¥é¢„æµ‹æœŸæ”¶ç›Š"""
        if df is None or len(df) == 0:
            return None

        market_df = df.copy()

        # å°†åˆ—åç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿åŒ¹é…
        market_df.columns = [str(col) for col in market_df.columns]

        if market_column not in market_df.columns:
            lower_map = {col.lower(): col for col in market_df.columns}
            if market_column.lower() in lower_map:
                market_df.rename(columns={lower_map[market_column.lower()]: market_column}, inplace=True)
            elif len(market_df.columns) == 1:
                market_df.rename(columns={market_df.columns[0]: market_column}, inplace=True)
            else:
                logger.warning("æœªåœ¨ market_returns ä¸­æ‰¾åˆ°åˆ— %sï¼Œå¿½ç•¥å¸‚åœºåŸºå‡†", market_column)
                return None

        if 'date' not in market_df.columns:
            if market_df.index.name == 'date' or isinstance(market_df.index, pd.DatetimeIndex):
                market_df = market_df.reset_index()
            elif 'index' in market_df.columns:
                market_df.rename(columns={'index': 'date'}, inplace=True)
            else:
                logger.warning("market_returns ç¼ºå°‘ date ä¿¡æ¯ï¼Œå¿½ç•¥å¸‚åœºåŸºå‡†")
                return None

        market_df = market_df[['date', market_column]].copy()
        market_df['date'] = pd.to_datetime(market_df['date'])
        market_df.sort_values('date', inplace=True)
        market_df[market_column] = pd.to_numeric(market_df[market_column], errors='coerce')

        # è®¡ç®—æœªæ¥é¢„æµ‹æœŸçš„ç´¯è®¡æ”¶ç›Š
        market_df['__cumprod__'] = (1.0 + market_df[market_column].fillna(0.0)).cumprod()
        market_df['__future_cumprod__'] = market_df['__cumprod__'].shift(-prediction_period)

        with np.errstate(divide='ignore', invalid='ignore'):
            market_df['market_future_return'] = market_df['__future_cumprod__'] / market_df['__cumprod__'] - 1.0

        market_df.drop(columns=['__cumprod__', '__future_cumprod__'], inplace=True)

        return market_df

    symbols = features_df['symbol'].unique()

    for idx, symbol in enumerate(symbols, 1):
        if idx % 50 == 0:
            logger.info("  è¿›åº¦: %d/%d", idx, len(symbols))

        symbol_features = features_df[features_df['symbol'] == symbol].copy()
        symbol_features.sort_values('date', inplace=True)

        price_symbol = price_data[price_data['symbol'] == symbol].copy()
        price_symbol.sort_values('date', inplace=True)
        price_symbol = price_symbol.dropna(subset=['close'])
        price_symbol = price_symbol.drop_duplicates(subset=['date'], keep='last')

        if price_symbol.empty:
            logger.warning("  %s: ä»·æ ¼æ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡æ ‡ç­¾æ„å»º", symbol)
            continue

        price_symbol['future_close'] = price_symbol['close'].shift(-prediction_period)
        price_symbol['future_return'] = (
            price_symbol['future_close'] - price_symbol['close']
        ) / price_symbol['close']
        price_symbol['future_return'].replace([np.inf, -np.inf], np.nan, inplace=True)

        merge_cols = ['date', 'future_return']

        if price_data_raw is not None:
            raw_symbol = price_data_raw[price_data_raw['symbol'] == symbol].copy()
            raw_symbol.sort_values('date', inplace=True)
            raw_symbol = raw_symbol.dropna(subset=['close'])
            raw_symbol = raw_symbol.drop_duplicates(subset=['date'], keep='last')

            if not raw_symbol.empty:
                raw_symbol['future_close_raw'] = raw_symbol['close'].shift(-prediction_period)
                raw_symbol['future_return_raw'] = (
                    raw_symbol['future_close_raw'] - raw_symbol['close']
                ) / raw_symbol['close']
                raw_symbol['future_return_raw'].replace([np.inf, -np.inf], np.nan, inplace=True)
                price_symbol = price_symbol.merge(
                    raw_symbol[['date', 'future_return_raw']],
                    on='date',
                    how='left'
                )
                merge_cols.append('future_return_raw')

        symbol_labeled = symbol_features.merge(
            price_symbol[['date'] + merge_cols[1:]],
            on='date',
            how='left'
        )

        stats['total_rows'] += len(symbol_labeled)
        missing_mask = symbol_labeled['future_return'].isna()
        stats['missing_future'] += int(missing_mask.sum())

        if 'future_return_raw' in symbol_labeled.columns:
            stats['extreme_raw'] += int((symbol_labeled['future_return_raw'].abs() > 0.3).sum())

        stats['extreme_returns'] += int((symbol_labeled['future_return'].abs() > 0.3).sum())

        symbol_results.append(symbol_labeled)

    if not symbol_results:
        raise ValueError("æ²¡æœ‰æˆåŠŸæ„å»ºæ ‡ç­¾çš„è‚¡ç¥¨ï¼Œæ‰€æœ‰ symbol è¢«è·³è¿‡")

    result = pd.concat(symbol_results, ignore_index=True)

    target_series = result['future_return'].copy()

    if use_market_baseline and market_returns is not None:
        market_baseline = _prepare_market_baseline(market_returns)
        if market_baseline is None:
            logger.warning("å¸‚åœºåŸºå‡†ä¸å¯ç”¨ï¼Œå›é€€ä¸ºç»å¯¹æ”¶ç›Šæ ‡ç­¾")
        else:
            result = result.merge(market_baseline, on='date', how='left')
            stats['market_baseline_rows'] = int(result['market_future_return'].notna().sum())
            if stats['market_baseline_rows'] == 0:
                logger.warning("å¸‚åœºåŸºå‡†æ•°æ®ä¸è‚¡ç¥¨æ—¥æœŸåŒºé—´æ— äº¤é›†ï¼Œå¿½ç•¥è¶…é¢æ”¶ç›Šè®¡ç®—")
                result.drop(columns=['market_future_return'], inplace=True, errors='ignore')
            else:
                result['future_excess_return'] = result['future_return'] - result['market_future_return']
                target_series = result['future_excess_return']

    if use_industry_neutral:
        if industry_column not in result.columns:
            logger.warning("æœªæ‰¾åˆ°è¡Œä¸šåˆ— %sï¼Œæ— æ³•æ‰§è¡Œè¡Œä¸šä¸­æ€§å¤„ç†", industry_column)
        else:
            base_col = 'future_excess_return' if 'future_excess_return' in result.columns else 'future_return'
            base_series = result[base_col]
            industry_series = result[industry_column].fillna('Unknown').astype(str)
            result['__industry__'] = industry_series
            grouped = result.groupby(['date', '__industry__'])[base_col]
            group_counts = grouped.transform('count')

            # éœ€è¦è‡³å°‘ä¸¤ä¸ªåŒæ—¥åŒè¡Œä¸šæ ·æœ¬æ‰æœ‰æ„ä¹‰è¿›è¡Œæ®‹å·®è®¡ç®—
            min_required = max(min(min_samples_per_date, 3), 2)
            sufficient_mask = group_counts >= min_required

            if sufficient_mask.any():
                industry_mean = grouped.transform('mean')
                residual_series = base_series - industry_mean
                result['future_residual_return'] = residual_series
                if (~sufficient_mask).any():
                    result.loc[~sufficient_mask, 'future_residual_return'] = np.nan
                residual_rows = int(sufficient_mask.sum())
                coverage = residual_rows / len(result) if len(result) else 0.0
                logger.info("  è¡Œä¸šä¸­æ€§è¦†ç›–ç‡: %.2f%% (%d/%d)", coverage * 100, residual_rows, len(result))
                if coverage < 0.35:
                    logger.warning("  è¡Œä¸šä¸­æ€§è¦†ç›–ç‡è¿‡ä½(<35%%)ï¼Œå›é€€ä½¿ç”¨åŸå§‹æ”¶ç›Šæ ‡ç­¾")
                    stats['industry_residual_rows'] = 0
                    result['future_residual_return'] = np.nan
                    target_series = base_series
                else:
                    stats['industry_residual_rows'] = residual_rows
                    target_series = result['future_residual_return'].where(sufficient_mask, base_series)
            else:
                logger.debug("è¡Œä¸šä¸­æ€§è·³è¿‡: æ—¥æœŸ/è¡Œä¸šæ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨åŸå§‹æ”¶ç›Šæ ‡ç­¾")
                result['future_residual_return'] = np.nan

    result['label_reg'] = target_series

    if strategy == 'absolute':
        result['label_cls'] = (result['label_reg'] > threshold).astype(int)
    else:
        grouped = result.groupby('date')['label_reg']

        def _quantile_or_nan(series: pd.Series, q: float) -> float:
            valid = series.dropna()
            if len(valid) < max(min_samples_per_date, 1):
                return np.nan
            return float(np.nanquantile(valid, q))

        per_date_high = grouped.transform(lambda s: _quantile_or_nan(s, quantile))

        fallback_mask = per_date_high.isna()
        stats['quantile_fallback_rows'] = int(fallback_mask.sum())

        applied_high = per_date_high.fillna(threshold)
        labels = (result['label_reg'] >= applied_high).astype(int)

        per_date_low = None
        if negative_quantile is not None:
            per_date_low = grouped.transform(lambda s: _quantile_or_nan(s, negative_quantile))

        neutral_upper = None
        if enable_neutral_band and neutral_quantile is not None:
            neutral_upper = grouped.transform(lambda s: _quantile_or_nan(s, neutral_quantile))

        if per_date_low is not None:
            low_mask = (per_date_low.notna()) & (result['label_reg'] <= per_date_low)
            labels = np.where(low_mask, 0, labels)

        result['label_cls'] = labels.astype(int)

        if enable_neutral_band and per_date_low is not None:
            neutral_mask = (result['label_cls'] == 0)
            neutral_mask &= per_date_low.notna()
            if neutral_upper is not None:
                neutral_mask &= neutral_upper.notna()
                neutral_mask &= result['label_reg'] > per_date_low
                neutral_mask &= result['label_reg'] < neutral_upper
            else:
                neutral_mask &= result['label_reg'] > per_date_low
                neutral_mask &= result['label_reg'] < applied_high

            if neutral_mask.any():
                stats['neutral_dropped'] = int(neutral_mask.sum())
                result = result.loc[~neutral_mask].copy()
                per_date_high = per_date_high[~neutral_mask]
                if per_date_low is not None:
                    per_date_low = per_date_low[~neutral_mask]
                if neutral_upper is not None:
                    neutral_upper = neutral_upper[~neutral_mask]
                result.reset_index(drop=True, inplace=True)

        valid_thresholds = per_date_high[~per_date_high.isna()]
        if not valid_thresholds.empty:
            logger.info(
                "  quantile é˜ˆå€¼ç»Ÿè®¡: å‡å€¼ %.4f, ä¸­ä½æ•° %.4f, æœ€å° %.4f, æœ€å¤§ %.4f",
                float(valid_thresholds.mean()),
                float(valid_thresholds.median()),
                float(valid_thresholds.min()),
                float(valid_thresholds.max())
            )
        if stats['quantile_fallback_rows'] > 0:
            logger.info(
                "  %d æ¡è®°å½•å› å½“æ—¥æ ·æœ¬ä¸è¶³å›é€€ä¸º absolute é˜ˆå€¼",
                stats['quantile_fallback_rows']
            )

    result['label_cls'] = result['label_cls'].astype(int)

    valid_mask = result['label_reg'].notna()
    stats['valid_rows'] = int(valid_mask.sum())

    total_rows = stats['total_rows'] or 1
    logger.info("æ ‡ç­¾è®¡ç®—å®Œæˆ:")
    logger.info(
        "  æœ‰æ•ˆæ ·æœ¬: %d/%d (%.1f%%)",
        stats['valid_rows'],
        stats['total_rows'],
        stats['valid_rows'] / total_rows * 100
    )
    pos_rate = (
        result.loc[valid_mask, 'label_cls'].mean() * 100
        if stats['valid_rows'] else 0.0
    )
    logger.info("  æ­£æ ·æœ¬ç‡: %.2f%%", pos_rate)
    logger.info(
        "  å¹³å‡æ”¶ç›Š: %.4f",
        result.loc[valid_mask, 'label_reg'].mean() if stats['valid_rows'] else 0.0
    )
    logger.info(
        "  æ”¶ç›Šæ ‡å‡†å·®: %.4f",
        result.loc[valid_mask, 'label_reg'].std() if stats['valid_rows'] else 0.0
    )
    logger.info(
        "  ç¼ºå¤±æœªæ¥ä»·æ ¼: %d (%.1f%%)",
        stats['missing_future'],
        stats['missing_future'] / total_rows * 100
    )
    logger.info("  å‰å¤æƒæç«¯æ”¶ç›Š(|r|>0.3): %d", stats['extreme_returns'])
    if stats['market_baseline_rows'] > 0:
        logger.info("  å¸‚åœºåŸºå‡†åŒ¹é…: %d è¡Œ", stats['market_baseline_rows'])
    if stats['neutral_dropped'] > 0:
        logger.info("  ä¸­æ€§åŒºç§»é™¤: %d è¡Œ", stats['neutral_dropped'])
    if price_data_raw is not None:
        logger.info("  åŸå§‹ä»·æ ¼æç«¯æ”¶ç›Š(|r|>0.3): %d", stats['extreme_raw'])

    result = result.loc[valid_mask].copy()

    # ï¿½ æ–¹æ¡ˆC2ä¿®æ”¹: ä¿ç•™future_residual_returnä¾›åç»­ä½¿ç”¨ï¼Œä½†ä¸ä½œä¸ºè®­ç»ƒç‰¹å¾
    # å…ˆå¤‡ä»½future_residual_returnï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    residual_return_backup = None
    if 'future_residual_return' in result.columns:
        residual_return_backup = result['future_residual_return'].copy()

    # ï¿½ğŸ”’ ç§»é™¤æ‰€æœ‰å¯èƒ½å¯¼è‡´æ•°æ®æ³„æ¼çš„æœªæ¥æ”¶ç›Šåˆ—
    # è¿™äº›åˆ—ä»…ç”¨äºæ ‡ç­¾è®¡ç®—ï¼Œä¸åº”ä½œä¸ºç‰¹å¾ä½¿ç”¨
    leakage_cols = [
        'future_return',           # æœªæ¥ç»å¯¹æ”¶ç›Š - ç›´æ¥æ³„æ¼ï¼
        'future_excess_return',    # æœªæ¥è¶…é¢æ”¶ç›Š - ç›´æ¥æ³„æ¼ï¼
        'future_residual_return',  # æœªæ¥æ®‹å·®æ”¶ç›Š - ç›´æ¥æ³„æ¼ï¼
        'future_return_raw',       # åŸå§‹ä»·æ ¼æœªæ¥æ”¶ç›Š
        'market_future_return',    # å¸‚åœºæœªæ¥æ”¶ç›Š
        '__industry__'             # ä¸´æ—¶è¡Œä¸šåˆ—
    ]
    
    cols_to_drop = [col for col in leakage_cols if col in result.columns]
    if cols_to_drop:
        logger.info(f"ğŸ”’ ç§»é™¤æ³„æ¼ç‰¹å¾åˆ—: {cols_to_drop}")
        result.drop(columns=cols_to_drop, inplace=True)

    # ğŸ”´ æ–¹æ¡ˆC2ä¿®æ”¹: æ¢å¤future_residual_returnï¼ˆä½†æ ‡è®°ä¸ºéç‰¹å¾åˆ—ï¼‰
    # è¿™ä¸ªåˆ—å°†åœ¨train_c2_solution.pyä¸­ç”¨äºæ›¿æ¢label_reg
    if residual_return_backup is not None:
        result['future_residual_return'] = residual_return_backup
        logger.info("âœ… ä¿ç•™ future_residual_return åˆ—ä¾›å›å½’æ ‡ç­¾ä½¿ç”¨ï¼ˆéè®­ç»ƒç‰¹å¾ï¼‰")

    return result
