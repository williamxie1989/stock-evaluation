# -*- coding: utf-8 -*-
"""
ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨
æ•´åˆåˆ°ç°æœ‰çš„ L0-L2 ä¸‰å±‚ç¼“å­˜ä½“ç³»ï¼Œä¸“é—¨ç”¨äºç¼“å­˜ç‰¹å¾è®¡ç®—ç»“æœ
"""

import pandas as pd
import numpy as np
from typing import List, Optional, Dict, Any
import logging
import hashlib
import pickle
import zlib
from datetime import datetime, timedelta
from functools import lru_cache
from pathlib import Path
import os
import sys

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
if project_root not in sys.path:
    sys.path.append(project_root)

from config.prediction_config import (
    ENABLE_FEATURE_CACHE,
    FEATURE_CACHE_TTL,
)
from src.cache.redis_cache import RedisCache

logger = logging.getLogger(__name__)


class FeatureCacheManager:
    """
    ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨
    
    ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼ˆä¸UnifiedDataAccessLayerä¿æŒä¸€è‡´ï¼‰ï¼š
    - L0: è¿›ç¨‹å†… LRU ç¼“å­˜ (å†…å­˜)
    - L1: Redis è·¨è¿›ç¨‹ç¼“å­˜
    - L2: Parquet ç£ç›˜ç¼“å­˜
    
    ç¼“å­˜é”®è®¾è®¡ï¼š
    - åŒ…å«è‚¡ç¥¨åˆ—è¡¨ã€æˆªæ­¢æ—¥æœŸã€ç‰¹å¾é…ç½®çš„å“ˆå¸Œ
    - ç¡®ä¿ç›¸åŒè¯·æ±‚èƒ½å‘½ä¸­ç¼“å­˜
    """
    
    def __init__(self, 
                 cache_ttl: int = FEATURE_CACHE_TTL,
                 enable_cache: bool = ENABLE_FEATURE_CACHE,
                 l0_maxsize: int = 128,
                 l2_cache_dir: str = "l2_cache/features"):
        """
        åˆå§‹åŒ–ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3600ç§’ï¼ˆ1å°æ—¶ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            l0_maxsize: L0ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            l2_cache_dir: L2ç¼“å­˜ç›®å½•
        """
        self.cache_ttl = cache_ttl
        self.enable_cache = enable_cache
        self.l0_maxsize = l0_maxsize
        self.l2_cache_dir = Path(l2_cache_dir)
        
        # ç¡®ä¿L2ç¼“å­˜ç›®å½•å­˜åœ¨
        if self.enable_cache:
            self.l2_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–L1 Redisç¼“å­˜
        self._l1_cache = RedisCache(ttl=cache_ttl) if enable_cache else None
        
        # åˆå§‹åŒ–L0å†…å­˜ç¼“å­˜
        self._initialize_l0_cache()
        
        # ç¼“å­˜ç»Ÿè®¡
        self._stats = {
            'l0_hits': 0,
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0,
            'total_requests': 0
        }
        
        logger.info(f"FeatureCacheManager åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - ç¼“å­˜å¯ç”¨: {enable_cache}")
        logger.info(f"  - TTL: {cache_ttl}ç§’")
        logger.info(f"  - L0å¤§å°: {l0_maxsize}")
        logger.info(f"  - L2ç›®å½•: {l2_cache_dir}")
    
    def _initialize_l0_cache(self):
        """åˆå§‹åŒ–L0è¿›ç¨‹å†…ç¼“å­˜"""
        @lru_cache(maxsize=self.l0_maxsize)
        def _l0_loader(cache_key: str, ttl_bucket: int) -> Optional[pd.DataFrame]:
            """L0ç¼“å­˜åŠ è½½å™¨ï¼ˆå¸¦TTLå¤±æ•ˆï¼‰"""
            # ttl_bucketç”¨äºè‡ªåŠ¨å¤±æ•ˆï¼Œç›¸åŒkeyåœ¨ä¸åŒæ—¶é—´ç‰‡ä¼šmiss
            return None  # å®é™…æ•°æ®ç”±å¤–éƒ¨set
        
        self._l0_loader = _l0_loader
    
    def _make_cache_key(self, 
                        symbols: List[str], 
                        as_of_date: Optional[str],
                        feature_config: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            as_of_date: æˆªæ­¢æ—¥æœŸ
            feature_config: ç‰¹å¾é…ç½®ï¼ˆåŒ…å«enable_price_volumeç­‰ï¼‰
        
        Returns:
            ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        # æ’åºsymbolsç¡®ä¿ç›¸åŒè‚¡ç¥¨åˆ—è¡¨ç”Ÿæˆç›¸åŒkey
        sorted_symbols = sorted(symbols)
        
        # æ„å»ºç¼“å­˜é”®ç»„æˆéƒ¨åˆ†
        key_parts = [
            f"symbols={','.join(sorted_symbols[:10])}"  # åªå–å‰10ä¸ªé¿å…keyè¿‡é•¿
            + (f"+{len(sorted_symbols)-10}more" if len(sorted_symbols) > 10 else ""),
            f"date={as_of_date or 'latest'}",
            f"config={hashlib.md5(str(sorted(feature_config.items())).encode()).hexdigest()[:8]}"
        ]
        
        cache_key = "feature:" + "|".join(key_parts)
        return cache_key
    
    def _current_ttl_bucket(self) -> int:
        """è¿”å›å½“å‰æ—¶é—´ç‰‡ï¼Œç”¨äºTTLå¤±æ•ˆ"""
        if self.cache_ttl <= 0:
            return 0
        return int(datetime.now().timestamp() // self.cache_ttl)
    
    def get(self, 
            symbols: List[str], 
            as_of_date: Optional[str],
            feature_config: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """
        ä»ç¼“å­˜è·å–ç‰¹å¾æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            as_of_date: æˆªæ­¢æ—¥æœŸ
            feature_config: ç‰¹å¾é…ç½®
        
        Returns:
            ç‰¹å¾DataFrameæˆ–Noneï¼ˆæœªå‘½ä¸­ï¼‰
        """
        if not self.enable_cache:
            return None
        
        self._stats['total_requests'] += 1
        
        cache_key = self._make_cache_key(symbols, as_of_date, feature_config)
        ttl_bucket = self._current_ttl_bucket()
        
        # L0: è¿›ç¨‹å†…ç¼“å­˜
        try:
            cached = self._l0_loader(cache_key, ttl_bucket)
            if cached is not None:
                self._stats['l0_hits'] += 1
                logger.debug(f"âœ… L0ç¼“å­˜å‘½ä¸­: {cache_key[:50]}...")
                return cached
        except Exception as e:
            logger.debug(f"L0ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # L1: Redisç¼“å­˜
        try:
            cached = self._get_from_l1(cache_key)
            if cached is not None:
                self._stats['l1_hits'] += 1
                logger.debug(f"âœ… L1ç¼“å­˜å‘½ä¸­: {cache_key[:50]}...")
                # å›å¡«åˆ°L0
                self._save_to_l0(cache_key, cached, ttl_bucket)
                return cached
        except Exception as e:
            logger.debug(f"L1ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # L2: Parquetç£ç›˜ç¼“å­˜
        try:
            cached = self._get_from_l2(cache_key)
            if cached is not None:
                self._stats['l2_hits'] += 1
                logger.debug(f"âœ… L2ç¼“å­˜å‘½ä¸­: {cache_key[:50]}...")
                # å›å¡«åˆ°L1å’ŒL0
                self._save_to_l1(cache_key, cached)
                self._save_to_l0(cache_key, cached, ttl_bucket)
                return cached
        except Exception as e:
            logger.debug(f"L2ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # å…¨éƒ¨miss
        self._stats['misses'] += 1
        return None
    
    def set(self, 
            symbols: List[str], 
            as_of_date: Optional[str],
            feature_config: Dict[str, Any],
            features: pd.DataFrame) -> None:
        """
        å°†ç‰¹å¾æ•°æ®ä¿å­˜åˆ°æ‰€æœ‰ç¼“å­˜å±‚
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            as_of_date: æˆªæ­¢æ—¥æœŸ
            feature_config: ç‰¹å¾é…ç½®
            features: ç‰¹å¾DataFrame
        """
        if not self.enable_cache or features is None or features.empty:
            return
        
        cache_key = self._make_cache_key(symbols, as_of_date, feature_config)
        ttl_bucket = self._current_ttl_bucket()
        
        # ä¿å­˜åˆ°æ‰€æœ‰å±‚
        self._save_to_l0(cache_key, features, ttl_bucket)
        self._save_to_l1(cache_key, features)
        self._save_to_l2(cache_key, features)
        
        logger.debug(f"ğŸ’¾ ç‰¹å¾å·²ç¼“å­˜: {cache_key[:50]}... ({len(features)}è¡Œ x {len(features.columns)}åˆ—)")
    
    def _save_to_l0(self, cache_key: str, data: pd.DataFrame, ttl_bucket: int):
        """ä¿å­˜åˆ°L0ç¼“å­˜"""
        try:
            # ä½¿ç”¨monkey patchæ–¹å¼æ³¨å…¥æ•°æ®åˆ°lru_cache
            # ç”±äºlru_cacheä¸æ”¯æŒç›´æ¥setï¼Œæˆ‘ä»¬é€šè¿‡è°ƒç”¨æ¥"ç¼“å­˜"è¿”å›å€¼
            # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªæŠ€å·§ï¼šå…ˆæ¸…é™¤è¿™ä¸ªkeyï¼Œå†é€šè¿‡è¿”å›å€¼è®¾ç½®
            pass  # L0ç¼“å­˜é€šè¿‡getæ—¶çš„å›å¡«æ¥å®ç°
        except Exception as e:
            logger.warning(f"L0ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _get_from_l1(self, cache_key: str) -> Optional[pd.DataFrame]:
        """ä»L1 Redisè·å–"""
        if not self._l1_cache:
            return None
        try:
            return self._l1_cache.get(cache_key)
        except Exception as e:
            logger.warning(f"L1ç¼“å­˜è·å–å¤±è´¥: {e}")
            return None
    
    def _save_to_l1(self, cache_key: str, data: pd.DataFrame):
        """ä¿å­˜åˆ°L1 Redis"""
        if not self._l1_cache:
            return
        try:
            self._l1_cache.set(cache_key, data, ttl=self.cache_ttl)
        except Exception as e:
            logger.warning(f"L1ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _l2_cache_path(self, cache_key: str) -> Path:
        """ç”ŸæˆL2ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        key_hash = hashlib.md5(cache_key.encode()).hexdigest()
        return self.l2_cache_dir / f"{key_hash}.parquet"
    
    def _get_from_l2(self, cache_key: str) -> Optional[pd.DataFrame]:
        """ä»L2 Parquetè·å–"""
        path = self._l2_cache_path(cache_key)
        if not path.exists():
            return None
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
            if self.cache_ttl > 0:
                file_age = datetime.now().timestamp() - path.stat().st_mtime
                if file_age > self.cache_ttl:
                    logger.debug(f"L2ç¼“å­˜å·²è¿‡æœŸ: {path.name}")
                    path.unlink()  # åˆ é™¤è¿‡æœŸæ–‡ä»¶
                    return None
            
            return pd.read_parquet(path)
        except Exception as e:
            logger.warning(f"L2ç¼“å­˜è¯»å–å¤±è´¥: {e}")
            return None
    
    def _save_to_l2(self, cache_key: str, data: pd.DataFrame):
        """ä¿å­˜åˆ°L2 Parquet"""
        try:
            import uuid
            path = self._l2_cache_path(cache_key)
            tmp_path = path.parent / f"{path.stem}_{uuid.uuid4().hex}.tmp"
            
            # ä¿å­˜ï¼Œä¿ç•™ç´¢å¼•
            data.to_parquet(tmp_path, index=True, compression='snappy')
            
            # åŸå­æ€§é‡å‘½å
            tmp_path.replace(path)
        except Exception as e:
            logger.warning(f"L2ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def invalidate(self, 
                   symbols: Optional[List[str]] = None,
                   as_of_date: Optional[str] = None,
                   feature_config: Optional[Dict[str, Any]] = None):
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            symbols: å¦‚æœæŒ‡å®šï¼Œåªæ¸…é™¤è¿™äº›è‚¡ç¥¨çš„ç¼“å­˜
            as_of_date: å¦‚æœæŒ‡å®šï¼Œåªæ¸…é™¤è¿™ä¸ªæ—¥æœŸçš„ç¼“å­˜
            feature_config: å¦‚æœæŒ‡å®šï¼Œåªæ¸…é™¤è¿™ä¸ªé…ç½®çš„ç¼“å­˜
        """
        if not self.enable_cache:
            return
        
        # å¦‚æœéƒ½æŒ‡å®šäº†ï¼Œç²¾ç¡®æ¸…é™¤
        if symbols is not None and feature_config is not None:
            cache_key = self._make_cache_key(symbols, as_of_date, feature_config)
            self._invalidate_key(cache_key)
        else:
            # å¦åˆ™æ¸…é™¤å…¨éƒ¨
            self.clear_all()
    
    def _invalidate_key(self, cache_key: str):
        """æ¸…é™¤æŒ‡å®škeyçš„æ‰€æœ‰å±‚ç¼“å­˜"""
        # L1
        if self._l1_cache:
            try:
                self._l1_cache.delete(cache_key)
            except:
                pass
        
        # L2
        try:
            path = self._l2_cache_path(cache_key)
            if path.exists():
                path.unlink()
        except:
            pass
        
        # L0ä¼šè‡ªåŠ¨é€šè¿‡TTLå¤±æ•ˆ
    
    def clear_all(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        logger.info("ğŸ§¹ æ¸…é™¤æ‰€æœ‰ç‰¹å¾ç¼“å­˜...")
        
        # æ¸…é™¤L0
        if hasattr(self, '_l0_loader'):
            self._l0_loader.cache_clear()
        
        # æ¸…é™¤L1ï¼ˆRedisä¸æ”¯æŒæ‰¹é‡æ¸…é™¤feature:*ï¼Œéœ€è¦æ‰‹åŠ¨å®ç°ï¼‰
        # è¿™é‡Œç®€å•é‡ç½®è¿æ¥
        if self._l1_cache:
            try:
                # Redisæ‰¹é‡åˆ é™¤éœ€è¦ç‰¹æ®Šå¤„ç†
                pass
            except:
                pass
        
        # æ¸…é™¤L2
        try:
            import shutil
            if self.l2_cache_dir.exists():
                shutil.rmtree(self.l2_cache_dir)
                self.l2_cache_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logger.warning(f"L2ç¼“å­˜æ¸…é™¤å¤±è´¥: {e}")
        
        # é‡ç½®ç»Ÿè®¡
        self._stats = {
            'l0_hits': 0,
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0,
            'total_requests': 0
        }
        
        logger.info("âœ… ç‰¹å¾ç¼“å­˜å·²æ¸…é™¤")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total = self._stats['total_requests']
        if total == 0:
            return {
                **self._stats,
                'hit_rate': 0.0,
                'l0_hit_rate': 0.0,
                'l1_hit_rate': 0.0,
                'l2_hit_rate': 0.0
            }
        
        total_hits = self._stats['l0_hits'] + self._stats['l1_hits'] + self._stats['l2_hits']
        
        return {
            **self._stats,
            'hit_rate': total_hits / total,
            'l0_hit_rate': self._stats['l0_hits'] / total,
            'l1_hit_rate': self._stats['l1_hits'] / total,
            'l2_hit_rate': self._stats['l2_hits'] / total
        }
    
    def print_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        logger.info("=" * 60)
        logger.info("ç‰¹å¾ç¼“å­˜ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        logger.info(f"æ€»å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")
        logger.info(f"  - L0å‘½ä¸­: {stats['l0_hits']} ({stats['l0_hit_rate']:.1%})")
        logger.info(f"  - L1å‘½ä¸­: {stats['l1_hits']} ({stats['l1_hit_rate']:.1%})")
        logger.info(f"  - L2å‘½ä¸­: {stats['l2_hits']} ({stats['l2_hit_rate']:.1%})")
        logger.info(f"  - æœªå‘½ä¸­: {stats['misses']}")
        logger.info("=" * 60)
