#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½é€‰è‚¡æœåŠ¡
åŸºäºæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè‚¡ç¥¨é¢„æµ‹å’Œæ’åº
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import joblib
import os
# æ–°å¢ json å¯¼å…¥
import json
from sklearn.calibration import CalibratedClassifierCV
from ...data.db.unified_database_manager import UnifiedDatabaseManager
from src.core.unified_data_access_factory import (
    create_unified_data_access,
    get_unified_data_access,
)
from .features import FeatureGenerator
from ...ml.features.enhanced_features import EnhancedFeatureGenerator
from ...trading.signals.signal_generator import SignalGenerator
from ...services.stock.stock_status_filter import StockStatusFilter
# å¼•å…¥å­—æ®µæ˜ å°„å·¥å…·ï¼Œç»Ÿä¸€å­—æ®µå
from src.data.field_mapping import FieldMapper
# å¯¼å…¥æ€§èƒ½ä¼˜åŒ–é…ç½®
from config.prediction_config import (
    PREDICTION_PERIOD_DAYS,
    MAX_STOCK_POOL_SIZE,
    ENABLE_FEATURE_CACHE,
    FEATURE_CACHE_TTL,
    FEATURE_CACHE_DIR,
    ENABLE_SYMBOL_CACHE,
    SYMBOL_CACHE_TTL,
)
import logging
import re

# å¯¼å…¥å¢å¼ºé¢„å¤„ç†pipeline
from src.ml.features.enhanced_preprocessing import (
    EnhancedPreprocessingPipeline,
    create_enhanced_preprocessing_config,
)
# å¯¼å…¥V2ç»Ÿä¸€ç‰¹å¾æ„å»ºå™¨
from src.ml.features.unified_feature_builder import UnifiedFeatureBuilder

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntelligentStockSelector:
    """
    æ™ºèƒ½é€‰è‚¡æœåŠ¡
    """

    # æ–°å¢: å·¥å…·æ–¹æ³• â€“ ç»Ÿä¸€æ›¿æ¢åŒ¿åç‰¹å¾åï¼Œé¿å…å¤šå¤„é‡å¤å®ç°
    def _replace_anonymous_feature_names(self, model, actual_column_names):
        """å¦‚æœæ¨¡å‹çš„ feature_names_in_ ä¸ºåŒ¿åçš„ feature_0ã€feature_1 ç­‰ï¼Œå ä½åå­—ï¼Œåˆ™ä½¿ç”¨å®é™…åˆ—åæ›¿æ¢ã€‚
        å‚æ•°:
            model: ä»»ä½•å¸¦æœ‰ feature_names_in_ å±æ€§çš„ sklearn/XGBoost æ¨¡å‹æˆ– Pipeline
            actual_column_names: list[str] â€“ çœŸå®çš„ç‰¹å¾åˆ—åç§°ï¼Œé¡ºåºä¸è®­ç»ƒ/é¢„æµ‹ä¸€è‡´
        """
        try:
            # å¦‚æœæ˜¯Pipeline, é€’å½’å¤„ç†å…¶æœ€åä¸€ä¸ªEstimatorï¼Œå¹¶åŒæ­¥Pipelineæœ¬èº«
            try:
                from sklearn.pipeline import Pipeline
                if isinstance(model, Pipeline):
                    inner_est = model.steps[-1][1]
                    self._replace_anonymous_feature_names(inner_est, actual_column_names)
                    model.feature_names_in_ = np.array(actual_column_names)
                    return
            except Exception:
                pass

            # é’ˆå¯¹ xgboost æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†ï¼Œéœ€è¦åŒæ—¶æ›´æ–° booster çš„ feature_names
            try:
                import xgboost as xgb  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å®‰è£…ç¼ºå¤±æ—¶æŠ¥é”™
                if isinstance(model, xgb.XGBModel):
                    model.feature_names_in_ = np.array(actual_column_names)
                    booster = model.get_booster()
                    booster.feature_names = actual_column_names
                    return
            except Exception:
                pass  # ä¸æ˜¯ xgboost æ¨¡å‹æˆ–å¯¼å…¥å¤±è´¥

            # å…¶ä»– sklearn æ¨¡å‹é€»è¾‘
            if not hasattr(model, "feature_names_in_"):
                # è‹¥æ¨¡å‹å®Œå…¨æ²¡æœ‰è¯¥å±æ€§ï¼Œåˆ™ç›´æ¥èµ‹å€¼
                model.feature_names_in_ = np.array(actual_column_names)
                return

            # å¯¹å·²æœ‰å±æ€§è¿›è¡Œæ£€æŸ¥/è¦†ç›–
            feature_names = list(getattr(model, "feature_names_in_", []))
            if not feature_names or len(feature_names) != len(actual_column_names):
                model.feature_names_in_ = np.array(actual_column_names)
                return

            # å¦‚æœé•¿åº¦ä¸€è‡´ä½†å­˜åœ¨åŒ¿åå ä½æˆ–è€…ä¸å½“å‰åˆ—ä¸ä¸€è‡´ï¼Œä¹Ÿè¦†ç›–
            anonymous = all(re.match(r"^feature_\d+$", str(fn)) for fn in feature_names)
            if anonymous or feature_names != actual_column_names:
                model.feature_names_in_ = np.array(actual_column_names)
        except Exception as e:
            logger.debug(f"æ›¿æ¢åŒ¿åç‰¹å¾åå¤±è´¥: {e}")
    
    def __init__(self, db_manager: UnifiedDatabaseManager = None, use_enhanced_features: bool = 1, 
                 use_enhanced_preprocessing: bool = 1, preprocessing_complexity: str = 'medium',
                 model_config_path: str = 'config/selector_models.json', prediction_period: Optional[int] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½é€‰è‚¡å™¨
        
        Parameters
        ----------
        prediction_period : int, optional
            é¢„æµ‹å‘¨æœŸï¼ˆå¤©æ•°ï¼‰ï¼Œé»˜è®¤è¯»å– SELECTOR_PREDICTION_PERIOD ç¯å¢ƒå˜é‡ï¼Œ
            æœªè®¾ç½®åˆ™ä½¿ç”¨ PREDICTION_PERIOD_DAYS
        """
        # ç¡®å®šé¢„æµ‹å‘¨æœŸ
        if prediction_period is None:
            prediction_period = int(os.getenv('SELECTOR_PREDICTION_PERIOD', 
                                             str(PREDICTION_PERIOD_DAYS)))
        self.prediction_period = prediction_period
        logger.info(f"IntelligentStockSelector åˆå§‹åŒ– (é¢„æµ‹å‘¨æœŸ={prediction_period}å¤©)")
        
        self.db = db_manager or UnifiedDatabaseManager(db_type='mysql')
        self.data_access = get_unified_data_access()
        if self.data_access is None:
            try:
                self.data_access = create_unified_data_access(
                    data_access_config={"use_cache": True, "auto_sync": False},
                    validate_sources=False,
                )
            except Exception:
                self.data_access = None
        self.use_enhanced_features = use_enhanced_features
        self.use_enhanced_preprocessing = use_enhanced_preprocessing
        self.preprocessing_complexity = preprocessing_complexity
        
        # æ ¹æ®å‚æ•°é€‰æ‹©ç‰¹å¾ç”Ÿæˆå™¨
        if use_enhanced_features:
            self.feature_generator = EnhancedFeatureGenerator()
        else:
            self.feature_generator = FeatureGenerator()
        
        # æ–°å¢ï¼šä¸ºV2æ¨¡å‹åˆ›å»ºç»Ÿä¸€ç‰¹å¾æ„å»ºå™¨
        try:
            self.unified_feature_builder = UnifiedFeatureBuilder(
                data_access=self.data_access,
                db_manager=self.db,
                lookback_days=180  # ä¸V2è®­ç»ƒä¸€è‡´
            )
            logger.info("âœ… UnifiedFeatureBuilder åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"UnifiedFeatureBuilder åˆå§‹åŒ–å¤±è´¥: {e}ï¼ŒV2æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")
            self.unified_feature_builder = None
        
        self.signal_generator = SignalGenerator()
        self.stock_filter = StockStatusFilter()
        
        # æ¨¡å‹ç›¸å…³
        self.model = None
        self.scaler = None
        self.feature_names = None
        
        # æ–°å¢ï¼šåˆ†åˆ«æŒæœ‰åˆ†ç±»ä¸å›å½’æ¨¡å‹æ•°æ®
        self.cls_model_data = None
        self.reg_model_data = None
        self.cls_feature_names = None
        self.reg_feature_names = None
        self.cls_model_bundle = None
        self.reg_model_bundle = None
        self.cls_metadata = {}
        self.reg_metadata = {}
        self.cls_calibrator = None
        self.reg_calibrator = None
        
        # å¢å¼ºé¢„å¤„ç†pipeline
        self.cls_preprocessor = None
        self.reg_preprocessor = None
        
        # æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒé€šè¿‡å¤–éƒ¨æ–‡ä»¶/ç¯å¢ƒå˜é‡æŒ‡å®šæ¨¡å‹ä½ç½®ï¼‰
        self.model_config_path = model_config_path

        if self.use_enhanced_preprocessing:
            # åˆå§‹åŒ–é¢„å¤„ç†é…ç½®
            self.cls_preprocessing_config = create_enhanced_preprocessing_config('classification', preprocessing_complexity)
            self.reg_preprocessing_config = create_enhanced_preprocessing_config('regression', preprocessing_complexity)
            logger.info(f"å¯ç”¨å¢å¼ºé¢„å¤„ç†pipelineï¼Œå¤æ‚åº¦: {preprocessing_complexity}")
    
    def load_model(self, model_path: str = None, scaler_path: str = None):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        """
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            if model_path is None:
                models_dir = "models"
                if not os.path.exists(models_dir):
                    logger.warning(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
                    return 0

                # æŸ¥æ‰¾æ‰€æœ‰pklæ–‡ä»¶
                model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]
                if not model_files:
                    logger.warning(f"æ¨¡å‹ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°pklæ–‡ä»¶: {models_dir}")
                    return 0
                
                # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼Œæ—¶é—´æˆ³è¶Šæ–°è¶Šé åï¼‰
                model_files.sort()
                latest_model = model_files[-1]
                model_path = os.path.join(models_dir, latest_model)
                
                logger.info(f"è‡ªåŠ¨é€‰æ‹©æ¨¡å‹æ–‡ä»¶: {model_path}")
            
            if os.path.exists(model_path):
                # åŠ è½½æ¨¡å‹æ–‡ä»¶
                model_data = joblib.load(model_path)
                
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ç»“æ„
                if isinstance(model_data, dict):
                    # æ–°æ ¼å¼ï¼šåŒ…å«å¤šä¸ªç»„ä»¶çš„å­—å…¸
                    if 'model' in model_data:
                        self.model = model_data['model']
                        logger.info(f"ä»å­—å…¸ä¸­åŠ è½½æ¨¡å‹æˆåŠŸ: {model_path}")
                        
                        # åŒæ­¥åŠ è½½ç‰¹å¾åï¼Œç¡®ä¿é¢„æµ‹æ—¶ç‰¹å¾é¡ºåºä¸€è‡´
                        if 'feature_names' in model_data and model_data['feature_names']:
                            self.feature_names = list(model_data['feature_names'])
                            logger.info(f"å·²åŠ è½½ç‰¹å¾åï¼Œæ•°é‡: {len(self.feature_names)}")
                        
                        # å¦‚æœä¿å­˜äº†scalerä¹Ÿè®°å½•ä¸‹æ¥ï¼ˆä»…å½“æ¨¡å‹ä¸æ˜¯åŒ…å«scalerçš„Pipelineæ—¶æ‰ä¼šåœ¨é¢„æµ‹ä¸­ä½¿ç”¨ï¼‰
                        if 'scaler' in model_data:
                            self.scaler = model_data['scaler']
                            logger.info(f"ä»å­—å…¸ä¸­åŠ è½½æ ‡å‡†åŒ–å™¨æˆåŠŸ")
                        else:
                            logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸­æœªåŒ…å«æ ‡å‡†åŒ–å™¨ï¼Œå°†å°è¯•å•ç‹¬åŠ è½½")
                            # å°è¯•åŠ è½½å•ç‹¬çš„scaleræ–‡ä»¶
                            if not self._load_separate_scaler(scaler_path):
                                logger.warning(f"æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹ç‰¹å¾/æˆ–ç”±Pipelineå†…éƒ¨å¤„ç†")
                                self.scaler = None
                        
                        # å¦‚æ¨¡å‹ä¸ºåŒ…å«scalerçš„Pipelineï¼Œé¢„æµ‹æ—¶åº”ç›´æ¥ä¼ å…¥åŸå§‹ç‰¹å¾
                        try:
                            if hasattr(self.model, 'named_steps') and 'scaler' in getattr(self.model, 'named_steps', {}):
                                logger.info("æ£€æµ‹åˆ°æ¨¡å‹ä¸ºåŒ…å«scalerçš„Pipelineï¼Œé¢„æµ‹æ—¶å°†è·³è¿‡å¤–éƒ¨æ ‡å‡†åŒ–ï¼Œé¿å…äºŒæ¬¡ç¼©æ”¾")
                        except Exception:
                            pass
                    else:
                        logger.error(f"æ¨¡å‹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œæœªæ‰¾åˆ°modelå­—æ®µ")
                        return 0
                else:
                    # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯æ¨¡å‹å¯¹è±¡
                    self.model = model_data
                    logger.info(f"åŠ è½½æ—§æ ¼å¼æ¨¡å‹æˆåŠŸ: {model_path}")
                    # å°è¯•åŠ è½½å•ç‹¬çš„scaleræ–‡ä»¶
                    if not self._load_separate_scaler(scaler_path):
                        logger.warning(f"æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹ç‰¹å¾")
                        self.scaler = None
            else:
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return 0
                
            return 1
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return 0
    
    # ========= æ–°å¢æ–¹æ³•ï¼šè·å–æ¨¡å‹è·¯å¾„ï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼Œå…¶æ¬¡é…ç½®æ–‡ä»¶ï¼‰ =========
    def _get_config_model_paths(self) -> Tuple[Optional[str], Optional[str]]:
        """
        æ ¹æ®ç¯å¢ƒå˜é‡æˆ– JSON é…ç½®æ–‡ä»¶è·å–åˆ†ç±»ä¸å›å½’æ¨¡å‹è·¯å¾„

        ä¼˜å…ˆçº§ï¼š
        1. ç¯å¢ƒå˜é‡ SELECTOR_CLS_MODEL_PATHã€SELECTOR_REG_MODEL_PATH
        2. JSON é…ç½®æ–‡ä»¶ self.model_config_pathï¼Œæ ¼å¼ç¤ºä¾‹ï¼š
           {
               "classification": "/abs/path/to/cls_model.pkl",
               "regression": "/abs/path/to/reg_model.pkl"
           }

        Returns
        -------
        Tuple[Optional[str], Optional[str]]
            (cls_model_path, reg_model_path)
        """
        cls_path = os.getenv('SELECTOR_CLS_MODEL_PATH')
        reg_path = os.getenv('SELECTOR_REG_MODEL_PATH')

        # å¤„ç†ç›¸å¯¹è·¯å¾„ -> ç»å¯¹è·¯å¾„
        if cls_path and not os.path.isabs(cls_path):
            cls_path = os.path.abspath(cls_path)
        if reg_path and not os.path.isabs(reg_path):
            reg_path = os.path.abspath(reg_path)

        # å¦‚æœä»»æ„ä¸€ä¸ªè·¯å¾„ç¼ºå¤±æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•è¯»å–é…ç½®æ–‡ä»¶
        need_config = (not cls_path or not os.path.exists(cls_path)) or (not reg_path or not os.path.exists(reg_path))
        if need_config and self.model_config_path and os.path.exists(self.model_config_path):
            try:
                with open(self.model_config_path, 'r', encoding='utf-8') as cf:
                    cfg = json.load(cf)
                if (not cls_path or not os.path.exists(cls_path)):
                    cls_candidate = cfg.get('classification') or cfg.get('cls')
                    if cls_candidate:
                        cls_candidate = os.path.abspath(cls_candidate)
                        if os.path.exists(cls_candidate):
                            cls_path = cls_candidate
                if (not reg_path or not os.path.exists(reg_path)):
                    reg_candidate = cfg.get('regression') or cfg.get('reg')
                    if reg_candidate:
                        reg_candidate = os.path.abspath(reg_candidate)
                        if os.path.exists(reg_candidate):
                            reg_path = reg_candidate
            except Exception as e:
                logger.debug(f"è§£ææ¨¡å‹é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        # æœ€ç»ˆè¿”å›å­˜åœ¨çš„è·¯å¾„ï¼Œå¦åˆ™è¿”å› None
        return (cls_path if cls_path and os.path.exists(cls_path) else None,
                reg_path if reg_path and os.path.exists(reg_path) else None)

    @staticmethod
    def _infer_period_from_name(name: str) -> Optional[str]:
        if not name:
            return None
        lowered = name.lower()
        for token in ("30d", "20d", "15d", "10d", "7d", "5d"):
            if token in lowered:
                return token
        return None

    @staticmethod
    def _infer_task_from_name(name: str) -> Optional[str]:
        if not name:
            return None
        lowered = name.lower()
        if any(key in lowered for key in ("cls", "class", "classification")):
            return "classification"
        if any(key in lowered for key in ("reg", "regr", "regression")):
            return "regression"
        return None

    def _as_model_bundle(self, data: Any, source: str, filename: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """å°†æ¨¡å‹artifactè½¬æ¢ä¸ºç»Ÿä¸€ç»“æ„ï¼Œä¾¿äºåç»­å¤„ç†ã€‚"""
        bundle: Dict[str, Any] = {
            'artifact': None,
            'is_v2': False,
            'task': None,
            'period': None,
            'priority': 0,
            'pipeline': None,
            'fitted_model': None,
            'model': None,
            'calibrator': None,
            'selected_features': [],
            'feature_names': [],
            'metadata': {},
            'metrics': {},
            'is_best': False,
            'source': source,
            'name': filename or source
        }

        artifact = data
        task = None
        period = None
        priority = 0

        try:
            if isinstance(data, dict):
                if 'pipeline' in data:  # V2æ ¼å¼
                    bundle['is_v2'] = True
                    task = data.get('task')
                    config_period = data.get('config', {}).get('prediction_period')
                    if config_period:
                        period = f"{config_period}d" if isinstance(config_period, (int, float)) else str(config_period)
                    bundle['pipeline'] = data.get('pipeline')
                    bundle['fitted_model'] = data.get('pipeline')
                    bundle['model'] = data.get('pipeline')
                    bundle['calibrator'] = data.get('calibrator')
                    bundle['selected_features'] = list(data.get('selected_features') or [])
                    bundle['feature_names'] = list(data.get('selected_features') or [])
                    bundle['metadata'] = {
                        'task': data.get('task'),
                        'model_type': data.get('model_type'),
                        'training_date': data.get('training_date'),
                        'is_best': data.get('is_best'),
                        'is_v2': True,
                        'config': data.get('config', {}),
                        'metrics': data.get('metrics', {})
                    }
                    bundle['metrics'] = data.get('metrics', {})
                    bundle['is_best'] = bool(data.get('is_best'))
                    priority = 100 if bundle['is_best'] else 50
                else:  # V1æ ¼å¼
                    task = data.get('metadata', {}).get('task') or data.get('metadata', {}).get('type')
                    period = data.get('metadata', {}).get('period')
                    bundle['model'] = data.get('model')
                    bundle['fitted_model'] = data.get('model')
                    bundle['calibrator'] = data.get('calibrator')
                    bundle['feature_names'] = list(data.get('feature_names') or [])
                    bundle['metadata'] = {**data.get('metadata', {}), 'is_v2': False, 'metrics': data.get('metrics', {})}
                    bundle['metrics'] = data.get('metrics', {})
                    artifact = data
                    priority = 10
            else:
                # æœ€æ—§æ ¼å¼ï¼šç›´æ¥å­˜å‚¨æ¨¡å‹/ç®¡é“å¯¹è±¡
                task = None
                period = None
                wrapper: Dict[str, Any] = {'model': data, 'feature_names': []}
                if hasattr(data, 'feature_names_in_'):
                    wrapper['feature_names'] = list(getattr(data, 'feature_names_in_'))
                artifact = wrapper
                bundle['model'] = data
                bundle['fitted_model'] = data
                bundle['feature_names'] = list(wrapper['feature_names'])
                priority = 5

            if task is None and filename:
                task = self._infer_task_from_name(filename)
            if period is None and filename:
                period = self._infer_period_from_name(filename)

            bundle['artifact'] = artifact
            bundle['task'] = task
            bundle['period'] = period
            bundle['priority'] = priority

            if bundle['is_v2'] and bundle['pipeline'] is None:
                raise ValueError("V2æ¨¡å‹ç¼ºå°‘pipelineå­—æ®µ")

            # è‹¥ä»ç¼ºå°‘å…³é”®å­—æ®µåˆ™è¿”å›None
            if bundle['task'] is None:
                return None

            return bundle
        except Exception as parse_err:
            logger.debug(f"è§£ææ¨¡å‹artifactå¤±è´¥ ({source}): {parse_err}")
            return None

    def _load_bundle_from_path(self, path: str, source_label: str, expected_period: str) -> Optional[Dict[str, Any]]:
        if not path or not os.path.exists(path):
            return None
        try:
            data = joblib.load(path)
            bundle = self._as_model_bundle(data, source_label, filename=os.path.basename(path))
            if not bundle:
                logger.warning(f"å¿½ç•¥æ¨¡å‹ {path}ï¼šæ— æ³•è§£æ")
                return None
            if bundle['period'] and expected_period and bundle['period'] != expected_period:
                logger.warning(f"å¿½ç•¥æ¨¡å‹ {path}ï¼šå‘¨æœŸ {bundle['period']} ä¸æœŸæœ› {expected_period} ä¸åŒ¹é…")
                return None
            return bundle
        except Exception as err:
            logger.warning(f"åŠ è½½æ¨¡å‹å¤±è´¥ {path}: {err}")
            return None

    def _apply_model_bundle(self, bundle: Dict[str, Any], model_kind: str) -> None:
        if not bundle:
            return

        artifact = bundle.get('artifact')
        features = list(bundle.get('feature_names') or [])
        selected_features = list(bundle.get('selected_features') or [])
        fitted_model = bundle.get('fitted_model')
        pipeline = bundle.get('pipeline')
        metadata = bundle.get('metadata') or {}
        calibrator = bundle.get('calibrator')

        if not features and selected_features:
            features = selected_features

        # è‹¥ä»æ— ç‰¹å¾åï¼Œå°è¯•ä»æ¨¡å‹å¯¹è±¡è·å–
        if not features and fitted_model is not None and hasattr(fitted_model, 'feature_names_in_'):
            try:
                features = list(getattr(fitted_model, 'feature_names_in_'))
            except Exception:
                features = []

        # è‹¥ç‰¹å¾åä»ä¸ºç©ºæˆ–ä¸ºåŒ¿å feature_ åºåˆ—ï¼Œä»ç¼“å­˜è¡¥å……
        def _maybe_inject_from_cache(current: List[str], cache_path: str) -> List[str]:
            if current and not all(str(name).startswith('feature_') for name in current):
                return current
            if not os.path.exists(cache_path):
                logger.debug(f"ç‰¹å¾ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
                return current
            try:
                with open(cache_path, 'r') as fc:
                    cached = json.load(fc)
                if not current or len(cached) == len(current):
                    logger.info(f"å·²ä»ç¼“å­˜æ³¨å…¥ç‰¹å¾åï¼Œå…± {len(cached)} ä¸ª")
                    return cached
            except Exception as cache_err:
                logger.debug(f"è¯»å–ç‰¹å¾ç¼“å­˜å¤±è´¥: {cache_err}")
            return current

        cache_file = os.path.join('feature_cache', 'selected_features.json')
        features = _maybe_inject_from_cache(features, cache_file)

        # æ›´æ–°æ¨¡å‹å¯¹è±¡çš„ feature_names_in_
        def _update_feature_names(model_obj: Any, names: List[str]) -> None:
            if not model_obj or not names:
                return
            try:
                model_obj.feature_names_in_ = np.array(names)
            except Exception:
                pass

        _update_feature_names(fitted_model, features)
        if pipeline is not None and pipeline is not fitted_model:
            _update_feature_names(pipeline, features)

        # å†™å› bundleï¼Œä¾¿äºåç»­ä»»åŠ¡ä½¿ç”¨
        bundle['feature_names'] = features

        if model_kind == 'cls':
            self.cls_model_bundle = bundle
            self.cls_model_data = artifact
            self.cls_feature_names = features
            self.cls_metadata = metadata
            self.cls_calibrator = calibrator
        else:
            self.reg_model_bundle = bundle
            self.reg_model_data = artifact
            self.reg_feature_names = features
            self.reg_metadata = metadata
            self.reg_calibrator = calibrator

        metrics = bundle.get('metrics') or {}
        model_type = metadata.get('model_type') or bundle.get('name')
        if bundle.get('is_v2') and metrics:
            try:
                if model_kind == 'cls' and 'val_auc' in metrics:
                    logger.info(f"  V2æ¨¡å‹ {model_type} éªŒè¯AUC={metrics['val_auc']:.4f}")
                elif model_kind == 'reg' and ('val_r2' in metrics or 'val_mse' in metrics):
                    if 'val_r2' in metrics:
                        logger.info(f"  V2æ¨¡å‹ {model_type} éªŒè¯RÂ²={metrics['val_r2']:.4f}")
                    else:
                        logger.info(f"  V2æ¨¡å‹ {model_type} éªŒè¯MSE={metrics['val_mse']:.4f}")
            except Exception:
                pass

    # æ–°å¢ï¼šåŒæ—¶åŠ è½½30dçš„åˆ†ç±»ä¸å›å½’æ¨¡å‹
    def load_models(self, period: str = '30d') -> bool:
        try:
            models_dir = "models"
            if not os.path.exists(models_dir):
                logger.warning(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
                return 0
            cls_candidates: List[Dict[str, Any]] = []
            reg_candidates: List[Dict[str, Any]] = []
            for fname in sorted([f for f in os.listdir(models_dir) if f.endswith('.pkl')]):
                fpath = os.path.join(models_dir, fname)
                try:
                    data = joblib.load(fpath)
                except Exception as load_err:
                    logger.debug(f"è¯»å–æ¨¡å‹å¤±è´¥ {fpath}: {load_err}")
                    continue

                bundle = self._as_model_bundle(data, source=f"file:{fname}", filename=fname)
                if not bundle:
                    continue
                if bundle['period'] and period and bundle['period'] != period:
                    continue

                if bundle['task'] == 'classification':
                    cls_candidates.append(bundle)
                elif bundle['task'] == 'regression':
                    reg_candidates.append(bundle)
            # ========= æ–°å¢: ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶/ç¯å¢ƒå˜é‡æŒ‡å®šæ¨¡å‹ =========
            cls_cfg_path, reg_cfg_path = self._get_config_model_paths()
            cfg_cls_bundle = self._load_bundle_from_path(cls_cfg_path, 'config:classification', period) if cls_cfg_path else None
            cfg_reg_bundle = self._load_bundle_from_path(reg_cfg_path, 'config:regression', period) if reg_cfg_path else None

            if cfg_cls_bundle:
                logger.info(f"âœ… æ ¹æ®é…ç½®åŠ è½½åˆ†ç±»æ¨¡å‹: {cfg_cls_bundle['name']}")
                self._apply_model_bundle(cfg_cls_bundle, 'cls')
            if cfg_reg_bundle:
                logger.info(f"âœ… æ ¹æ®é…ç½®åŠ è½½å›å½’æ¨¡å‹: {cfg_reg_bundle['name']}")
                self._apply_model_bundle(cfg_reg_bundle, 'reg')
            if cfg_cls_bundle or cfg_reg_bundle:
                return bool(self.cls_model_data or self.reg_model_data)

            if cls_candidates:
                cls_candidates.sort(key=lambda x: x['priority'], reverse=True)
                chosen_cls = cls_candidates[0]
                logger.info(f"âœ… é€‰æ‹©åˆ†ç±»æ¨¡å‹: {chosen_cls['name']} (ä¼˜å…ˆçº§={chosen_cls['priority']})")
                self._apply_model_bundle(chosen_cls, 'cls')
            else:
                logger.warning(f"æœªæ‰¾åˆ°{period}åˆ†ç±»æ¨¡å‹")

            if reg_candidates:
                reg_candidates.sort(key=lambda x: x['priority'], reverse=True)
                chosen_reg = reg_candidates[0]
                logger.info(f"âœ… é€‰æ‹©å›å½’æ¨¡å‹: {chosen_reg['name']} (ä¼˜å…ˆçº§={chosen_reg['priority']})")
                self._apply_model_bundle(chosen_reg, 'reg')
            else:
                logger.warning(f"æœªæ‰¾åˆ°{period}å›å½’æ¨¡å‹")

            return bool(self.cls_model_data or self.reg_model_data)
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹é›†åˆå¤±è´¥: {e}")
            return 0
    
    def _load_separate_scaler(self, scaler_path: str = None) -> bool:
        """
        åŠ è½½å•ç‹¬çš„æ ‡å‡†åŒ–å™¨æ–‡ä»¶
        """
        try:
            if scaler_path is None:
                models_dir = "models"
                scaler_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl') and 'scaler' in f]
                if scaler_files:
                    scaler_files.sort()
                    latest_scaler = scaler_files[-1]
                    scaler_path = os.path.join(models_dir, latest_scaler)
                else:
                    return 0
                    
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
                logger.info(f"æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {scaler_path}")
                return 1
            else:
                return 0
        except Exception as e:
            logger.error(f"æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
            return 0

    def _fetch_prices_bulk(
        self,
        symbols: List[str],
        lookback_days: int = 180,
    ) -> Dict[str, pd.DataFrame]:
        """ä½¿ç”¨ç»Ÿä¸€æ•°æ®è®¿é—®å±‚æ‰¹é‡è·å–å†å²æ•°æ®ï¼Œå¹¶ç»Ÿä¸€åˆ—åã€‚"""

        if not symbols or self.data_access is None:
            return {}

        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=lookback_days)

        try:
            data_map = self.data_access.get_bulk_stock_data(
                symbols,
                start_date.strftime("%Y-%m-%d"),
                end_date.strftime("%Y-%m-%d"),
                # åˆ é™¤ turnover å­—æ®µä»¥å…¼å®¹å½“å‰æ•°æ®åº“ç»“æ„
                fields=["open", "high", "low", "close", "volume", "amount"],
                force_refresh=False,
                auto_sync=False,
            )
        except Exception as exc:  # pragma: no cover - å›é€€åˆ°æ—§é€»è¾‘
            logger.warning(f"æ‰¹é‡è·å–å†å²æ•°æ®å¤±è´¥ï¼Œå›é€€æ•°æ®åº“æ¥å£: {exc}")
            return {}

        normalized: Dict[str, pd.DataFrame] = {}
        for symbol, raw_df in (data_map or {}).items():
            if raw_df is None or raw_df.empty:
                continue

            df = raw_df.copy()
            if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
            else:
                df = df.reset_index()
                if "index" in df.columns:
                    df.rename(columns={"index": "date"}, inplace=True)
                df["date"] = pd.to_datetime(df["date"])

            df = df.sort_values("date")
            df["symbol"] = symbol

            rename_map = {
                "open": "Open",
                "high": "High",
                "low": "Low",
                "close": "Close",
                "volume": "Volume",
                "turnover": "Turnover",
                "amount": "Amount",
            }
            for src_col, dst_col in rename_map.items():
                if src_col in df.columns and dst_col not in df.columns:
                    df[dst_col] = df[src_col]

            for col in ("Open", "High", "Low", "Close", "Volume"):
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

            normalized[symbol] = df

        return normalized

    def get_latest_features_v2(self, symbols: List[str], 
                              end_date: Optional[str] = None) -> pd.DataFrame:
        """
        ä½¿ç”¨UnifiedFeatureBuilderè·å–V2æ¨¡å‹æ‰€éœ€çš„ç‰¹å¾
        é€‚ç”¨äºV2æ¨¡å‹ï¼ˆåŸºäºä»·é‡+å¸‚åœº+è¡Œä¸šç‰¹å¾ï¼‰
        """
        if self.unified_feature_builder is None:
            logger.error("UnifiedFeatureBuilder æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ„å»ºV2ç‰¹å¾")
            return pd.DataFrame()
        
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨UnifiedFeatureBuilderä¸º {len(symbols)} åªè‚¡ç¥¨æ„å»ºç‰¹å¾...")
            
            # ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾æ„å»ºå™¨
            features_df = self.unified_feature_builder.build_features(
                symbols=symbols,
                as_of_date=end_date,
                return_labels=False
            )
            
            if features_df.empty:
                logger.warning("UnifiedFeatureBuilder è¿”å›ç©ºç»“æœ")
                return pd.DataFrame()
            
            # ç¡®ä¿æœ‰symbolåˆ—
            if 'symbol' not in features_df.columns:
                logger.error("ç‰¹å¾DataFrameç¼ºå°‘symbolåˆ—")
                return pd.DataFrame()
            
            logger.info(f"âœ… V2ç‰¹å¾æ„å»ºå®Œæˆ: {len(features_df)} è¡Œ x {len(features_df.columns)} åˆ—")
            logger.debug(f"ç‰¹å¾åˆ—: {list(features_df.columns[:20])}...")
            
            return features_df
            
        except Exception as e:
            logger.error(f"V2ç‰¹å¾æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def get_latest_features(self, symbols: List[str], 
                           end_date: Optional[str] = None) -> pd.DataFrame:
        """è·å–æœ€æ–°ç‰¹å¾æ•°æ®ï¼Œå¹¶ä¼˜å…ˆèµ°ç»Ÿä¸€æ•°æ®è®¿é—®å±‚çš„æ‰¹é‡ç¼“å­˜ã€‚"""

        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')

        all_features: List[pd.Series] = []

        def build_feature(symbol: str, prices: pd.DataFrame) -> Optional[pd.Series]:
            if prices is None or prices.empty or len(prices) < 20:
                return None

            prices = prices.copy()
            if 'date' in prices.columns:
                prices['date'] = pd.to_datetime(prices['date'])
            prices = prices.sort_values('date')

            try:
                prices = FieldMapper.normalize_fields(prices, 'prices_daily')
                prices = FieldMapper.ensure_required_fields(prices, 'prices_daily')
            except Exception as fm_err:
                logger.debug(f"FieldMapperå¤„ç†å¤±è´¥: {fm_err}")

            for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
                if col in prices.columns:
                    prices[col] = pd.to_numeric(prices[col], errors='coerce').astype(float)

            factor_features = self.feature_generator.calculate_factor_features(prices)
            if factor_features.empty:
                return None

            feature_dict = factor_features.iloc[-1].to_dict()
            feature_dict['symbol'] = symbol
            return pd.Series(feature_dict)

        batch_size = 50
        for i in range(0, len(symbols), batch_size):
            batch_symbols = symbols[i:i + batch_size]
            if i == 0 or (i + batch_size) % 200 == 0 or i + batch_size >= len(symbols):
                logger.info(
                    "æ‰¹é‡è·å–è‚¡ç¥¨å†å²æ•°æ®: %s-%s/%s",
                    i + 1,
                    min(i + batch_size, len(symbols)),
                    len(symbols),
                )

            price_map = self._fetch_prices_bulk(batch_symbols)
            missing_symbols = [sym for sym in batch_symbols if sym not in price_map]

            fallback_prices = pd.DataFrame()
            if missing_symbols:
                fallback_prices = self.db.get_last_n_bars(missing_symbols, n=180)

            for symbol in batch_symbols:
                prices = price_map.get(symbol)
                if prices is None and not fallback_prices.empty:
                    subset = fallback_prices[fallback_prices['symbol'] == symbol].copy()
                    if not subset.empty:
                        subset = subset.sort_values('date')
                        prices = subset

                feature_series = build_feature(symbol, prices)
                if feature_series is not None:
                    all_features.append(feature_series)

        if not all_features:
            logger.warning("æ‰¹é‡ç‰¹å¾è®¡ç®—ç»“æœä¸ºç©ºï¼Œå°è¯•é€è‚¡ç¥¨å›é€€")
            for symbol in symbols:
                prices = self.db.get_last_n_bars([symbol], n=180)
                prices = prices[prices['symbol'] == symbol].copy() if not prices.empty else pd.DataFrame()
                feature_series = build_feature(symbol, prices)
                if feature_series is not None:
                    all_features.append(feature_series)

        if all_features:
            result = pd.DataFrame(all_features)
            logger.info(f"ç‰¹å¾è·å–å®Œæˆï¼Œå…±å¤„ç† {len(result)} åªè‚¡ç¥¨")
            return result

        logger.warning("ç‰¹å¾è·å–ç»“æœä¸ºç©º")
        return pd.DataFrame()
    
    def predict_stocks(self, symbols: List[str], top_n: int = 10) -> List[Dict[str, Any]]:
        """
        é¢„æµ‹è‚¡ç¥¨å¹¶è¿”å›æ’åºç»“æœ
        """
        # å…è®¸æ¨¡å‹å­˜åœ¨ä½†å¤–éƒ¨scalerç¼ºå¤±ï¼ˆè‹¥æ¨¡å‹æ˜¯åŒ…å«scalerçš„Pipelineï¼‰
        if not (self.cls_model_data or self.reg_model_data or self.model):
            logger.error("æ¨¡å‹æœªåŠ è½½")
            return []
        
        # ï¿½ æ€§èƒ½ä¼˜åŒ–: é™åˆ¶è‚¡ç¥¨æ± å¤§å°ï¼Œé¿å…å¤„ç†å…¨å¸‚åœº
        original_count = len(symbols)
        if original_count > MAX_STOCK_POOL_SIZE:
            logger.warning(
                f"ğŸ“Š è‚¡ç¥¨æ± ä¼˜åŒ–: åŸå§‹={original_count}åª â†’ é™åˆ¶={MAX_STOCK_POOL_SIZE}åª "
                f"(å¯é€šè¿‡ç¯å¢ƒå˜é‡ MAX_STOCK_POOL_SIZE è°ƒæ•´)"
            )
            symbols = symbols[:MAX_STOCK_POOL_SIZE]
        else:
            logger.info(f"ğŸ“Š è‚¡ç¥¨æ± å¤§å°: {original_count}åª (é™åˆ¶={MAX_STOCK_POOL_SIZE}åª)")
        
        # ï¿½ğŸ”§ æ£€æµ‹æ˜¯å¦ä¸ºV2æ¨¡å‹
        # V2æ¨¡å‹çš„metadataå­˜å‚¨åœ¨å•ç‹¬çš„å˜é‡ä¸­
        is_v2_cls = False
        if self.cls_model_data and isinstance(self.cls_model_data, dict):
            # æ£€æŸ¥artifactæœ¬èº«çš„is_v2æ ‡è®°
            if 'pipeline' in self.cls_model_data and 'task' in self.cls_model_data:
                is_v2_cls = True
                logger.debug("æ£€æµ‹åˆ°V2åˆ†ç±»æ¨¡å‹ï¼ˆåŸºäºpipelineå’Œtaskå­—æ®µï¼‰")
            # æ£€æŸ¥metadata
            elif hasattr(self, 'cls_metadata') and self.cls_metadata.get('is_v2'):
                is_v2_cls = True
                logger.debug("æ£€æµ‹åˆ°V2åˆ†ç±»æ¨¡å‹ï¼ˆåŸºäºcls_metadataï¼‰")
        
        is_v2_reg = False
        if self.reg_model_data and isinstance(self.reg_model_data, dict):
            if 'pipeline' in self.reg_model_data and 'task' in self.reg_model_data:
                is_v2_reg = True
                logger.debug("æ£€æµ‹åˆ°V2å›å½’æ¨¡å‹ï¼ˆåŸºäºpipelineå’Œtaskå­—æ®µï¼‰")
            elif hasattr(self, 'reg_metadata') and self.reg_metadata.get('is_v2'):
                is_v2_reg = True
                logger.debug("æ£€æµ‹åˆ°V2å›å½’æ¨¡å‹ï¼ˆåŸºäºreg_metadataï¼‰")
        
        is_v2_model = is_v2_cls or is_v2_reg
        logger.info(f"æ¨¡å‹ç‰ˆæœ¬æ£€æµ‹: V2åˆ†ç±»={is_v2_cls}, V2å›å½’={is_v2_reg}")
        
        # ğŸ”§ æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç‰¹å¾æ„å»ºæ–¹æ³•
        if is_v2_model:
            logger.info("æ£€æµ‹åˆ°V2æ¨¡å‹ï¼Œä½¿ç”¨UnifiedFeatureBuilderæ„å»ºç‰¹å¾")
            features_df = self.get_latest_features_v2(symbols)
        else:
            logger.info("ä½¿ç”¨ä¼ ç»Ÿç‰¹å¾æ„å»ºæ–¹æ³•")
            features_df = self.get_latest_features(symbols)
            
        if features_df.empty:
            logger.warning("æ— æ³•è·å–ç‰¹å¾æ•°æ®")
            return []
            
        # -------- ç»Ÿä¸€æ•°å€¼ç±»å‹ï¼Œé¿å…ç±»å‹ä¸ä¸€è‡´ --------
        # æ³¨æ„ï¼šä¸å†ç»Ÿä¸€åˆ—åå¤§å°å†™ï¼ŒV2æ¨¡å‹ä½¿ç”¨åŸå§‹å¤§å°å†™ç‰¹å¾åï¼ˆå¦‚ADV_20ï¼‰
        try:
            for col in features_df.columns:
                if col == 'symbol':
                    continue
                # å°è¯•å°†åˆ—è½¬æ¢ä¸ºfloatï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™è®¾ä¸ºNaN
                features_df[col] = pd.to_numeric(features_df[col], errors='coerce').astype(float)
        except Exception as e:
            logger.debug(f"ç‰¹å¾åˆ—ç±»å‹ç»Ÿä¸€å¤±è´¥: {e}")
        # ----------------------------------------------------------------------
        
        # é’ˆå¯¹åˆ†ç±»ä¸å›å½’åˆ†åˆ«å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        probs = np.array([0.5] * len(features_df))
        preds_cls = np.array([0] * len(features_df))
        exp_returns = np.array([0.0] * len(features_df))
        
        # åˆ†ç±»æ¨¡å‹é¢„æµ‹
        try:
            if self.cls_model_data:
                # V2æ ¼å¼ä½¿ç”¨'pipeline'ï¼ŒV1æ ¼å¼ä½¿ç”¨'model'
                if isinstance(self.cls_model_data, dict) and 'pipeline' in self.cls_model_data:
                    model = self.cls_model_data['pipeline']
                    # V2æ£€æµ‹ï¼šæœ‰pipelineå’Œtaskå­—æ®µå³ä¸ºV2
                    is_v2 = 'task' in self.cls_model_data
                    logger.debug(f"ä½¿ç”¨V2 pipelineè¿›è¡Œåˆ†ç±»é¢„æµ‹ (is_v2={is_v2})")
                elif isinstance(self.cls_model_data, dict):
                    model = self.cls_model_data['model']
                    is_v2 = False
                else:
                    model = self.cls_model_data
                    is_v2 = False
                
                # ğŸ”§ V2æ¨¡å‹ï¼šä½¿ç”¨selected_featuresæˆ–pipelineçš„feature_names_in_
                if is_v2:
                    selected_features = self.cls_model_data.get('selected_features')
                    
                    # å¦‚æœselected_featuresä¸ºNoneï¼Œå°è¯•ä»pipelineè·å–
                    if selected_features is None or len(selected_features) == 0:
                        pipeline = self.cls_model_data.get('pipeline')
                        if hasattr(pipeline, 'feature_names_in_'):
                            expected_features = list(pipeline.feature_names_in_)
                            logger.info(f"V2æ¨¡å‹ä»pipelineè·å–ç‰¹å¾: {len(expected_features)} ä¸ª")
                        else:
                            # ä½¿ç”¨è®­ç»ƒé…ç½®ä¸­çš„ç‰¹å¾åˆ—è¡¨
                            logger.warning("V2æ¨¡å‹æ— selected_featuresä¸”pipelineæ— feature_names_in_ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼ç‰¹å¾")
                            expected_features = [c for c in Xc.columns if c != 'symbol' and np.issubdtype(Xc[c].dtype, np.number)]
                    else:
                        expected_features = selected_features
                        logger.info(f"V2æ¨¡å‹ä½¿ç”¨selected_features: {len(expected_features)} ä¸ªç‰¹å¾")
                else:
                    # V1æ¨¡å‹ï¼šä½¿ç”¨cls_feature_names
                    expected_features = self.cls_feature_names or []
                    logger.debug(f"V1æ¨¡å‹ä½¿ç”¨feature_names: {len(expected_features)} ä¸ªç‰¹å¾")
                
                Xc = features_df.copy()
                
                # å½“æœªæä¾›ç‰¹å¾åæ—¶ï¼Œå›é€€ä¸ºä½¿ç”¨æ•°å€¼å‹ç‰¹å¾ï¼ˆæ’é™¤symbolï¼‰
                if not expected_features:
                    candidate_cols = [c for c in Xc.columns if c != 'symbol']
                    expected_features = [c for c in candidate_cols if np.issubdtype(Xc[c].dtype, np.number)]
                    logger.warning(f"æœªæ‰¾åˆ°ç‰¹å¾åˆ—è¡¨ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼ç‰¹å¾: {len(expected_features)} ä¸ª")
                
                # æ£€æŸ¥ç‰¹å¾å¯ç”¨æ€§
                available = [c for c in expected_features if c in Xc.columns]
                missing = [c for c in expected_features if c not in Xc.columns]
                
                if missing:
                    logger.warning(f"ç¼ºå¤± {len(missing)} ä¸ªç‰¹å¾: {missing[:10]}...")
                
                # V2æ¨¡å‹ï¼šå¦‚æœç¼ºå¤±ç‰¹å¾è¿‡å¤šï¼ŒæŠ¥é”™è€Œä¸æ˜¯å›é€€
                if is_v2 and len(available) < len(expected_features) * 0.8:
                    logger.error(f"V2æ¨¡å‹ç¼ºå¤±è¿‡å¤šç‰¹å¾ ({len(available)}/{len(expected_features)})")
                    logger.error(f"å¯ç”¨ç‰¹å¾: {available[:10]}...")
                    logger.error(f"ç‰¹å¾DataFrameåˆ—: {list(Xc.columns[:10])}...")
                    # ä¸å›é€€ï¼Œç›´æ¥ä½¿ç”¨å¯ç”¨ç‰¹å¾
                    expected_features = available
                
                # V1æ¨¡å‹ï¼šå…è®¸å›é€€
                if not is_v2 and len(available) <= max(3, len(expected_features)*0.3):
                    logger.warning(f"V1æ¨¡å‹æ‰€éœ€ç‰¹å¾ç¼ºå¤±ä¸¥é‡({len(available)}/{len(expected_features)}), å›é€€åˆ°æ•°å€¼ç‰¹å¾å…¨é›†")
                    expected_features = [c for c in Xc.columns if c != 'symbol' and np.issubdtype(Xc[c].dtype, np.number)]
                
                # ç¡®ä¿æ‰€æœ‰æœŸæœ›ç‰¹å¾å‡å­˜åœ¨
                for col in expected_features:
                    if col not in Xc.columns:
                        Xc[col] = 0
                
                Xc = Xc[expected_features].fillna(0)
                
                # æ–¹å·®æ£€æŸ¥ï¼ˆä»…å¯¹V1æ¨¡å‹ï¼‰
                if not is_v2 and np.isclose(Xc.var().sum(), 0):
                    logger.error("V1åˆ†ç±»ç‰¹å¾çŸ©é˜µæ–¹å·®ä¸º0ï¼Œå›é€€åˆ°åŸå§‹æ•°å€¼ç‰¹å¾å…¨é›†")
                    Xc = features_df[[c for c in features_df.columns if c != 'symbol' and np.issubdtype(features_df[c].dtype, np.number)]].fillna(0)
                
                # ğŸ”§ V2æ¨¡å‹ï¼šPipelineå·²åŒ…å«é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨
                if is_v2:
                    logger.debug("V2æ¨¡å‹ä½¿ç”¨pipelineç›´æ¥é¢„æµ‹ï¼ˆpipelineåŒ…å«é¢„å¤„ç†ï¼‰")
                    Xc_input = Xc
                    
                    # V2æ¨¡å‹ï¼šä½¿ç”¨calibratorï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    calibrator = self.cls_model_data.get('calibrator')
                    if calibrator is not None:
                        logger.debug("ä½¿ç”¨V2 calibratorè¿›è¡Œæ ¡å‡†é¢„æµ‹")
                        # calibratorå·²ç»åŒ…è£…äº†å®Œæ•´çš„pipelineï¼Œç›´æ¥ä¼ å…¥æ•°æ®
                        probs = calibrator.predict_proba(Xc_input)[:, 1]
                        preds_cls = calibrator.predict(Xc_input)
                    else:
                        logger.debug("V2æ¨¡å‹æ— calibratorï¼Œä½¿ç”¨pipelineç›´æ¥é¢„æµ‹")
                        probs = model.predict_proba(Xc_input)[:, 1]
                        preds_cls = model.predict(Xc_input)
                
                # V1æ¨¡å‹ï¼šåº”ç”¨å¢å¼ºé¢„å¤„ç†æˆ–ä¼ ç»Ÿé¢„å¤„ç†
                elif self.use_enhanced_preprocessing and self.cls_preprocessor is not None:
                    logger.info("V1æ¨¡å‹ä½¿ç”¨å¢å¼ºé¢„å¤„ç†pipeline")
                    Xc_input = self.cls_preprocessor.transform(Xc)
                    probs = model.predict_proba(Xc_input)[:, 1]
                    preds_cls = model.predict(Xc_input)
                else:
                    # V1åŸæœ‰çš„é¢„å¤„ç†é€»è¾‘
                    use_pipeline_scaler = hasattr(model, 'named_steps') and 'scaler' in getattr(model, 'named_steps', {})
                    if use_pipeline_scaler:
                        Xc_input = Xc
                    else:
                        scaler = self.cls_model_data.get('scaler')
                        if scaler is None:
                            try:
                                col_std = Xc.std().replace(0, 1e-6)
                                Xc_norm = (Xc - Xc.mean()) / col_std
                                Xc_input = Xc_norm.clip(-5, 5).fillna(0)
                                logger.info("V1æ¨¡å‹åœ¨çº¿æ ‡å‡†åŒ–")
                            except Exception:
                                logger.warning("åœ¨çº¿æ ‡å‡†åŒ–å¤±è´¥")
                                Xc_input = Xc
                        else:
                            Xc_input = scaler.transform(Xc)
                    
                    probs = model.predict_proba(Xc_input)[:, 1]
                    preds_cls = model.predict(Xc_input)
                
            elif self.model:
                # å…¼å®¹æ—§é€»è¾‘
                if self.feature_names:
                    for col in self.feature_names:
                        if col not in features_df.columns:
                            features_df[col] = 0
                    X_old = features_df[self.feature_names].fillna(0)
                else:
                    feature_cols = [c for c in features_df.columns if c != 'symbol']
                    X_old = features_df[feature_cols].fillna(0)
                use_pipeline_scaler = hasattr(self.model, 'named_steps') and 'scaler' in getattr(self.model, 'named_steps', {})
                X_input = X_old if use_pipeline_scaler else (self.scaler.transform(X_old) if self.scaler else X_old)
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒpredict_proba
                if hasattr(self.model, 'predict_proba'):
                    probs = self.model.predict_proba(X_input)[:, 1]
                else:
                    # å¯¹äºRidgeç­‰å›å½’æ¨¡å‹ï¼Œä½¿ç”¨decision_functionæˆ–predictè½¬æ¢ä¸ºæ¦‚ç‡
                    if hasattr(self.model, 'decision_function'):
                        scores = self.model.decision_function(X_input)
                        # ä½¿ç”¨sigmoidå‡½æ•°å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡
                        probs = 1 / (1 + np.exp(-scores))
                    else:
                        # ä½¿ç”¨predictç»“æœï¼Œå‡è®¾æ˜¯è¿ç»­å€¼ï¼Œè½¬æ¢ä¸ºæ¦‚ç‡
                        predictions_raw = self.model.predict(X_input)
                        # ä½¿ç”¨tanhå‡½æ•°å°†é¢„æµ‹å€¼æ˜ å°„åˆ°æ¦‚ç‡ç©ºé—´
                        probs = 0.5 + 0.4 * np.tanh(predictions_raw)
                
                preds_cls = self.model.predict(X_input)
                # å¯¹äºå›å½’æ¨¡å‹ï¼Œå°†è¿ç»­é¢„æµ‹è½¬æ¢ä¸ºåˆ†ç±»
                if not hasattr(self.model, 'predict_proba'):
                    preds_cls = (preds_cls > 0).astype(int)
                    
        except Exception as e:
            logger.error(f"åˆ†ç±»é¢„æµ‹å¤±è´¥: {e}")
        
        # å›å½’æ¨¡å‹é¢„æµ‹ï¼ˆé¢„æœŸæ”¶ç›Šï¼‰
        try:
            if self.reg_model_data:
                # V2æ ¼å¼ä½¿ç”¨'pipeline'ï¼ŒV1æ ¼å¼ä½¿ç”¨'model'
                if isinstance(self.reg_model_data, dict) and 'pipeline' in self.reg_model_data:
                    model_r = self.reg_model_data['pipeline']
                    is_v2_reg = 'task' in self.reg_model_data
                    logger.debug(f"ä½¿ç”¨V2 pipelineè¿›è¡Œå›å½’é¢„æµ‹ (is_v2={is_v2_reg})")
                elif isinstance(self.reg_model_data, dict):
                    model_r = self.reg_model_data['model']
                    is_v2_reg = False
                else:
                    model_r = self.reg_model_data
                    is_v2_reg = False
                
                # ğŸ”§ V2æ¨¡å‹ï¼šä½¿ç”¨selected_featuresæˆ–pipelineçš„feature_names_in_
                if is_v2_reg:
                    selected_features_r = self.reg_model_data.get('selected_features')
                    
                    # å¦‚æœselected_featuresä¸ºNoneï¼Œå°è¯•ä»pipelineè·å–
                    if selected_features_r is None or len(selected_features_r) == 0:
                        pipeline_r = self.reg_model_data.get('pipeline')
                        if hasattr(pipeline_r, 'feature_names_in_'):
                            expected_features_r = list(pipeline_r.feature_names_in_)
                            logger.info(f"V2å›å½’æ¨¡å‹ä»pipelineè·å–ç‰¹å¾: {len(expected_features_r)} ä¸ª")
                        else:
                            logger.warning("V2å›å½’æ¨¡å‹æ— selected_featuresä¸”pipelineæ— feature_names_in_ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼ç‰¹å¾")
                            expected_features_r = [c for c in Xr.columns if c != 'symbol' and np.issubdtype(Xr[c].dtype, np.number)]
                    else:
                        expected_features_r = selected_features_r
                        logger.info(f"V2å›å½’æ¨¡å‹ä½¿ç”¨selected_features: {len(expected_features_r)} ä¸ªç‰¹å¾")
                else:
                    expected_features_r = self.reg_feature_names or []
                    logger.debug(f"V1å›å½’æ¨¡å‹ä½¿ç”¨feature_names: {len(expected_features_r)} ä¸ªç‰¹å¾")
                
                Xr = features_df.copy()
                
                # å½“æœªæä¾›ç‰¹å¾åæ—¶ï¼Œå›é€€ä¸ºä½¿ç”¨æ•°å€¼å‹ç‰¹å¾ï¼ˆæ’é™¤symbolï¼‰
                if not expected_features_r:
                    candidate_cols = [c for c in Xr.columns if c != 'symbol']
                    expected_features_r = [c for c in candidate_cols if np.issubdtype(Xr[c].dtype, np.number)]
                    logger.warning(f"å›å½’æ¨¡å‹ç‰¹å¾åä¸ºç©ºï¼Œä½¿ç”¨æ•°å€¼ç‰¹å¾: {len(expected_features_r)} ä¸ª")
                
                # æ£€æŸ¥ç‰¹å¾å¯ç”¨æ€§
                available_r = [c for c in expected_features_r if c in Xr.columns]
                missing_r = [c for c in expected_features_r if c not in Xr.columns]
                
                if missing_r:
                    logger.warning(f"å›å½’æ¨¡å‹ç¼ºå¤± {len(missing_r)} ä¸ªç‰¹å¾: {missing_r[:10]}...")
                
                # V2æ¨¡å‹ï¼šå¦‚æœç¼ºå¤±ç‰¹å¾è¿‡å¤šï¼ŒæŠ¥é”™
                if is_v2_reg and len(available_r) < len(expected_features_r) * 0.8:
                    logger.error(f"V2å›å½’æ¨¡å‹ç¼ºå¤±è¿‡å¤šç‰¹å¾ ({len(available_r)}/{len(expected_features_r)})")
                    expected_features_r = available_r
                
                # V1æ¨¡å‹ï¼šå…è®¸å›é€€
                if not is_v2_reg and len(available_r) <= max(3, len(expected_features_r)*0.3):
                    logger.warning(f"V1å›å½’æ¨¡å‹æ‰€éœ€ç‰¹å¾ç¼ºå¤±ä¸¥é‡({len(available_r)}/{len(expected_features_r)}), å›é€€")
                    expected_features_r = [c for c in Xr.columns if c != 'symbol' and np.issubdtype(Xr[c].dtype, np.number)]
                
                # ç¡®ä¿æ‰€æœ‰æœŸæœ›ç‰¹å¾å‡å­˜åœ¨
                for col in expected_features_r:
                    if col not in Xr.columns:
                        Xr[col] = 0
                
                Xr = Xr[expected_features_r].fillna(0)
                
                # æ–¹å·®æ£€æŸ¥ï¼ˆä»…å¯¹V1æ¨¡å‹ï¼‰
                if not is_v2_reg and np.isclose(Xr.var().sum(), 0):
                    logger.error("V1å›å½’ç‰¹å¾çŸ©é˜µæ–¹å·®ä¸º0ï¼Œå›é€€")
                    Xr = features_df[[c for c in features_df.columns if c != 'symbol' and np.issubdtype(features_df[c].dtype, np.number)]].fillna(0)
                
                # ğŸ”§ V2æ¨¡å‹ï¼šPipelineå·²åŒ…å«é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨
                if is_v2_reg:
                    logger.debug("V2å›å½’æ¨¡å‹ä½¿ç”¨pipelineç›´æ¥é¢„æµ‹")
                    exp_returns = model_r.predict(Xr)
                
                # V1æ¨¡å‹ï¼šåº”ç”¨å¢å¼ºé¢„å¤„ç†æˆ–ä¼ ç»Ÿé¢„å¤„ç†
                elif self.use_enhanced_preprocessing and self.reg_preprocessor is not None:
                    logger.info("V1å›å½’æ¨¡å‹ä½¿ç”¨å¢å¼ºé¢„å¤„ç†pipeline")
                    Xr_input = self.reg_preprocessor.transform(Xr)
                    exp_returns = model_r.predict(Xr_input)
                else:
                    # V1åŸæœ‰çš„é¢„å¤„ç†é€»è¾‘
                    use_pipeline_scaler_r = hasattr(model_r, 'named_steps') and 'scaler' in getattr(model_r, 'named_steps', {})
                    scaler_r = self.reg_model_data.get('scaler')
                    if use_pipeline_scaler_r:
                        Xr_input = Xr
                    elif scaler_r is not None:
                        Xr_input = scaler_r.transform(Xr)
                    else:
                        # stackingæ¨¡å‹æ£€æµ‹
                        is_stacking_model = False
                        try:
                            meta = self.reg_model_data.get('metadata', {}) if self.reg_model_data else {}
                            is_stacking_model = (meta.get('model_type') == 'stacking')
                        except Exception:
                            pass
                        if is_stacking_model:
                            Xr_input = Xr
                            logger.info("stackingå›å½’æ¨¡å‹è·³è¿‡åœ¨çº¿æ ‡å‡†åŒ–")
                        else:
                            try:
                                col_std = Xr.std().replace(0, 1e-6)
                                Xr_norm = (Xr - Xr.mean()) / col_std
                                Xr_input = Xr_norm.clip(-5, 5).fillna(0)
                                logger.info("V1å›å½’æ¨¡å‹åœ¨çº¿æ ‡å‡†åŒ–")
                            except Exception:
                                logger.warning("åœ¨çº¿æ ‡å‡†åŒ–å¤±è´¥")
                                Xr_input = Xr
                    
                    exp_returns = model_r.predict(Xr_input)
                
                logger.info(f"å›å½’é¢„æµ‹æˆåŠŸï¼Œæ ·æœ¬æ•°: {len(exp_returns)}")
        except Exception as e:
            logger.error(f"å›å½’é¢„æµ‹å¤±è´¥: {e}")
            # è®¾ç½®é»˜è®¤è¿”å›å€¼
            exp_returns = np.array([0.0] * len(features_df))
            
        # å¦‚æœåˆ†ç±»æ¦‚ç‡è¿‡äºæç«¯ï¼Œå°è¯•ç”¨æŠ€æœ¯æŒ‡æ ‡è¿›è¡Œå¾®è°ƒ
        try:
            prob_std = np.std(probs)
            prob_mean = np.mean(probs)
            logger.info(f"æ¨¡å‹é¢„æµ‹æ¦‚ç‡ç»Ÿè®¡: å¹³å‡å€¼={prob_mean:.3f}, æ ‡å‡†å·®={prob_std:.3f}")
            
            # åº”ç”¨æ”¹è¿›çš„æ¦‚ç‡æ ¡å‡†ï¼Œä¼ å…¥é¢„æµ‹ç»“æœå’Œé¢„æœŸæ”¶ç›Š
            probs = self._calibrate_probabilities(probs, preds_cls, exp_returns)
            
            # å¦‚æœæ ¡å‡†åä»ç„¶æç«¯ï¼Œä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡è¿›ä¸€æ­¥è°ƒæ•´
            calibrated_std = np.std(probs)
            calibrated_mean = np.mean(probs)
            if calibrated_std < 0.05 or calibrated_mean > 0.9 or calibrated_mean < 0.1:
                logger.warning("æ ¡å‡†åæ¦‚ç‡ä»è¿‡äºæç«¯ï¼Œä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡è¿›ä¸€æ­¥è°ƒæ•´")
                adjusted_probabilities = self._adjust_probabilities_with_technical_indicators(
                    features_df['symbol'].tolist(), probs)
                probs = adjusted_probabilities
        except Exception as e:
            logger.warning(f"æ¦‚ç‡è°ƒæ•´å¤±è´¥: {e}")
            
        # ç»„åˆç»“æœ
        results = []
        # æ‹‰å–ä¸€æ¬¡å…¨é‡symbolä¿¡æ¯ï¼Œå‡å°‘å¾ªç¯å†…æŸ¥è¯¢
        symbols_data = {s['symbol']: s for s in self.db.list_symbols(markets=['SH','SZ'])}
        for i, symbol in enumerate(features_df['symbol']):
            stock_info = symbols_data.get(symbol, {})
            # è·å–æœ€æ–°ä»·æ ¼
            latest_bars = self.db.get_last_n_bars([symbol], n=1)
            latest_price = None
            if not latest_bars.empty:
                latest_price = {'close': latest_bars.iloc[-1]['close']}
            
            prob = float(probs[i]) if i < len(probs) else 0.5
            exp_ret = float(exp_returns[i]) if i < len(exp_returns) else 0.0
            predictions = preds_cls if i < len(preds_cls) else [0]

            # å½“æœªåŠ è½½å›å½’æ¨¡å‹æˆ–å›å½’è¾“å‡ºä¸ºç©º/æ¥è¿‘0æ—¶ï¼Œä½¿ç”¨åŸºäºè¿‘æœŸä»·æ ¼çš„å›é€€é€»è¾‘ä¼°è®¡30å¤©é¢„æœŸæ”¶ç›Š
            try:
                need_fallback_exp = (self.reg_model_data is None) or (np.isnan(exp_ret)) or (abs(exp_ret) < 1e-6)
            except Exception:
                need_fallback_exp = 1
            if need_fallback_exp:
                fb_ret = 0.0
                try:
                    bars_30 = self.db.get_last_n_bars([symbol], n=30)
                    if not bars_30.empty:
                        bars_30 = bars_30[bars_30['symbol'] == symbol].sort_values('date')
                        closes = bars_30['close'].astype(float).values
                        if len(closes) >= 5:
                            prev = closes[:-1].copy()
                            prev = np.where(prev == 0, np.nan, prev)
                            daily_ret = (closes[1:] - prev) / prev
                            daily_ret = daily_ret[np.isfinite(daily_ret)]
                            if len(daily_ret) >= 3:
                                mean_daily = float(np.nanmean(daily_ret))
                                # çº¦20ä¸ªäº¤æ˜“æ—¥å¯¹åº”30å¤©
                                expected_30 = mean_daily * 20.0
                                # ä¾æ®æ¦‚ç‡è¿›è¡Œæ¸©å’ŒåŠ æƒï¼ˆèŒƒå›´çº¦0.5~1.2ï¼‰
                                weight = 0.8 + (prob - 0.5)
                                weight = max(0.5, min(1.2, weight))
                                # å¼•å…¥åŸºäºæ³¢åŠ¨çš„æƒ©ç½šï¼Œæ³¢åŠ¨è¶Šé«˜æƒ©ç½šè¶Šå¤§ï¼ŒèŒƒå›´çº¦0.6~1.0
                                vol_daily = float(np.nanstd(daily_ret))
                                vol_penalty = 1.0 / (1.0 + 3.0 * vol_daily)
                                vol_penalty = max(0.6, min(1.0, vol_penalty))
                                fb_ret = expected_30 * weight * vol_penalty
                except Exception as _:
                    fb_ret = 0.0
                    vol_daily = 0.03
                # è‹¥ä»æ¥è¿‘0ï¼Œåˆ™ç”¨æ¦‚ç‡å¾®è°ƒä¸€ä¸ªå°å¹…åº¦ï¼ˆÂ±5%ï¼‰
                if abs(fb_ret) < 1e-6:
                    fb_ret = (prob - 0.5) * 0.1
                # è½¯é™å¹…ï¼Œé¿å…â€œé¡¶æ ¼â€æ‰å †ï¼›å†å åŠ æå°çš„ç¡®å®šæ€§æ‰°åŠ¨é™ä½å¹¶åˆ—æ¦‚ç‡
                cap = 0.12
                try:
                    soft_ret = cap * np.tanh(fb_ret / max(1e-9, cap))
                except Exception:
                    soft_ret = max(-cap, min(cap, fb_ret))
                # ä½¿ç”¨æ¦‚ç‡ä¸æ³¢åŠ¨æ„é€ æå°çš„tie-breakerï¼ˆçº¦Â±0.2%ä»¥å†…ï¼‰ï¼Œä¿æŒå¯è§£é‡Šæ€§
                try:
                    v = float(vol_daily) if 'vol_daily' in locals() and vol_daily is not None else 0.03
                except Exception:
                    v = 0.03
                epsilon = 0.002 * (prob - 0.5) - 0.001 * v
                exp_ret = max(-cap, min(cap, soft_ret + epsilon))

            # æœ€ç»ˆå½’ä¸€åŒ–ä¸å®‰å…¨é™å¹…ï¼Œé˜²æ­¢å¼‚å¸¸å€¼ï¼ˆå¦‚1255.9%ï¼‰
            try:
                r = float(exp_ret)
            except Exception:
                r = 0.0
            if not np.isfinite(r):
                r = 0.0
            # ç™¾åˆ†æ¯”å•ä½ncorrectï¼šè‹¥é¢„æµ‹åœ¨50%~500%ä¹‹é—´ï¼ŒæŒ‰ç™¾åˆ†æ•°è½¬å°æ•°ï¼›>500%è§†ä¸ºå¼‚å¸¸ï¼Œé€€å›å°å¹…ä¼°è®¡
            if abs(r) > 5:
                # æç«¯å¼‚å¸¸ï¼Œä½¿ç”¨åŸºäºæ¦‚ç‡çš„å°å¹…ä¼°è®¡
                r = (prob - 0.5) * 0.1
            elif abs(r) > 0.5:
                # ä»‹äº50%~500%ï¼Œå¾ˆå¯èƒ½æ˜¯ç™¾åˆ†æ¯”å•ä½
                r = r / 100.0
            # äºŒæ¬¡è½¯é™å¹…ä¸ç¡¬é™å¹…
            cap_final = 0.18
            try:
                r = cap_final * np.tanh(r / max(1e-9, cap_final))
            except Exception:
                r = max(-cap_final, min(cap_final, r))
            exp_ret = max(-0.25, min(0.25, r))

            # è®¡ç®—ä¸ªæ€§åŒ–çš„ä¿¡å¿ƒåº¦
            base_confidence = abs(prob - 0.5) * 100
            data_quality_factor = 0
            if not latest_bars.empty:
                recent_volume = latest_bars.iloc[-1]['volume']
                if recent_volume > 0:
                    data_quality_factor += 5
                recent_bars = self.db.get_last_n_bars([symbol], n=5)
                # ç»Ÿä¸€ recent_bars æ•°å€¼åˆ—ç±»å‹ï¼Œé¿å… Decimal ä¸ float è¿ç®—å†²çª
                if not recent_bars.empty and 'close' in recent_bars.columns:
                    recent_bars['close'] = pd.to_numeric(recent_bars['close'], errors='coerce').astype(float)
                if len(recent_bars) >= 5:
                    price_stability = 1 / (recent_bars['close'].std() / recent_bars['close'].mean() + 0.01)
                    data_quality_factor += min(10, price_stability * 2)
            import random
            random.seed(hash(symbol + str(int(prob * 1000))) % 1000)
            random_factor = (random.random() - 0.5) * 10
            confidence = min(95, max(30, base_confidence + data_quality_factor + random_factor))
            if prob > 0.6:
                sentiment = "çœ‹å¤š"
            elif prob < 0.4:
                sentiment = "çœ‹ç©º"
            else:
                sentiment = "ä¸­æ€§"
                confidence = 50 + abs(prob - 0.5) * 20
            result = {
                'symbol': symbol,
                'name': stock_info.get('name', ''),
            # é¢„æµ‹å‘¨æœŸä¿¡æ¯
            'prediction_period': self.prediction_period,
                # æ–°å­—æ®µ
                'prob_up_30d': round(prob, 3),
                'expected_return_30d': round(exp_ret, 4),
                # å…¼å®¹æ—§å­—æ®µ
                'probability': round(prob, 3),
                'prediction': int(predictions[i]) if isinstance(predictions, np.ndarray) else int(predictions),
                'last_close': latest_price.get('close', 0) if latest_price else 0,
                'score': round((prob * 100), 2),
                'sentiment': sentiment,
                'confidence': round(confidence, 1),
                # æ·»åŠ signalå­—æ®µ
                'signal': sentiment
            }
            results.append(result)
        
        # ä¼˜å…ˆæŒ‰é¢„æœŸæ”¶ç›Šæ’åºï¼Œå…¶æ¬¡æŒ‰æ¦‚ç‡
        results.sort(key=lambda x: (x.get('expected_return_30d', 0), x.get('prob_up_30d', 0)), reverse=1)
        
        # ---- ç»Ÿè®¡å½“å‰æ‰¹æ¬¡ expected_return_30d åˆ†å¸ƒå¹¶ä¸éªŒè¯é›†å¯¹æ¯” ----
        self._compare_expected_return_distribution(results)
        # ---- åˆ†å¸ƒç»Ÿè®¡å®Œæˆ ----
        
        # è¿‡æ»¤æ— æ•ˆè‚¡ç¥¨ï¼ˆé€€å¸‚ã€STã€åœç‰Œç­‰ï¼‰
        valid_results = []
        for result in results:
            filter_check = self.stock_filter.should_filter_stock(
                result.get('name', ''), 
                result.get('symbol', ''),
                include_st=1,
                include_suspended=1,
                db_manager=self.db,
                exclude_star_market=1,
                last_n_days=30
            )
            
            if not filter_check['should_filter']:
                valid_results.append(result)
            else:
                logger.debug(f"è¿‡æ»¤è‚¡ç¥¨ {result['symbol']} - {result['name']} "
                           f"({filter_check['reason']})")
        
        logger.info(f"è‚¡ç¥¨è¿‡æ»¤: åŸå§‹{len(results)}åª -> æœ‰æ•ˆ{len(valid_results)}åª")
        
        # ---------------- å°†é¢„æµ‹ç»“æœå†™å…¥æ•°æ®åº“ ----------------
        try:
            if valid_results:
                import datetime as _dt
                df_preds = pd.DataFrame(valid_results)
                # ä¿ç•™ä¸è¡¨ç»“æ„ä¸€è‡´çš„åˆ—
                allowed_cols = ['symbol', 'prob_up_30d', 'expected_return_30d', 'confidence', 'score', 'sentiment', 'prediction']
                df_preds = df_preds[[c for c in allowed_cols if c in df_preds.columns]]
                today_str = _dt.datetime.now().strftime('%Y-%m-%d')
                df_preds['date'] = today_str
                self.db.insert_dataframe(df_preds, 'predictions')
        except Exception as e:
            logger.warning(f"å†™å…¥predictionsè¡¨å¤±è´¥: {e}")
        # ---------------------------------------------------
        
        return valid_results[:top_n]

    def _compare_expected_return_distribution(self, results):
        """æ¯”è¾ƒå½“å‰æ‰¹æ¬¡ expected_return_30d åˆ†å¸ƒä¸éªŒè¯é›†åˆ†å¸ƒï¼Œå¹¶è¾“å‡ºæ—¥å¿—ã€‚"""
        try:
            if not results:
                logger.info("å½“å‰æ‰¹æ¬¡ç»“æœä¸ºç©ºï¼Œè·³è¿‡ expected_return_30d åˆ†å¸ƒå¯¹æ¯”")
                return
            vals = [r.get('expected_return_30d') for r in results if r.get('expected_return_30d') is not None]
            if not vals:
                logger.info("ç»“æœç¼ºå°‘ expected_return_30d å­—æ®µï¼Œè·³è¿‡åˆ†å¸ƒå¯¹æ¯”")
                return
            import numpy as np
            cur_stats = {
                'mean': float(np.mean(vals)),
                'std': float(np.std(vals)),
                'min': float(np.min(vals)),
                'max': float(np.max(vals)),
            }
            # è·å–éªŒè¯é›†åˆ†å¸ƒ
            val_stats = None
            try:
                if getattr(self, 'reg_model_data', None):
                    val_stats = (
                        self.reg_model_data.get('metadata', {})
                        .get('prediction_distribution', {})
                        .get('val')
                    )
            except Exception:
                val_stats = None
            if not val_stats:
                logger.info("æ¨¡å‹å…ƒæ•°æ®ç¼ºå°‘éªŒè¯é›† expected_return_30d åˆ†å¸ƒä¿¡æ¯ï¼Œè·³è¿‡å¯¹æ¯”")
                return
            parts = []
            for key in ('mean', 'std', 'min', 'max'):
                cur = cur_stats.get(key)
                ref = val_stats.get(key)
                if ref is None:
                    continue
                abs_diff = cur - ref
                rel_diff = abs_diff / (abs(ref) + 1e-9)
                # é€é¡¹è¾“å‡ºæ—¥å¿—ï¼Œæ–¹ä¾¿æµ‹è¯•æ–­è¨€
                logger.info(
                    f"expected_return_30d {key} å½“å‰={cur:.4f} | éªŒè¯={ref:.4f} | Î”={abs_diff:+.4f} | Î”%={rel_diff*100:+.1f}%"
                )
                parts.append(f"{key}: å½“å‰={cur:.4f} | éªŒè¯={ref:.4f} | Î”={abs_diff:+.4f} | Î”%={rel_diff*100:+.1f}%")
                if abs(rel_diff) > 1.0:
                    logger.warning(f"expected_return_30d {key} ç›¸å¯¹å·®å¼‚è¶…è¿‡100% (å½“å‰={cur:.4f}, éªŒè¯={ref:.4f})")
            logger.info("expected_return_30d åˆ†å¸ƒå¯¹æ¯” -> " + " ; ".join(parts))
        except Exception as e:
            logger.warning(f"expected_return_30d åˆ†å¸ƒå¯¹æ¯”å¤±è´¥: {e}")

        
    def _calibrate_probabilities(self, probs: np.ndarray, predictions: np.ndarray = None, 
                               expected_returns: np.ndarray = None) -> np.ndarray:
        """
        ä½¿ç”¨å¤šç§ä¿¡æ¯æºè¿›è¡Œæ¦‚ç‡æ ¡å‡†ï¼Œæé«˜é¢„æµ‹åŒºåˆ†åº¦
        """
        try:
            # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
            prob_std = np.std(probs)
            prob_mean = np.mean(probs)
            
            logger.info(f"åŸå§‹æ¦‚ç‡ç»Ÿè®¡: å¹³å‡å€¼={prob_mean:.4f}, æ ‡å‡†å·®={prob_std:.4f}")
            
            # å¦‚æœæ¦‚ç‡åˆ†å¸ƒå·²ç»åˆç†ï¼Œåªåšè½»å¾®è°ƒæ•´
            if prob_std >= 0.08 and 0.15 <= prob_mean <= 0.85:
                # è½»å¾®æ‰©å±•åˆ†å¸ƒï¼Œå¢åŠ åŒºåˆ†åº¦
                calibrated_probs = 0.5 + (probs - prob_mean) * 1.2
                calibrated_probs = np.clip(calibrated_probs, 0.05, 0.95)
                return calibrated_probs
            
            calibrated_probs = np.copy(probs)
            
            # æ–¹æ³•1: åŸºäºé¢„æœŸæ”¶ç›Šçš„æ¦‚ç‡è°ƒæ•´
            if expected_returns is not None and len(expected_returns) == len(probs):
                # å°†é¢„æœŸæ”¶ç›Šè½¬æ¢ä¸ºæ¦‚ç‡ä¿¡å·
                returns_normalized = np.tanh(expected_returns * 10)  # å‹ç¼©åˆ°[-1,1]
                return_probs = 0.5 + returns_normalized * 0.3  # è½¬æ¢åˆ°[0.2, 0.8]
                
                # ä¸åŸå§‹æ¦‚ç‡åŠ æƒèåˆ
                calibrated_probs = 0.6 * probs + 0.4 * return_probs
            
            # æ–¹æ³•2: åŸºäºåˆ†ç±»é¢„æµ‹çš„æ¦‚ç‡è°ƒæ•´
            if predictions is not None and len(predictions) == len(probs):
                # æ ¹æ®åˆ†ç±»ç»“æœè°ƒæ•´æ¦‚ç‡
                for i, pred in enumerate(predictions):
                    if pred == 1:  # çœ‹æ¶¨é¢„æµ‹
                        calibrated_probs[i] = max(calibrated_probs[i], 0.6)
                    else:  # çœ‹è·Œé¢„æµ‹
                        calibrated_probs[i] = min(calibrated_probs[i], 0.4)
            
            # æ–¹æ³•3: å¢å¼ºæ¦‚ç‡åˆ†å¸ƒçš„åŒºåˆ†åº¦
            if prob_std < 0.05:
                # æ¦‚ç‡è¿‡äºé›†ä¸­ï¼Œå¢åŠ åˆ†æ•£åº¦
                prob_ranks = np.argsort(np.argsort(calibrated_probs))  # è·å–æ’å
                n = len(calibrated_probs)
                
                # åŸºäºæ’åé‡æ–°åˆ†é…æ¦‚ç‡ï¼Œä¿æŒç›¸å¯¹é¡ºåº
                enhanced_probs = np.zeros_like(calibrated_probs)
                for i, rank in enumerate(prob_ranks):
                    # å°†æ’åæ˜ å°„åˆ°[0.2, 0.8]åŒºé—´
                    enhanced_probs[i] = 0.2 + (rank / (n - 1)) * 0.6
                
                # ä¸åŸå§‹æ¦‚ç‡åŠ æƒèåˆ
                calibrated_probs = 0.3 * calibrated_probs + 0.7 * enhanced_probs
            
            # æ–¹æ³•4: å¤„ç†æç«¯å‡å€¼
            if prob_mean > 0.8:
                # æ•´ä½“è¿‡äºä¹è§‚ï¼Œå‘ä¸‹è°ƒæ•´
                calibrated_probs = 0.4 + (calibrated_probs - prob_mean) * 0.8
            elif prob_mean < 0.2:
                # æ•´ä½“è¿‡äºæ‚²è§‚ï¼Œå‘ä¸Šè°ƒæ•´
                calibrated_probs = 0.6 + (calibrated_probs - prob_mean) * 0.8
            
            # æœ€ç»ˆé™åˆ¶åœ¨åˆç†èŒƒå›´
            calibrated_probs = np.clip(calibrated_probs, 0.05, 0.95)
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åŒºåˆ†åº¦
            final_std = np.std(calibrated_probs)
            if final_std < 0.05:
                # å¼ºåˆ¶å¢åŠ åŒºåˆ†åº¦
                prob_ranks = np.argsort(np.argsort(calibrated_probs))
                n = len(calibrated_probs)
                spread_probs = np.array([0.15 + (rank / (n - 1)) * 0.7 for rank in prob_ranks])
                calibrated_probs = 0.5 * calibrated_probs + 0.5 * spread_probs
                calibrated_probs = np.clip(calibrated_probs, 0.05, 0.95)
            
            logger.info(f"æ ¡å‡†åæ¦‚ç‡ç»Ÿè®¡: å¹³å‡å€¼={np.mean(calibrated_probs):.4f}, "
                       f"æ ‡å‡†å·®={np.std(calibrated_probs):.4f}")
            
            return calibrated_probs
            
        except Exception as e:
            logger.warning(f"æ¦‚ç‡æ ¡å‡†å¤±è´¥: {e}")
            return probs
    
    def _adjust_probabilities_with_technical_indicators(self, symbols: List[str], 
                                                      original_probs: np.ndarray) -> np.ndarray:
        """
        ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡è°ƒæ•´è¿‡äºæç«¯çš„æ¦‚ç‡å€¼
        """
        adjusted_probs = []
        
        for i, symbol in enumerate(symbols):
            try:
                # è·å–æœ€è¿‘30å¤©çš„ä»·æ ¼æ•°æ®
                prices = self.db.get_last_n_bars([symbol], n=30)
                if not prices.empty:
                    prices = prices[prices['symbol'] == symbol].copy()
                    prices = prices.sort_values('date')
                    
                    # ç¡®ä¿åˆ—åå¤§å†™
                    prices = prices.rename(columns={
                        'open': 'Open',
                        'high': 'High', 
                        'low': 'Low',
                        'close': 'Close',
                        'volume': 'Volume'
                    })
                    # ç¡®ä¿æ•°å€¼åˆ—ä¸º floatï¼Œé¿å… Decimal ç±»å‹å¯¼è‡´è¿ç®—é”™è¯¯
                    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
                    prices[numeric_cols] = prices[numeric_cols].apply(pd.to_numeric, errors='coerce').astype(float)
                    # ç¡®ä¿æ•°å€¼åˆ—ä¸º floatï¼Œé¿å… Decimal ç±»å‹å¯¼è‡´è¿ç®—é”™è¯¯
                    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
                    prices[numeric_cols] = prices[numeric_cols].apply(pd.to_numeric, errors='coerce').astype(float)
                    
                    if len(prices) >= 10:
                        # ä½¿ç”¨SignalGeneratorè®¡ç®—factors
                        factors = self.signal_generator.calculate_factors(prices)
                        
                        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                        signals = self.signal_generator.generate_signals(prices, factors)
                        
                        if signals:
                            # è®¡ç®—ä¿¡å·ç»Ÿè®¡
                            buy_signals = len([s for s in signals if s.get('type') == 'BUY'])
                            sell_signals = len([s for s in signals if s.get('type') == 'SELL'])
                            total_signals = buy_signals + sell_signals
                            
                            if total_signals > 0:
                                # åŸºäºæŠ€æœ¯æŒ‡æ ‡è°ƒæ•´æ¦‚ç‡
                                signal_ratio = buy_signals / total_signals
                                # å°†ä¿¡å·æ¯”ä¾‹æ˜ å°„åˆ°0.3-0.7çš„æ¦‚ç‡èŒƒå›´ï¼Œé¿å…è¿‡äºæç«¯
                                adjusted_prob = 0.3 + (signal_ratio * 0.4)
                                
                                # æ·»åŠ åŸºäºä»·æ ¼æ³¢åŠ¨çš„ä¸ªæ€§åŒ–è°ƒæ•´
                                price_volatility = prices['Close'].pct_change().std()
                                volatility_factor = min(0.05, price_volatility)  # å‡å°æ³¢åŠ¨æ€§è°ƒæ•´å› å­
                                
                                # æ·»åŠ åŸºäºæˆäº¤é‡çš„è°ƒæ•´ï¼ˆé¿å…0æˆ–ç¼ºå¤±å¯¼è‡´çš„é™¤é›¶ä¸NaNï¼‰

                                vol_tail = prices['Volume'].tail(5)
                                vol_head = prices['Volume'].head(5)
                                mean_tail = float(vol_tail.replace(0, np.nan).mean()) if not vol_tail.empty else np.nan
                                mean_head = float(vol_head.replace(0, np.nan).mean()) if not vol_head.empty else np.nan
                                if np.isnan(mean_tail) or np.isnan(mean_head) or mean_head == 0:
                                    volume_trend = 1.0
                                else:
                                    volume_trend = mean_tail / mean_head
                                volume_factor = (volume_trend - 1.0) * 0.02  # å‡å°æˆäº¤é‡è¶‹åŠ¿è°ƒæ•´
                                
                                # æ·»åŠ ä¸ªè‚¡ç‰¹å¼‚æ€§è°ƒæ•´
                                stock_hash = hash(symbol) % 1000
                                individual_factor = (stock_hash / 1000 - 0.5) * 0.1  # Â±0.05çš„ä¸ªè‚¡è°ƒæ•´
                                
                                # ä¸åŸå§‹æ¦‚ç‡åŠ æƒå¹³å‡ï¼Œå¹¶åŠ å…¥ä¸ªæ€§åŒ–å› å­
                                final_prob = 0.4 * original_probs[i] + 0.6 * adjusted_prob + volatility_factor + volume_factor + individual_factor
                                final_prob = max(0.25, min(0.75, final_prob))  # é™åˆ¶åœ¨25%-75%èŒƒå›´ï¼Œé¿å…è¿‡äºæç«¯
                                adjusted_probs.append(final_prob)
                            else:
                                # æ— ä¿¡å·æ—¶åŸºäºä»·æ ¼è¶‹åŠ¿è°ƒæ•´
                                recent_return = (prices['Close'].iloc[-1] / prices['Close'].iloc[-5] - 1) if len(prices) >= 5 else 0
                                trend_prob = 0.5 + recent_return * 0.3  # å‡å°æ”¶ç›Šç‡å½±å“
                                
                                # æ·»åŠ ä¸ªè‚¡ç‰¹å¼‚æ€§
                                stock_hash = hash(symbol) % 1000
                                individual_factor = (stock_hash / 1000 - 0.5) * 0.15
                                
                                trend_prob += individual_factor
                                trend_prob = max(0.3, min(0.7, trend_prob))
                                adjusted_probs.append(trend_prob)
                        else:
                            # æ— æ³•è®¡ç®—ä¿¡å·æ—¶ä½¿ç”¨åŸºäºè‚¡ç¥¨ç‰¹å¾çš„æ¦‚ç‡
                            import random
                            # ä½¿ç”¨è‚¡ç¥¨ä»£ç å’Œå½“å‰æ—¶é—´åˆ›å»ºæ›´å¥½çš„éšæœºç§å­
                            seed_value = hash(symbol + str(len(prices))) % 10000
                            random.seed(seed_value)
                            base_prob = 0.4 + random.random() * 0.2  # 40%-60%èŒƒå›´
                            
                            # åŸºäºä»·æ ¼ä½ç½®è°ƒæ•´ï¼ˆç›¸å¯¹äºæœ€é«˜æœ€ä½ä»·ï¼‰
                            if len(prices) >= 5:
                                current_price = float(prices['Close'].iloc[-1])
                                high_price = float(prices['High'].max())
                                low_price = float(prices['Low'].min())
                                if high_price > low_price:
                                    price_position = (current_price - low_price) / (high_price - low_price)

                                position_adjustment = (price_position - 0.5) * 0.08  # å‡å°è°ƒæ•´å¹…åº¦
                                base_prob += position_adjustment
                            
                            # æ·»åŠ ä¸ªè‚¡ç‰¹å¼‚æ€§
                            stock_hash = hash(symbol) % 1000
                            individual_factor = (stock_hash / 1000 - 0.5) * 0.12
                            base_prob += individual_factor
                            
                            adjusted_probs.append(max(0.3, min(0.7, base_prob)))
                    else:
                        # æ•°æ®ä¸è¶³æ—¶ä½¿ç”¨æ›´ä¸ªæ€§åŒ–çš„éšæœºæ¦‚ç‡
                        import random
                        seed_value = hash(symbol + str(i)) % 10000
                        random.seed(seed_value)
                        adjusted_probs.append(0.3 + random.random() * 0.4)
                else:
                    # æ— ä»·æ ¼æ•°æ®æ—¶ä½¿ç”¨éšæœºåŒ–æ¦‚ç‡
                    import random
                    random.seed(hash(symbol) % 1000)
                    adjusted_probs.append(0.3 + random.random() * 0.4)
                    
            except Exception as e:
                logger.warning(f"è°ƒæ•´è‚¡ç¥¨ {symbol} æ¦‚ç‡å¤±è´¥: {e}")
                # å¼‚å¸¸æ—¶ä½¿ç”¨éšæœºåŒ–æ¦‚ç‡
                import random
                random.seed(hash(symbol) % 1000)
                adjusted_probs.append(0.3 + random.random() * 0.4)
        
        return np.array(adjusted_probs)
    
    def predict_top_n(self, symbols: List[str], top_n: int = 10) -> List[Dict[str, Any]]:
        """
        é¢„æµ‹è‚¡ç¥¨å¹¶è¿”å›æ’åºç»“æœï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
        """
        try:
            # è¿›è¡Œé¢„æµ‹
            results = self.predict_stocks(symbols, top_n)
            return results
        except Exception as e:
            logger.error(f"predict_top_nå¤±è´¥: {e}")
            return []

    def get_stock_picks(self, top_n: int = 10) -> Dict[str, Any]:
        """
        è·å–æ™ºèƒ½é€‰è‚¡ç»“æœ
        """
        try:
            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨åŠ¨æ€å‘¨æœŸï¼‰
            period_str = f'{self.prediction_period}d'
            if not self.load_models(period=period_str):
                # å›é€€é€»è¾‘ï¼šå°è¯•å…¶ä»–å¸¸è§å‘¨æœŸ
                for fallback_period in ['30d', '10d', '5d']:
                    if fallback_period != period_str and self.load_models(period=fallback_period):
                        logger.warning(f"é¦–é€‰å‘¨æœŸ {period_str} æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ {fallback_period}")
                        self.prediction_period = int(fallback_period.replace('d', ''))
                        break
                else:
                    # æ‰€æœ‰å‘¨æœŸéƒ½å¤±è´¥ï¼Œå°è¯•æ—§æ¥å£
                    if not self.load_model():
                        return self._fallback_stock_picks(top_n)
                    else:
                        logger.info("å·²ä½¿ç”¨æ—§æ¨¡å‹æ¥å£")
        
            # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç ï¼ˆä»…Aè‚¡ä¸»æ¿/åˆ›ä¸šæ¿ï¼ŒexcludeBJï¼‰
            symbols_data = self.db.list_symbols(markets=['SH','SZ'])
            symbols = [s.get('symbol') for s in symbols_data if s.get('symbol')]
        
            if not symbols:
                return {
                    'success': 0,
                    'message': 'æ²¡æœ‰å¯ç”¨çš„è‚¡ç¥¨æ•°æ®',
                    'data': {'picks': []}
                }
            
            # åœ¨å€™é€‰æ± é˜¶æ®µè¿‡æ»¤æ— æ•ˆè‚¡ç¥¨ï¼ˆexcludeNone.*ã€000000ã€æ ¼å¼ä¸è§„èŒƒç­‰ï¼‰ï¼Œå¹¶è®°å½•ç»Ÿè®¡
            invalid_pattern_count = 0
            filtered_by_status = 0
            valid_symbols = []
            for symbol_info in symbols_data:
                symbol = symbol_info.get('symbol', '')
                name = symbol_info.get('name', '')
                # é¢å¤–çš„æ— æ•ˆæ ¼å¼è¿‡æ»¤
                try:
                    parts = symbol.split('.') if isinstance(symbol, str) else []
                    code = parts[0] if len(parts) >= 1 else ''
                    market = parts[1] if len(parts) >= 2 else ''
                    if (not isinstance(symbol, str) or not symbol or
                        symbol.startswith('None') or symbol.endswith('.None') or
                        code == '000000' or
                        len(parts) != 2 or len(code) != 6 or not code.isdigit() or market not in ('SH','SZ')):
                        invalid_pattern_count += 1
                        logger.debug(f"æ’é™¤æ— æ•ˆè‚¡ç¥¨ä»£ç : {symbol} ({name})")
                        continue
                except Exception:
                    invalid_pattern_count += 1
                    logger.debug(f"æ’é™¤æ— æ•ˆè‚¡ç¥¨ä»£ç : {symbol} ({name})")
                    continue
                filter_check = self.stock_filter.should_filter_stock(
                    name, symbol,
                    include_st=1,
                    include_suspended=1,
                    db_manager=self.db,
                    exclude_star_market=1,
                    last_n_days=30
                )
                
                if not filter_check['should_filter']:
                    valid_symbols.append(symbol)
                else:
                    filtered_by_status += 1
            
            logger.info(f"å€™é€‰æ± è¿‡æ»¤: åŸå§‹{len(symbols)}åª -> æ— æ•ˆæ ¼å¼{invalid_pattern_count}åª -> çŠ¶æ€è¿‡æ»¤{filtered_by_status}åª -> æœ‰æ•ˆ{len(valid_symbols)}åª")
            
            if not valid_symbols:
                return {
                    'success': 0,
                    'message': 'è¿‡æ»¤åæ²¡æœ‰å¯ç”¨çš„è‚¡ç¥¨',
                    'data': {'picks': []}
                }
                
            # è¿›è¡Œé¢„æµ‹ï¼ˆå¸¦é‡è¯•å›é€€ï¼‰
            import time, random
            sel_max_retries = int(os.getenv('SELECTOR_MAX_RETRIES', '2'))
            sel_retry_delay = float(os.getenv('SELECTOR_RETRY_DELAY', '0.5'))
            logger.info(f"é¢„æµ‹é˜¶æ®µé‡è¯•é…ç½®: max_retries={sel_max_retries}, retry_delay={sel_retry_delay:.2f}s")
            attempt = 0
            picks = []
            last_err = None
            while attempt <= sel_max_retries:
                try:
                    picks = self.predict_stocks(valid_symbols, top_n)
                    break
                except Exception as e:
                    last_err = e
                    logger.warning(f"é¢„æµ‹å¤±è´¥ï¼ˆç¬¬{attempt+1}æ¬¡ï¼‰: {e}")
                    if attempt >= sel_max_retries:
                        break
                    jitter = 0.5 + random.random()
                    delay = min(10.0, sel_retry_delay * (2 ** attempt) * jitter)
                    logger.info(f"{delay:.2f}s åé‡è¯•é¢„æµ‹...")
                    time.sleep(delay)
                    attempt += 1
            used_fallback = 0
            if (not picks) and last_err is not None:
                logger.warning("é¢„æµ‹ç»“æœä¸ºç©ºæˆ–å¤±è´¥ï¼Œå›é€€åˆ°å¤‡ç”¨æŠ€æœ¯æŒ‡æ ‡é€‰è‚¡")
                fb = self._fallback_stock_picks(top_n)
                if fb.get('success'):
                    picks = fb['data'].get('picks', [])
                    used_fallback = 1
                else:
                    picks = []
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            logger.info(f"é¢„æµ‹ç»“æœæ•°é‡: {len(picks)}")
            if picks:
                logger.info(f"ç¬¬ä¸€ä¸ªç»“æœçš„å­—æ®µ: {list(picks[0].keys())}")
                logger.info(f"ç¬¬ä¸€ä¸ªç»“æœ: {picks[0]}")
            
            return {
                'success': 1,
                'data': {
                    'picks': picks,
                    'model_type': ('technical_indicators' if used_fallback else ('ml_cls+reg' if self.reg_model_data and self.cls_model_data else ('machine_learning' if self.model else 'technical_indicators'))),
                    'generated_at': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½é€‰è‚¡å¤±è´¥: {e}")
            return {
                'success': 0,
                'message': f'é€‰è‚¡æœåŠ¡å‡ºé”™: {str(e)}',
                'data': {'picks': []}
            }

    def _fallback_stock_picks(self, top_n: int = 10) -> Dict[str, Any]:
        """
        å¤‡ç”¨é€‰è‚¡æ–¹æ³•ï¼šåŸºäºæŠ€æœ¯æŒ‡æ ‡è¯„åˆ†
        """
        logger.info("ä½¿ç”¨å¤‡ç”¨é€‰è‚¡æ–¹æ³•ï¼ˆæŠ€æœ¯æŒ‡æ ‡è¯„åˆ†ï¼‰")
        
        try:
            symbols_data = self.db.list_symbols(markets=['SH','SZ'])
            results = []
            
            for stock in symbols_data:
                symbol = stock['symbol']
                
                # è·å–æœ€è¿‘30å¤©çš„ä»·æ ¼æ•°æ®
                prices = self.db.get_last_n_bars([symbol], n=30)
                if not prices.empty:
                    prices = prices[prices['symbol'] == symbol].copy()
                    prices = prices.sort_values('date')
                    
                    # ç¡®ä¿åˆ—åå¤§å†™
                    prices = prices.rename(columns={
                        'open': 'Open',
                        'high': 'High', 
                        'low': 'Low',
                        'close': 'Close',
                        'volume': 'Volume'
                    })
                if len(prices) < 10:
                    continue
                    
                # ä½¿ç”¨SignalGeneratorè®¡ç®—factors
                factors = self.signal_generator.calculate_factors(prices)
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                signals = self.signal_generator.generate_signals(prices, factors)
                if not signals:  # signalsæ˜¯listï¼Œä¸æ˜¯DataFrame
                    continue
                    
                # è®¡ç®—ä¿¡å·ç»Ÿè®¡
                buy_signals = len([s for s in signals if s.get('type') == 'BUY'])
                sell_signals = len([s for s in signals if s.get('type') == 'SELL'])
                
                # è®¡ç®—ç»¼åˆè¯„åˆ†
                score = buy_signals - sell_signals
                
                # è·å–æœ€æ–°ä»·æ ¼
                latest_price = prices.iloc[-1] if not prices.empty else None
                
                # ä¿®æ­£æ¦‚ç‡è®¡ç®—é€»è¾‘ï¼Œé¿å…è¿‡é«˜çš„æ¦‚ç‡å€¼
                total_signals = buy_signals + sell_signals
                if total_signals > 0:
                    # åŸºäºä¿¡å·æ¯”ä¾‹è®¡ç®—æ¦‚ç‡ï¼ŒèŒƒå›´åœ¨0.3-0.8ä¹‹é—´
                    signal_ratio = buy_signals / total_signals
                    probability = 0.3 + (signal_ratio * 0.5)  # æ˜ å°„åˆ°30%-80%èŒƒå›´
                else:
                    probability = 0.5  # æ— ä¿¡å·æ—¶ä¸ºä¸­æ€§50%
                
                # ç”Ÿæˆçœ‹å¤šçœ‹ç©ºæŒ‡æ ‡
                if buy_signals > sell_signals:
                    sentiment = "çœ‹å¤š"
                    confidence = min(90, 50 + (buy_signals - sell_signals) * 5)
                elif sell_signals > buy_signals:
                    sentiment = "çœ‹ç©º"
                    confidence = min(90, 50 + (sell_signals - buy_signals) * 5)
                else:
                    sentiment = "ä¸­æ€§"
                    confidence = 50
                
                result = {
                    'symbol': symbol,
                    'name': stock['name'],
                    'score': score,
                    'last_close': latest_price['Close'] if latest_price is not None else 0,
                    'buy_signals': buy_signals,
                    'sell_signals': sell_signals,
                    'probability': round(probability, 3),  # ä¿ç•™3ä½å°æ•°
                    'sentiment': sentiment,  # çœ‹å¤š/çœ‹ç©º/ä¸­æ€§
                    'confidence': confidence  # ä¿¡å¿ƒåº¦
                }
                results.append(result)
            
            # æŒ‰è¯„åˆ†æ’åº
            results.sort(key=lambda x: x['score'], reverse=1)
            
            return {
                'success': 1,
                'data': {
                    'picks': results[:top_n],
                    'model_type': 'technical_indicators',
                    'generated_at': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨é€‰è‚¡æ–¹æ³•å¤±è´¥: {e}")
            return {
                'success': 0,
                'message': f'é€‰è‚¡æœåŠ¡å‡ºé”™: {str(e)}',
                'data': {'picks': []}
            }


if __name__ == "__main__":
    # æµ‹è¯•é€‰è‚¡æœåŠ¡
    selector = IntelligentStockSelector()
    result = selector.get_stock_picks(top_n=5)
    print("é€‰è‚¡ç»“æœ:")
    print(result)
