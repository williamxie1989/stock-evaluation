"""统一数据访问层 - 整合数据库操作和统一数据提供者"""

import asyncio
import hashlib
import logging
import os
import time
import threading
from dataclasses import dataclass
from datetime import datetime, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Set, Tuple, Union

import numpy as np
import pandas as pd

from src.cache.redis_cache import RedisCache

# 使用绝对导入
from src.data.db.unified_database_manager import UnifiedDatabaseManager
from src.data.providers.unified_data_provider import UnifiedDataProvider
from src.data.providers.data_source_validator import DataSourceValidator
from src.data.db.symbol_standardizer import get_symbol_standardizer, standardize_symbol

logger = logging.getLogger(__name__)


@dataclass
class DataAccessConfig:
    """数据访问配置"""
    use_cache: bool = True
    cache_ttl: int = 300  # 5分钟
    quality_threshold: float = 0.7
    auto_sync: bool = True
    sync_batch_size: int = 50
    sync_delay: float = 0.5
    max_sync_retries: int = 3
    enable_data_validation: bool = True
    preferred_data_sources: List[str] = None
    
    def __post_init__(self):
        # 如果没有显式传入 preferred_data_sources，则尝试读取环境变量覆盖，否则使用默认值
        if self.preferred_data_sources is None:
            # 尝试加载 .env 并读取环境变量
            try:
                from dotenv import load_dotenv  # 局部导入，避免非必要依赖
                load_dotenv()
            except Exception:
                # 若未安装 python-dotenv 或加载失败，直接忽略
                pass

            env_sources = os.getenv('PREFERRED_DATA_SOURCES')
            if env_sources:
                # 允许用户通过逗号分隔的形式自定义优先级，例如 "sina,eastmoney,akshare"
                self.preferred_data_sources = [s.strip() for s in env_sources.split(',') if s.strip()]
                logger.info(f"Preferred data sources set from environment variable: {self.preferred_data_sources}")
            else:
                # 默认优先使用 tushare，其次为其他国内行情源
                self.preferred_data_sources = ['tushare', 'eastmoney', 'tencent', 'sina', 'optimized_enhanced', 'enhanced_realtime', 'akshare', 'netease']


@dataclass(eq=False)
class CacheFetchResult:
    """缓存加载结果，携带数据源信息，便于统计"""

    dataframe: pd.DataFrame
    source: str  # l1 / l2 / db / provider


class UnifiedDataAccessLayer:
    """统一数据访问层 - 整合数据库和外部数据源"""

    def clear_all_caches(self) -> None:
        """清空内部缓存，用于测试或重置"""
        try:
            if hasattr(self, "unified_provider") and hasattr(self.unified_provider, "cache"):
                self.unified_provider.cache.clear()
            if hasattr(self.db_manager, "cache"):
                self.db_manager.cache.clear()
            # 重置 L0/L1 缓存与统计
            if hasattr(self, "_l0_cached_loader"):
                self._l0_cached_loader.cache_clear()
            self._cache_stats = {'hits': 0, 'misses': 0}
            self._reset_perf_metrics()
            self._l1_cache = RedisCache(ttl=self.config.cache_ttl)
        except Exception as exc:  # pragma: no cover
            logger.warning("clear_all_caches failed: %s", exc)
    
    def __init__(self, db_manager: Optional[UnifiedDatabaseManager] = None, 
                 config: Optional[DataAccessConfig] = None):
        """
        初始化统一数据访问层
        
        Args:
            db_manager: 数据库管理器
            config: 数据访问配置
        """
        self.db_manager = db_manager or UnifiedDatabaseManager()
        self.config = config or DataAccessConfig()
        self.unified_provider = UnifiedDataProvider(
            cache_ttl=self.config.cache_ttl
        )
        self.data_validator = DataSourceValidator(self.unified_provider)
        self._sync_lock = threading.Lock()
        self._cache_stats = {'hits': 0, 'misses': 0}
        self._perf_lock = threading.Lock()
        self._reset_perf_metrics()
        self._l1_cache = RedisCache(ttl=self.config.cache_ttl)
        self._initialize_l0_cache()

        logger.info("UnifiedDataAccessLayer initialized")

    # ------------------------------------------------------------------
    # 内部初始化与度量
    # ------------------------------------------------------------------
    def _initialize_l0_cache(self) -> None:
        """使用 functools.lru_cache 构建进程级 L0 缓存"""

        maxsize = int(os.getenv("UDA_L0_MAXSIZE", "256"))
        maxsize = max(32, maxsize)  # 防止配置过小导致频繁失效

        @lru_cache(maxsize=maxsize)
        def _cached_loader(
            symbol: str,
            start_str: str,
            end_str: str,
            fields_key: Tuple[str, ...],
            ttl_bucket: int,
            auto_sync_flag: bool,
        ) -> CacheFetchResult:
            # 新线程执行时没有事件循环，需要自行创建
            start_dt = datetime.strptime(start_str, "%Y-%m-%d")
            end_dt = datetime.strptime(end_str, "%Y-%m-%d")
            loop = asyncio.new_event_loop()
            try:
                asyncio.set_event_loop(loop)
                return loop.run_until_complete(
                    self._load_historical_data_uncached(
                        symbol,
                        start_dt,
                        end_dt,
                        fields_key,
                        auto_sync_flag,
                    )
                )
            finally:
                loop.close()

        self._l0_cached_loader = _cached_loader

    def _reset_perf_metrics(self) -> None:
        """重置性能统计"""

        self._perf_metrics = {
            "requests": 0,
            "l0_hits": 0,
            "l1_hits": 0,
            "l2_hits": 0,
            "db_hits": 0,
            "provider_hits": 0,
            "total_latency": 0.0,
            "last_updated": None,
        }

    def _record_performance(self, source: str, latency: float) -> None:
        """记录缓存命中层级与耗时"""

        with self._perf_lock:
            metrics = self._perf_metrics
            metrics["requests"] += 1
            metrics["total_latency"] += latency
            metrics["last_updated"] = datetime.now().isoformat()

            hit_map = {
                "l0": "l0_hits",
                "l1": "l1_hits",
                "l2": "l2_hits",
                "db": "db_hits",
                "provider": "provider_hits",
            }
            metrics[hit_map.get(source, "db_hits")] += 1

    def _get_performance_snapshot(self) -> Dict[str, Any]:
        with self._perf_lock:
            return dict(self._perf_metrics)

    def _current_ttl_bucket(self) -> int:
        """根据 cache_ttl 返回当前时间片，供 lru_cache 失效"""

        ttl = getattr(self.config, "cache_ttl", 0) or 0
        if ttl <= 0:
            return 0
        return int(time.time() // ttl)

    @staticmethod
    def _fields_to_tuple(fields: Optional[Sequence[str]]) -> Tuple[str, ...]:
        if not fields:
            return tuple()
        return tuple(sorted({field for field in fields if field}))
    
    def initialize_data_sources(self, validate_sources: bool = True) -> Dict[str, Any]:
        """
        初始化数据源
        
        Args:
            validate_sources: 是否验证数据源
            
        Returns:
            初始化结果
        """
        logger.info("Initializing data sources...")
        
        try:
            # 导入并初始化各种数据提供者
            from src.data.providers.akshare_provider import AkshareDataProvider
            from src.data.providers.enhanced_realtime_provider import EnhancedRealtimeProvider
            from src.data.providers.optimized_enhanced_data_provider import OptimizedEnhancedDataProvider
            from src.data.providers.domestic.eastmoney_provider import EastmoneyDataProvider
            from src.data.providers.domestic.tencent_provider import TencentDataProvider
            from src.data.providers.domestic.netease_provider import NeteaseDataProvider
            from src.data.providers.domestic.tushare_provider import TushareDataProvider
            from src.data.providers.domestic.sina_provider import SinaDataProvider
            
            providers = {
                'akshare': AkshareDataProvider(),
                'enhanced_realtime': EnhancedRealtimeProvider(),
                'optimized_enhanced': OptimizedEnhancedDataProvider(),
                'eastmoney': EastmoneyDataProvider(),
                'tencent': TencentDataProvider(),
                'netease': NeteaseDataProvider(),
                'tushare': TushareDataProvider(),
                'sina': SinaDataProvider()
            }
            
            initialization_results = {}
            
            # 根据配置的首选顺序添加数据提供者
            for source_name in self.config.preferred_data_sources:
                if source_name in providers:
                    provider = providers[source_name]
                    
                    if validate_sources and self.config.enable_data_validation:
                        # 验证数据源
                        validation_result = self.unified_provider.validate_data_source(provider)
                        initialization_results[source_name] = validation_result
                        
                        # 根据验证结果决定是否添加
                        if validation_result['overall_score'] >= self.config.quality_threshold:
                            self.unified_provider.add_primary_provider(provider)
                            logger.info(f"Added {source_name} as primary provider (score: {validation_result['overall_score']:.2f})")
                        else:
                            self.unified_provider.add_fallback_provider(provider)
                            logger.warning(f"Added {source_name} as fallback provider (score: {validation_result['overall_score']:.2f})")
                    else:
                        # 不验证直接添加为主要提供者
                        self.unified_provider.add_primary_provider(provider)
                        initialization_results[source_name] = {
                            'provider': source_name,
                            'overall_score': 1.0,
                            'recommendation': 'added_without_validation'
                        }
                        logger.info(f"Added {source_name} as primary provider (validation skipped)")
            
            logger.info("Data sources initialization completed")
            return initialization_results
            
        except Exception as e:
            logger.error(f"Failed to initialize data sources: {e}")
            raise
    

        


    
    async def get_realtime_data(self, symbols: List[str]) -> Optional[Dict[str, Dict[str, Any]]]:
        """
        获取实时数据
        
        Args:
            symbols: 股票代码列表
            
        Returns:
            实时数据字典
        """
        try:
            if not symbols:
                return {}
            
            # 标准化股票代码列表
            standardized_symbols = [standardize_symbol(symbol) for symbol in symbols]
            logger.info(f"Standardized symbols: {symbols} -> {standardized_symbols}")
            
            # 从统一数据提供者获取实时数据（注意：这是同步方法，不需要await）
            realtime_data = self.unified_provider.get_realtime_data(standardized_symbols)
            
            if realtime_data:
                # 保存实时数据到数据库
                await self.db_manager.save_realtime_data(realtime_data)
                logger.info(f"Retrieved and saved realtime data for {len(realtime_data)} symbols")
            
            return realtime_data
            
        except Exception as e:
            logger.error(f"Error getting realtime data: {e}")
            return None
    
    def _convert_stock_list_to_dataframe(self, stock_list: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:
        """
        将股票列表转换为DataFrame格式
        
        Args:
            stock_list: 股票列表（List[Dict]）
            
        Returns:
            股票列表DataFrame，格式与akshare_provider一致
        """
        if not stock_list:
            logger.warning("No stocks to convert to DataFrame")
            return None
        
        try:
            # 转换为DataFrame格式，与akshare_provider保持一致
            df_data = []
            for stock in stock_list:
                # 根据symbol提取市场信息
                symbol = stock.get('symbol', '')
                if '.SH' in symbol or '.SS' in symbol:
                    market_code = 'SH'
                elif '.SZ' in symbol:
                    market_code = 'SZ'
                else:
                    market_code = stock.get('market', 'UNKNOWN')
                
                # 提取股票代码（去掉后缀）
                code = symbol.replace('.SH', '').replace('.SS', '').replace('.SZ', '') if symbol else stock.get('code', '')
                
                # 生成带后缀的标准化 symbol
                try:
                    if symbol:
                        std_symbol = standardize_symbol(symbol)
                    else:
                        # 如果原始数据只有 code，没有 symbol 字段，根据市场信息补全后缀
                        tentative_symbol = f"{code}.{market_code}" if market_code in ['SH', 'SZ'] else code
                        std_symbol = standardize_symbol(tentative_symbol)
                except Exception:
                    # 如果标准化失败，则回退使用原始 symbol 或 code
                    std_symbol = symbol if symbol else code

                df_data.append({
                    'code': code,
                    'symbol': std_symbol,
                    'name': stock.get('name', ''),
                    'market': market_code,
                    'board_type': stock.get('board_type', '')
                })
            
            result_df = pd.DataFrame(df_data)
            logger.info(f"Converted {len(result_df)} stocks to DataFrame format")
            return result_df
            
        except Exception as e:
            logger.error(f"Error converting stock list to DataFrame: {e}")
            return None

    def get_all_stock_list(self, market: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        获取全市场股票列表（同步方法）
        
        Args:
            market: 市场代码 (可选)
            
        Returns:
            股票列表DataFrame，格式与akshare_provider一致
        """
        try:
            # 使用异步的get_stock_list方法，但同步调用
            import asyncio
            import threading
            
            # 检查当前是否已经有事件循环在运行
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # 如果事件循环已经在运行，我们不能使用run_until_complete
                    # 使用线程来运行异步方法
                    result = None
                    
                    def run_async_in_thread():
                        nonlocal result
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            stock_list = new_loop.run_until_complete(self.get_stock_list(market=market))
                            # 确保转换为DataFrame格式
                            result = self._convert_stock_list_to_dataframe(stock_list)
                        finally:
                            new_loop.close()
                    
                    thread = threading.Thread(target=run_async_in_thread)
                    thread.start()
                    thread.join()
                    return result
            except RuntimeError:
                pass
            
            # 如果没有事件循环或事件循环未运行，直接使用当前循环
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # 运行异步方法
            stock_list = loop.run_until_complete(self.get_stock_list(market=market))
            return self._convert_stock_list_to_dataframe(stock_list)
                
        except Exception as e:
            logger.error(f"Error getting all stock list: {e}")
            return None
    
    def get_stock_data(
        self,
        symbol: str,
        start_date: Union[str, datetime],
        end_date: Union[str, datetime],
        fields: Optional[Sequence[str]] = None,
        force_refresh: bool = False,
        auto_sync: bool = None,
    ) -> Optional[pd.DataFrame]:
        """同步获取股票历史数据，包装异步接口并支持字段选择"""
        try:
            # 标准化股票代码
            standardized_symbol = standardize_symbol(symbol)
            logger.info(f"Standardized symbol: {symbol} -> {standardized_symbol}")
            
            import asyncio
            import threading
            result: Optional[pd.DataFrame] = None

            def run_async_in_thread():
                nonlocal result
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    coro = self.get_historical_data(
                        standardized_symbol,
                        start_date,
                        end_date,
                        fields=fields,
                        force_refresh=force_refresh,
                        auto_sync=auto_sync,
                    )
                    result = new_loop.run_until_complete(coro)
                finally:
                    new_loop.close()

            # 如果当前已有事件循环在运行，则在新线程中执行
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    thread = threading.Thread(target=run_async_in_thread)
                    thread.start()
                    thread.join()
                    return result
            except RuntimeError:
                # 当前线程没有事件循环
                pass

            # 无事件循环，直接新建并运行
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            coro = self.get_historical_data(
                standardized_symbol,
                start_date,
                end_date,
                fields=fields,
                force_refresh=force_refresh,
                auto_sync=auto_sync,
            )
            result = loop.run_until_complete(coro)
            return result

        except Exception as e:
            logger.error(f"Error getting stock data synchronously for {symbol}: {e}")
            return None

    async def get_stock_list(self, market: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        获取股票列表（异步方法）
        
        Args:
            market: 市场代码 (可选)
            
        Returns:
            股票列表
        """
        try:
            # 首先尝试从数据库获取
            db_stocks = await self.db_manager.get_stock_list(market=market)
            
            if db_stocks:
                logger.info(f"Retrieved {len(db_stocks)} stocks from database")
                return db_stocks
            
            # 从外部数据源获取
            external_stocks = await self.unified_provider.get_all_stock_list(market=market)
            
            if external_stocks:
                # 保存到数据库
                await self.db_manager.upsert_stocks(external_stocks)
                logger.info(f"Saved {len(external_stocks)} stocks to database")
                return external_stocks
            
            logger.warning("Failed to get stock list from all sources")
            return []
            
        except Exception as e:
            logger.error(f"Error getting stock list: {e}")
            return []
    
    async def save_stock_data(self, symbol: str, data: pd.DataFrame) -> bool:
        """
        保存股票数据到数据库
        
        Args:
            symbol: 股票代码
            data: 股票数据DataFrame
            
        Returns:
            是否成功
        """
        try:
            return await self.db_manager.save_stock_data(symbol, data)
        except Exception as e:
            logger.error(f"Error saving stock data for {symbol}: {e}")
            return False
    
    async def upsert_stocks(self, stocks: List[Dict[str, Any]]) -> bool:
        """
        更新或插入股票列表
        
        Args:
            stocks: 股票列表
            
        Returns:
            是否成功
        """
        try:
            return await self.db_manager.upsert_stocks(stocks)
        except Exception as e:
            logger.error(f"Error upserting stocks: {e}")
            return False
    
    async def sync_market_data(self, symbols: Optional[List[str]] = None,
                              batch_size: Optional[int] = None,
                              delay: Optional[float] = None) -> Dict[str, Any]:
        """
        （已存在）按股票代码批量同步市场数据
        """
        
        """
        同步市场数据
        
        Args:
            symbols: 股票代码列表 (可选，None表示全市场)
            batch_size: 批次大小
            delay: 延迟时间
            
        Returns:
            同步结果
        """
        try:
            # 使用配置参数
            batch_size = batch_size or self.config.sync_batch_size
            delay = delay or self.config.sync_delay
            
            with self._sync_lock:
                logger.info(f"Starting market data sync for {len(symbols) if symbols else 'all'} symbols")
                
                # 获取股票列表
                if symbols is None:
                    symbols = [stock['symbol'] for stock in await self.get_stock_list()]
                
                if not symbols:
                    return {'status': 'no_symbols', 'synced': 0, 'errors': 0}
                
                # 分批同步
                total_synced = 0
                total_errors = 0
                
                for i in range(0, len(symbols), batch_size):
                    batch = symbols[i:i + batch_size]
                    
                    # 同步批次
                    batch_result = await self._sync_batch(batch, delay)
                    total_synced += batch_result['synced']
                    total_errors += batch_result['errors']
                    
                    logger.info(f"Batch {i//batch_size + 1}: synced {batch_result['synced']}, errors {batch_result['errors']}")
                
                result = {
                    'status': 'completed',
                    'total_symbols': len(symbols),
                    'synced': total_synced,
                    'errors': total_errors,
                    'success_rate': total_synced / len(symbols) if symbols else 0
                }
                
                logger.info(f"Market data sync completed: {result}")
                return result
                
        except Exception as e:
            logger.error(f"Error during market data sync: {e}")
            return {'status': 'error', 'error': str(e), 'synced': 0, 'errors': 0}
    
    async def _sync_batch(self, symbols: List[str], delay: float) -> Dict[str, int]:
        """同步一批股票"""
        synced = 0
        errors = 0
        
        for symbol in symbols:
            try:
                # 获取最近30天的数据
                end_date = datetime.now()
                start_date = end_date - timedelta(days=30)
                
                # 同步数据
                data = await self.get_historical_data(symbol, start_date, end_date, force_refresh=False)
                
                if data is not None and not data.empty:
                    synced += 1
                else:
                    errors += 1
                    
            except Exception as e:
                logger.error(f"Error syncing {symbol}: {e}")
                errors += 1
            
            # 延迟
            await asyncio.sleep(delay)
        
        return {'synced': synced, 'errors': errors}
    
    async def _sync_and_return_data(self, symbol: str, start_date: datetime, end_date: datetime, 
                                   existing_data: pd.DataFrame) -> pd.DataFrame:
        """同步缺失数据并返回完整数据 - 添加防循环机制"""
        try:
            # 检查是否已经在同步中，防止循环调用
            sync_key = f"syncing_{symbol}"
            if hasattr(self, '_sync_in_progress'):
                if self._sync_in_progress.get(sync_key, False):
                    logger.warning(f"Sync already in progress for {symbol}, skipping to prevent loop")
                    return existing_data
            else:
                self._sync_in_progress = {}
            
            # 标记同步进行中
            self._sync_in_progress[sync_key] = True
            
            try:
                # 找出缺失的日期范围
                missing_ranges = self._find_missing_date_ranges(existing_data, start_date, end_date)
                
                if not missing_ranges:
                    return existing_data
                
                logger.info(f"Found {len(missing_ranges)} missing date ranges for {symbol}")
                
                # 同步缺失的数据 - 限制重试次数和范围
                all_data = [existing_data]
                max_sync_ranges = 3  # 最多同步3个缺失范围
                sync_count = 0
                
                for missing_start, missing_end in missing_ranges:
                    if sync_count >= max_sync_ranges:
                        logger.warning(f"Reached maximum sync ranges ({max_sync_ranges}) for {symbol}")
                        break
                    
                    # 限制单次同步的时间范围（最多30天）
                    if (missing_end - missing_start).days > 30:
                        logger.warning(f"Missing range too large ({(missing_end - missing_start).days} days), limiting to 30 days")
                        missing_end = missing_start + timedelta(days=30)
                    
                    logger.info(f"Syncing missing data for {symbol}: {missing_start.date()} to {missing_end.date()}")
                    
                    # unified_provider.get_historical_data 是同步方法，不应使用 await
                    missing_data = self.unified_provider.get_historical_data(
                        symbol, missing_start.strftime('%Y-%m-%d'), missing_end.strftime('%Y-%m-%d'),
                        quality_threshold=self.config.quality_threshold * 0.7  # 降低质量要求
                    )
                    
                    if missing_data is not None and not missing_data.empty:
                        all_data.append(missing_data)
                        await self.db_manager.save_stock_data(symbol, missing_data)
                        logger.info(f"Successfully synced {len(missing_data)} records for {symbol}")
                        sync_count += 1
                    else:
                        logger.warning(f"Failed to sync missing data for {symbol} in range {missing_start.date()} to {missing_end.date()}")
                        # 如果某个范围同步失败，继续尝试下一个范围，但不中断整个流程
                        sync_count += 1  # 仍然计数，避免无限尝试
                
                # 合并所有数据
                if len(all_data) > 1:
                    combined_data = pd.concat(all_data).sort_index().drop_duplicates()
                    logger.info(f"Combined data for {symbol}: {len(combined_data)} total records")
                    return combined_data
                else:
                    logger.info(f"No additional data synced for {symbol}")
                    return existing_data
                    
            finally:
                # 清除同步标记
                self._sync_in_progress[sync_key] = False
                
        except Exception as e:
            logger.error(f"Error syncing missing data for {symbol}: {e}")
            return existing_data
    
    def _find_missing_date_ranges(self, existing_data: pd.DataFrame, 
                                  start_date: datetime, end_date: datetime) -> List[tuple]:
        """找出缺失的日期范围，仅考虑交易日"""
        if existing_data.empty:
            trading_days = [d for d in pd.date_range(start_date, end_date, freq='D') if self._is_trading_day(d)]
            if not trading_days:
                return []
            return [(start_date, end_date)]

        existing_dates = set(existing_data.index.date)
        expected_dates = {d.date() for d in pd.date_range(start_date, end_date, freq='D') if self._is_trading_day(d)}

        missing_dates = expected_dates - existing_dates
        if not missing_dates:
            return []

        missing_ranges = []
        sorted_missing = sorted(missing_dates)
        current_start = sorted_missing[0]
        current_end = sorted_missing[0]
        for date in sorted_missing[1:]:
            if date == current_end + timedelta(days=1):
                current_end = date
            else:
                missing_ranges.append((datetime.combine(current_start, datetime.min.time()),
                                        datetime.combine(current_end, datetime.min.time())))
                current_start = date
                current_end = date
        missing_ranges.append((datetime.combine(current_start, datetime.min.time()),
                               datetime.combine(current_end, datetime.min.time())))
        return missing_ranges
    
    def _is_data_complete(self, data: pd.DataFrame, start_date: datetime, end_date: datetime) -> bool:
        """改进后的完整性检测：
        1. 先确保时间范围覆盖（首尾日期基本落在目标范围内，可容忍1天误差）
        2. 再计算工作日数量的完整度，允许10% 以内缺失（考虑节假日造成的误判）
        """
        if data.empty:
            return False
    
        # 确保 index 为 datetime 类型
        if not isinstance(data.index, pd.DatetimeIndex):
            try:
                data = data.set_index(pd.to_datetime(data.index))
            except Exception:
                return False
    
        min_date = data.index.min().date()
        max_date = data.index.max().date()
    
        # 设置首尾日期允许的最大缺口天数，考虑到长假或停牌等情况
        tolerance_days = 10  # 过去的 3 天过于严格，放宽到 10 天
        # 计算首尾缺口
        head_gap = (min_date - start_date.date()).days
        if head_gap > tolerance_days:
            return False
        tail_gap = (end_date.date() - max_date).days
        if tail_gap > tolerance_days:
            return False
        # 如果尾部缺口<=3天则认为可接受
        tail_gap = (end_date.date() - max_date).days
        if tail_gap > 3:
            return False
    
        # 计算期望交易日数量（使用 _is_trading_day 而非简单工作日）
        expected_trade_dates = [d for d in pd.date_range(start_date, end_date, freq='D') if self._is_trading_day(d)]
        expected_days = len(expected_trade_dates)
        # 实际记录行数（假设每日唯一记录），如果索引有多行同一日期取 unique
        actual_days = len({idx.date() for idx in data.index})

        if expected_days == 0:
            return False

        completeness_ratio = actual_days / expected_days
        # 允许 20% 缺失（之前 10%，考虑长假 & 交易所停牌）
        return completeness_ratio >= 0.8
    
    def get_data_quality_report(self) -> Dict[str, Any]:
        """
        获取数据质量报告
        
        Returns:
            数据质量报告
        """
        try:
            # 获取数据源状态
            data_sources = {}
            
            # 获取所有提供者
            all_providers = (self.unified_provider.primary_providers + 
                          self.unified_provider.fallback_providers)
            
            for provider in all_providers:
                provider_name = provider.__class__.__name__
                try:
                    # 简单检查提供者是否可用
                    data_sources[provider_name] = {
                        'available': True,
                        'type': 'primary' if provider in self.unified_provider.primary_providers else 'fallback',
                        'last_check': datetime.now().isoformat()
                    }
                except Exception as e:
                    data_sources[provider_name] = {
                        'available': False,
                        'error': str(e),
                        'last_check': datetime.now().isoformat()
                    }
            
            # 获取缓存统计
            cache_hits = self._cache_stats.get('hits', 0)
            cache_misses = self._cache_stats.get('misses', 0)
            total_l0 = cache_hits + cache_misses
            cache_stats = {
                'l0_hits': cache_hits,
                'l0_misses': cache_misses,
                'l0_hit_rate': (cache_hits / total_l0) if total_l0 else 0.0,
            }

            perf_metrics = self._get_performance_snapshot()
            perf_total = perf_metrics.get('requests', 0)
            avg_latency_ms = (
                (perf_metrics['total_latency'] / perf_total) * 1000
                if perf_total
                else 0.0
            )
            perf_summary = {
                'requests': perf_total,
                'avg_latency_ms': round(avg_latency_ms, 2),
                'l0_hit_rate': (perf_metrics['l0_hits'] / perf_total) if perf_total else 0.0,
                'l1_hit_rate': (perf_metrics['l1_hits'] / perf_total) if perf_total else 0.0,
                'l2_hit_rate': (perf_metrics['l2_hits'] / perf_total) if perf_total else 0.0,
                'db_share': (perf_metrics['db_hits'] / perf_total) if perf_total else 0.0,
                'provider_share': (perf_metrics['provider_hits'] / perf_total) if perf_total else 0.0,
                'last_updated': perf_metrics.get('last_updated'),
            }
            
            # 数据库状态
            db_status = {
                'connected': self.db_manager.test_connection(),
                'tables': ['stocks', 'stock_data', 'realtime_data'],
                'last_check': datetime.now().isoformat()
            }
            
            report = {
                'timestamp': datetime.now().isoformat(),
                'primary_source': self.config.preferred_data_sources[0] if self.config.preferred_data_sources else 'unknown',
                'data_sources': data_sources,
                'cache_stats': cache_stats,
                'performance': perf_summary,
                'database': db_status,
                'overall_health': 'healthy' if data_sources and any(ds.get('available') for ds in data_sources.values()) else 'degraded'
            }
            
            return report
            
        except Exception as e:
            logger.error(f"Error generating data quality report: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'error': str(e),
                'overall_health': 'error'
            }
    
    async def validate_data_sources(self) -> Dict[str, Any]:
        """
        验证数据源
        
        Returns:
            验证结果
        """
        try:
            # 初始化数据源
            return self.initialize_data_sources(validate_sources=True)
        except Exception as e:
            logger.error(f"Error validating data sources: {e}")
            return {'error': str(e)}
    
    def set_primary_data_source(self, source_name: str) -> bool:
        """
        设置主要数据源
        
        Args:
            source_name: 数据源名称
            
        Returns:
            是否成功
        """
        try:
            # 重新排序首选数据源
            if source_name in self.config.preferred_data_sources:
                # 将指定数据源移到第一位
                self.config.preferred_data_sources.remove(source_name)
                self.config.preferred_data_sources.insert(0, source_name)
                
                # 重新初始化数据源
                self.initialize_data_sources(validate_sources=False)
                
                logger.info(f"Set {source_name} as primary data source")
                return True
            else:
                logger.warning(f"Data source {source_name} not found in preferred sources")
                return False
                
        except Exception as e:
            logger.error(f"Error setting primary data source: {e}")
            return False
    
    async def initialize(self) -> bool:
        """
        初始化统一数据访问层
        
        Returns:
            是否成功
        """
        try:
            # 初始化数据库
            db_initialized = await self.db_manager.initialize()
            if not db_initialized:
                logger.error("Failed to initialize database")
                return False
            
            # 初始化数据源
            self.initialize_data_sources()
            
            logger.info("UnifiedDataAccessLayer initialization completed")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize UnifiedDataAccessLayer: {e}")
            return False

    # 中国主要节假日列表（可根据需要补充）
    # 统一交易日缓存，首次使用时通过 akshare 接口加载
    _TRADE_DATES_CACHE: Optional[Set[datetime.date]] = None

    @classmethod
    def _load_trade_dates(cls) -> Set[datetime.date]:
        """从 akshare 获取所有历史交易日期并缓存。若 akshare 未安装则回退至简单周末判定。"""
        if cls._TRADE_DATES_CACHE is not None:
            return cls._TRADE_DATES_CACHE
        try:
            import akshare as ak  # 延迟导入减少启动开销
            df_calendar = ak.tool_trade_date_hist_sina()
            # 接口字段可能为 'trade_date' 或 'date'; 统一处理
            if 'trade_date' in df_calendar.columns:
                dates_series = df_calendar['trade_date']
            else:
                dates_series = df_calendar.iloc[:, 0]
            cls._TRADE_DATES_CACHE = set(pd.to_datetime(dates_series).dt.date.tolist())
            logger.info(f"Loaded {len(cls._TRADE_DATES_CACHE)} trade dates from akshare")
        except ModuleNotFoundError:
            logger.warning("akshare not installed, fallback to weekend-only trading day logic")
            cls._TRADE_DATES_CACHE = set()
        except Exception as exc:
            logger.warning(f"Failed to load trade calendar from akshare: {exc}; fallback to weekend-only logic")
            cls._TRADE_DATES_CACHE = set()
        return cls._TRADE_DATES_CACHE

    @classmethod
    def _is_trading_day(cls, date_obj: Union[datetime, pd.Timestamp, datetime.date]) -> bool:
        """判断是否为交易日：以 akshare 交易日历为准；若未能加载则退化为周一至周五。"""
        if isinstance(date_obj, datetime):
            date_obj = date_obj.date()
        elif isinstance(date_obj, pd.Timestamp):
            date_obj = date_obj.date()

        trade_dates = cls._load_trade_dates()
        if trade_dates:
            return date_obj in trade_dates
        # 回退逻辑（仅排除周末）
        return date_obj.weekday() < 5

    @staticmethod
    def _previous_trading_day(date_obj: datetime) -> datetime:
        """获取前一个交易日（包括当天往前找）。"""
        delta = timedelta(days=0)
        while True:
            candidate = (date_obj - delta).date()
            if UnifiedDataAccessLayer._is_trading_day(candidate):
                return datetime.combine(candidate, datetime.min.time())
            delta += timedelta(days=1)

    async def sync_market_data_by_date(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        step_days: int = 90,
        delay: float = 0.2,
        max_retries: int = 2,
        rollback_on_failure: bool = True,
    ) -> Dict[str, int]:
        """按日期范围递归同步市场数据，带重试与失败回退逻辑。            symbol: 股票代码。
            start_date: 起始日期（包含）。
            end_date: 结束日期（包含）。
            step_days: 每批最大天数，避免单次请求过大。
            delay: 每批同步完后的异步等待时间，避免频繁访问数据源。
            max_retries: 单批次最大重试次数。
            rollback_on_failure: 若批次重试失败，是否回滚删除已写入的区间数据。

        Returns:
            包含成功同步条数与错误次数的统计字典。
        """
        synced, errors = 0, 0

        async def _rollback_range(s_date: datetime, e_date: datetime):
            """按需删除指定区间内已写入的数据，用于失败回退。"""
            try:
                table = "prices_daily"
                if self.db_manager.db_type == "mysql":
                    query = f"DELETE FROM {table} WHERE symbol=%s AND date BETWEEN %s AND %s"
                    params = (symbol, s_date.date(), e_date.date())
                else:  # sqlite
                    query = f"DELETE FROM {table} WHERE symbol=? AND date BETWEEN ? AND ?"
                    params = (symbol, s_date.date().isoformat(), e_date.date().isoformat())
                self.db_manager.execute_update(query, params)
            except Exception as exc:
                logger.warning(f"Rollback failed for {symbol} {s_date.date()}-{e_date.date()}: {exc}")

        async def _sync_range(s_date: datetime, e_date: datetime):
            nonlocal synced, errors
            if (e_date - s_date).days < 0:
                return
            # 拆分过长区间
            if (e_date - s_date).days > step_days:
                mid = s_date + timedelta(days=step_days)
                await _sync_range(s_date, mid)
                await _sync_range(mid + timedelta(days=1), e_date)
                return

            attempt = 0
            while attempt <= max_retries:
                try:
                    data = await self.get_historical_data(
                        symbol, s_date, e_date, force_refresh=True
                    )
                    if data is not None and not data.empty:
                        synced += len(data)
                        break  # 成功即跳出循环
                    else:
                        raise ValueError("No data returned")
                except Exception as exc:
                    attempt += 1
                    if attempt > max_retries:
                        errors += 1
                        logger.error(
                            f"Sync failed for {symbol} {s_date.date()}-{e_date.date()} after {max_retries} retries: {exc}"
                        )
                        if rollback_on_failure:
                            await _rollback_range(s_date, e_date)
                    else:
                        logger.debug(
                            f"Retry {attempt}/{max_retries} for {symbol} {s_date.date()}-{e_date.date()} due to: {exc}"
                        )
                        await asyncio.sleep(delay)
            await asyncio.sleep(delay)

        await _sync_range(start_date, end_date)
        return {"synced": synced, "errors": errors}

    async def sync_market_data_all_by_date(
        self,
        trade_date: Union[str, datetime, pd.Timestamp],
        batch_size: int = 50,
        step_days: int = 15,
        delay: float = 0.2,
        max_retries: int = 2,
        rollback_on_failure: bool = True,
    ) -> Dict[str, int]:
        """按指定交易日同步全市场（或配置范围）数据。

        Args:
            trade_date: 交易日（字符串 YYYY-MM-DD 或 datetime）。
            batch_size: 每批并发股票数量，避免一次请求过大。
            step_days: 传递给单只股票同步函数的步长参数。
            delay: 每批次执行完后的 await 间隔（秒）。
            max_retries: 单股票区间最大重试次数。
            rollback_on_failure: 是否在单股票失败时回滚。

        Returns:
            成功与错误统计，例如 {"synced": 12345, "errors": 6}
        """
        # 解析日期
        if isinstance(trade_date, str):
            trade_date_dt = datetime.strptime(trade_date, "%Y-%m-%d")
        elif isinstance(trade_date, (datetime, pd.Timestamp)):
            trade_date_dt = pd.to_datetime(trade_date).to_pydatetime()
        else:
            raise ValueError("trade_date 必须是 YYYY-MM-DD 字符串或 datetime")

        # 优先尝试通过统一数据提供者一次性获取整日全市场数据，减少 API 调用次数
        try:
            bulk_df = self.unified_provider.get_market_data_by_date_range(
                start_date=trade_date_dt.strftime("%Y-%m-%d"),
                end_date=trade_date_dt.strftime("%Y-%m-%d"),
                symbols=None,
                quality_threshold=self.config.quality_threshold if hasattr(self, "config") else 0.7,
            )
        except Exception as e:
            logger.warning(f"Bulk market fetch failed, fallback to per-symbol: {e}")
            bulk_df = None

        synced_total, errors_total = 0, 0

        if bulk_df is not None and not bulk_df.empty:
            # 保存数据到数据库，bulk_df 以 (date, symbol) 为索引
            # 将数据按 symbol 分组写入，避免一次性过大
            for symbol, df_sym in bulk_df.groupby(level=1):
                # df_sym 的索引当前为 MultiIndex (date, symbol)，重置并处理
                df_sym = df_sym.reset_index(level=1, drop=True)
                try:
                    await self.db_manager.save_stock_data(symbol, df_sym)
                    synced_total += len(df_sym)
                except Exception as exc:
                    errors_total += 1
                    logger.error(f"Save bulk data failed for {symbol}: {exc}")
            return {"synced": synced_total, "errors": errors_total}

        # —— 若批量获取失败则退化为逐股票调用 ——
        symbols_df = self.get_all_stock_list()
        if symbols_df is None or symbols_df.empty or "symbol" not in symbols_df.columns:
            raise ValueError("无法获取股票列表进行按日期同步")
        symbols: List[str] = symbols_df["symbol"].tolist()

        async def _sync_symbol(sym: str):
            return await self.sync_market_data_by_date(
                symbol=sym,
                start_date=trade_date_dt,
                end_date=trade_date_dt,
                step_days=step_days,
                delay=delay,
                max_retries=max_retries,
                rollback_on_failure=rollback_on_failure,
            )

        # 简易批次异步调度，避免一次性创建过多任务
        for i in range(0, len(symbols), batch_size):
            batch_syms = symbols[i : i + batch_size]
            tasks = [asyncio.create_task(_sync_symbol(s)) for s in batch_syms]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for res in results:
                if isinstance(res, Exception):
                    errors_total += 1
                    logger.error(f"Batch symbol sync error: {res}")
                else:
                    synced_total += res.get("synced", 0)
                    errors_total += res.get("errors", 0)
            await asyncio.sleep(delay)

        return {"synced": synced_total, "errors": errors_total}

    def _make_cache_key(
        self,
        symbol: str,
        start_dt: datetime,
        end_dt: datetime,
        fields: Tuple[str, ...],
    ) -> str:  # noqa: D401
        """生成缓存键，包含 symbol + 日期范围 + 字段列表"""

        fields_part = ",".join(fields) if fields else "all"
        return f"{symbol}|{start_dt.strftime('%Y-%m-%d')}|{end_dt.strftime('%Y-%m-%d')}|{fields_part}"



    # ---------------------------------------------------------------------
    # L1 缓存（跨进程 Redis）辅助方法
    # ---------------------------------------------------------------------
    def _get_from_l1_cache(self, cache_key: str):  # noqa: D401
        """从 Redis (L1) 获取缓存数据"""
        try:
            if not self.config.use_cache:
                return None
            if not hasattr(self, "_l1_cache"):
                self._l1_cache = RedisCache(ttl=self.config.cache_ttl)
            return self._l1_cache.get(cache_key)
        except Exception as exc:  # pragma: no cover
            logger.warning("L1 cache get failed: %s", exc)
            return None

    def _save_to_l1_cache(self, cache_key: str, data):  # noqa: D401
        """保存数据到 Redis (L1)"""
        try:
            if self.config.use_cache and hasattr(self, "_l1_cache"):
                self._l1_cache.set(cache_key, data, ttl=self.config.cache_ttl)
        except Exception as exc:  # pragma: no cover
            logger.warning("L1 cache set failed: %s", exc)

        # ------------------------------------------------------------------
        # L2 磁盘缓存 (Parquet) 辅助方法
        # ------------------------------------------------------------------
    def _l2_cache_dir(self) -> Path:
        """返回 L2 缓存根目录"""
        base_dir = Path(os.getenv("L2_CACHE_DIR", "l2_cache"))
        base_dir.mkdir(parents=True, exist_ok=True)
        return base_dir

    def _l2_cache_path(self, cache_key: str) -> Path:
        key_hash = hashlib.md5(cache_key.encode()).hexdigest()
        return self._l2_cache_dir() / f"{key_hash}.parquet"

    def _get_from_l2_cache(self, cache_key: str):
        if not self.config.use_cache:
            return None
        path = self._l2_cache_path(cache_key)
        if path.exists():
            try:
                return pd.read_parquet(path)
            except Exception as exc:
                logger.warning("Read L2 cache failed: %s", exc)
        return None

    def _save_to_l2_cache(self, cache_key: str, df: pd.DataFrame):
        if not self.config.use_cache or df is None or df.empty:
            return
        try:
            path = self._l2_cache_path(cache_key)
            tmp_path = path.with_suffix(".tmp")
            df.to_parquet(tmp_path, index=False)
            tmp_path.replace(path)
        except Exception as exc:
            logger.warning("Save L2 cache failed: %s", exc)

    # ---------------------------------------------------------------------
    # 对外公开接口: 异步获取历史行情数据
    # ---------------------------------------------------------------------
    async def get_historical_data(
        self,
        symbol: str,
        start_date: "datetime | str",
        end_date: "datetime | str",
        *,
        fields: Optional[Sequence[str]] = None,
        force_refresh: bool = False,
        auto_sync: bool | None = None,
    ) -> "pd.DataFrame | None":  # noqa: D401
        """异步获取历史行情数据，带多层缓存与性能统计"""

        if isinstance(start_date, str):
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        else:
            start_dt = start_date
        if isinstance(end_date, str):
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        else:
            end_dt = end_date

        standardized_symbol = standardize_symbol(symbol)
        fields_tuple = self._fields_to_tuple(fields)
        auto_sync_flag = self.config.auto_sync if auto_sync is None else bool(auto_sync)

        perf_start = time.perf_counter()

        cache_source = "db"
        result: CacheFetchResult

        if self.config.use_cache and not force_refresh:
            loop = asyncio.get_running_loop()
            ttl_bucket = self._current_ttl_bucket()
            cache_info_before = self._l0_cached_loader.cache_info()
            result = await loop.run_in_executor(
                None,
                self._l0_cached_loader,
                standardized_symbol,
                start_dt.strftime("%Y-%m-%d"),
                end_dt.strftime("%Y-%m-%d"),
                fields_tuple,
                ttl_bucket,
                auto_sync_flag,
            )
            cache_info_after = self._l0_cached_loader.cache_info()
            l0_hit = cache_info_after.hits > cache_info_before.hits
            cache_source = "l0" if l0_hit else result.source
            if l0_hit:
                self._cache_stats['hits'] += 1
            else:
                self._cache_stats['misses'] += 1
        else:
            result = await self._load_historical_data_uncached(
                standardized_symbol,
                start_dt,
                end_dt,
                fields_tuple,
                auto_sync_flag,
            )
            cache_source = result.source
            if self.config.use_cache:
                self._cache_stats['misses'] += 1
            if force_refresh and self.config.use_cache:
                self._l0_cached_loader.cache_clear()

        df = result.dataframe
        self._record_performance(cache_source, time.perf_counter() - perf_start)
        return df

    async def _load_historical_data_uncached(
        self,
        symbol: str,
        start_dt: datetime,
        end_dt: datetime,
        fields: Tuple[str, ...],
        auto_sync_flag: bool,
    ) -> CacheFetchResult:
        """实际加载数据的实现，供 L0 缓存包装器调用"""

        cache_key = self._make_cache_key(symbol, start_dt, end_dt, fields)
        source = "db"

        df: pd.DataFrame | None = None

        if self.config.use_cache:
            df = self._get_from_l1_cache(cache_key)
            if df is not None:
                source = "l1"
            else:
                df = self._get_from_l2_cache(cache_key)
                if df is not None:
                    source = "l2"
                    self._save_to_l1_cache(cache_key, df)

        if df is None:
            try:
                df = await self.db_manager.get_stock_data(symbol, start_dt, end_dt)
            except AttributeError:
                df = await self.db_manager.get_stock_data(symbol, start_dt, end_dt)

            if df is None:
                df = pd.DataFrame()

        if auto_sync_flag and not self._is_data_complete(df, start_dt, end_dt):
            missing_df = self.unified_provider.get_historical_data(
                symbol,
                start_dt.strftime("%Y-%m-%d"),
                end_dt.strftime("%Y-%m-%d"),
            )
            if missing_df is not None and not missing_df.empty:
                df = missing_df
                source = "provider"
                try:
                    await self.db_manager.save_stock_data(symbol, missing_df)
                except Exception as exc:  # pragma: no cover
                    logger.warning("save_stock_data failed: %s", exc)

        if df is None:
            df = pd.DataFrame()

        if fields:
            available_fields = [field for field in fields if field in df.columns]
            if available_fields:
                df = df.loc[:, available_fields].copy()
            else:
                df = df.copy()
        else:
            df = df.copy()

        if self.config.use_cache and not df.empty:
            self._save_to_l1_cache(cache_key, df)
            self._save_to_l2_cache(cache_key, df)

        return CacheFetchResult(dataframe=df, source=source)

    def _bulk_concurrency_limit(self) -> int:
        """确定批量历史数据抓取的并发上限，兼顾连接池容量。"""
        try:
            env_limit = int(os.getenv("UDA_BULK_CONCURRENCY", "").strip() or 0)
        except ValueError:
            env_limit = 0

        if env_limit > 0:
            return max(1, env_limit)

        pool = getattr(self.db_manager, "_connection_pool", None)
        pool_size = getattr(pool, "pool_size", None)
        if isinstance(pool_size, int) and pool_size > 0:
            # 保留余量，避免压满连接池
            return max(1, pool_size - 2) if pool_size > 2 else pool_size

        return 6

    # ---------------------------------------------------------------------
    # 批量接口
    # ---------------------------------------------------------------------
    async def get_bulk_historical_data(
        self,
        symbols: "list[str]",
        start_date: "datetime | str",
        end_date: "datetime | str",
        *,
        fields: Optional[Sequence[str]] = None,
        force_refresh: bool = False,
        auto_sync: bool | None = None,
    ) -> "dict[str, pd.DataFrame]":  # noqa: D401
        """批量异步获取多支股票的历史行情数据。

        Args:
            symbols: 股票代码列表
            start_date: 开始日期 (含)
            end_date: 结束日期 (含)
            fields: 需要返回的字段列表
            force_refresh: 是否跳过缓存强制刷新
            auto_sync: 若数据库缺失数据，是否自动从数据源同步 (默认为配置值)

        Returns:
            dict: {symbol -> DataFrame}
        """
        import pandas as pd  # noqa: WPS433

        # 提前标准化，防止并行请求重复工作 -------------------------------
        standardized = [standardize_symbol(s) for s in symbols]
        concurrency_limit = self._bulk_concurrency_limit()
        semaphore = asyncio.Semaphore(concurrency_limit)

        async def _fetch(sym: str) -> "pd.DataFrame | None":
            async with semaphore:
                try:
                    return await self.get_historical_data(
                        sym,
                        start_date,
                        end_date,
                        fields=fields,
                        force_refresh=force_refresh,
                        auto_sync=auto_sync,
                    )
                except Exception as exc:  # pragma: no cover
                    logger.warning("bulk fetch for %s failed: %s", sym, exc)
                    return pd.DataFrame()

        tasks = [_fetch(s) for s in standardized]
        results = await asyncio.gather(*tasks)
        return {sym: df for sym, df in zip(standardized, results)}

    def get_bulk_stock_data(
        self,
        symbols: "list[str]",
        start_date: "datetime | str",
        end_date: "datetime | str",
        **kwargs,
    ) -> "dict[str, pd.DataFrame]":  # noqa: D401
        """同步包装器，便于在非 async 场景直接调用。"""
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop and loop.is_running():
            result: Dict[str, pd.DataFrame] | None = None

            def run_in_thread():  # noqa: WPS430
                nonlocal result
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    result = new_loop.run_until_complete(
                        self.get_bulk_historical_data(symbols, start_date, end_date, **kwargs)
                    )
                finally:
                    new_loop.close()

            thread = threading.Thread(target=run_in_thread)
            thread.start()
            thread.join()
            return result if result is not None else {}

        # 否则直接创建新的事件循环执行
        return asyncio.run(
            self.get_bulk_historical_data(symbols, start_date, end_date, **kwargs)
        )



    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ..
