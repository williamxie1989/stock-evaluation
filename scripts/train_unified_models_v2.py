# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ¨¡å‹è®­ç»ƒå…¥å£ V2
æ•´åˆæ–°ç‰¹å¾ä½“ç³»å’Œå¢å¼ºè®­ç»ƒå™¨
"""

import os
import sys
import logging
import argparse
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List
from collections import OrderedDict

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)

from config.prediction_config import *
from src.ml.features.unified_feature_builder import UnifiedFeatureBuilder
from src.ml.training.enhanced_trainer_v2 import EnhancedTrainerV2
from src.data.unified_data_access import UnifiedDataAccessLayer

# ğŸ”§ å¯¼å…¥ä¿®å¤å‡½æ•°
from src.ml.training.toolkit import (
    add_labels_corrected,
    evaluate_by_month,
    get_conservative_lgbm_params,
    get_conservative_xgb_params,
    improved_time_series_split
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/train_v2_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def prepare_training_data(
    symbols: List[str],
    start_date: str,
    end_date: str,
    prediction_period: int = PREDICTION_PERIOD_DAYS,
    classification_strategy: str = LABEL_STRATEGY,
    label_quantile: float = LABEL_POSITIVE_QUANTILE,
    label_min_samples: int = LABEL_MIN_SAMPLES_PER_DATE,
    enable_fundamental: bool = False
) -> pd.DataFrame:
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®
    
    é‡‡ç”¨æ··åˆä»·æ ¼ç­–ç•¥ï¼š
    - ç‰¹å¾æ„å»ºï¼šä½¿ç”¨ä¸å¤æƒä»·æ ¼ï¼ˆä¿æŒæŠ€æœ¯æŒ‡æ ‡å‡†ç¡®æ€§ï¼‰
    - æ ‡ç­¾è®¡ç®—ï¼šä½¿ç”¨å‰å¤æƒä»·æ ¼ï¼ˆåæ˜ çœŸå®æŠ•èµ„æ”¶ç›Šï¼‰
    
    Parameters
    ----------
    symbols : List[str]
        è‚¡ç¥¨ä»£ç åˆ—è¡¨
    start_date : str
        å¼€å§‹æ—¥æœŸ
    end_date : str
        ç»“æŸæ—¥æœŸ
    prediction_period : int
        é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰
    
    Returns
    -------
    df : DataFrame
        ç‰¹å¾+æ ‡ç­¾æ•°æ®
    """
    logger.info("="*80)
    logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆV2å¢å¼ºç‰ˆï¼‰")
    logger.info("="*80)
    logger.info(f"è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    logger.info(f"é¢„æµ‹å‘¨æœŸ: {prediction_period}å¤©")
    logger.info(f"ä»·æ ¼ç­–ç•¥: ç‰¹å¾ç”¨ä¸å¤æƒ + æ ‡ç­¾ç”¨å‰å¤æƒ")
    logger.info(f"æ ‡ç­¾ç­–ç•¥: {classification_strategy} (quantile={label_quantile:.2f})")
    logger.info(f"åŸºæœ¬é¢ç‰¹å¾: {'å¯ç”¨' if enable_fundamental else 'ç¦ç”¨'}")

    from src.data.unified_data_access import UnifiedDataAccessLayer, DataAccessConfig
    config = DataAccessConfig()
    config.use_cache = True  # âœ… ç¼“å­˜ç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼ˆL1 Redis + L2 Parquetéƒ½å·²éªŒè¯ï¼‰
    config.auto_sync = False  # âœ… è®­ç»ƒæ¨¡å¼ç¦ç”¨å¤–éƒ¨åŒæ­¥,ä»…ä½¿ç”¨æ•°æ®åº“æ•°æ®
    data_access = UnifiedDataAccessLayer(config=config)
    logger.info("âœ… ç¼“å­˜å·²å¯ç”¨, å¤–éƒ¨åŒæ­¥å·²ç¦ç”¨(ä»…ä½¿ç”¨æ•°æ®åº“æ•°æ®)")

    from src.data.db.unified_database_manager import UnifiedDatabaseManager
    db_manager = UnifiedDatabaseManager()

    builder = UnifiedFeatureBuilder(
        data_access=data_access,
        db_manager=db_manager,
        enable_fundamental=enable_fundamental
    )

    market_generator = builder.market_generator
    board_generator = builder.board_generator

    def _standardize_price_frame(df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ—¥çº¿æ•°æ®æ ¼å¼ï¼Œä¾¿äºç‰¹å¾ä¸å¸‚åœºå› å­å¤ç”¨"""
        tmp = df.copy()
        tmp.columns = tmp.columns.str.lower()

        if 'date' in tmp.columns:
            tmp['date'] = pd.to_datetime(tmp['date'])
            tmp.set_index('date', inplace=True)
        elif not isinstance(tmp.index, pd.DatetimeIndex):
            tmp.index = pd.to_datetime(tmp.index, errors='coerce')

        if isinstance(tmp.index, pd.DatetimeIndex):
            tmp.index.name = 'date'

        tmp.sort_index(inplace=True)

        if 'close' in tmp.columns and 'ret' not in tmp.columns:
            tmp['ret'] = tmp['close'].pct_change(fill_method=None)

        return tmp

    all_data: List[pd.DataFrame] = []
    failed_symbols: List[tuple] = []
    quality_stats = {
        'total_processed': 0,
        'data_insufficient': 0,
        'feature_build_failed': 0,
        'qfq_data_failed': 0,
        'qfq_negative_filtered': 0,
        'qfq_extreme_filtered': 0,
        'no_valid_labels': 0,
        'success': 0
    }

    price_frames: Dict[str, pd.DataFrame] = {}
    qfq_frames: Dict[str, pd.DataFrame] = {}

    for i, symbol in enumerate(symbols, 1):
        logger.info(f"[{i}/{len(symbols)}] å‡†å¤‡åŸå§‹æ•°æ® {symbol}")
        quality_stats['total_processed'] += 1

        try:
            price_df = data_access.get_stock_data(
                symbol,
                start_date,
                end_date,
                adjust_mode='none'
            )

            # ğŸ”§ ä½¿ç”¨ MIN_TRAINING_DAYS ä½œä¸ºæ•°æ®è¶³å¤Ÿæ€§æ£€æŸ¥é˜ˆå€¼ï¼ˆè€Œé LOOKBACK_DAYSï¼‰
            if price_df is None or len(price_df) < MIN_TRAINING_DAYS:
                logger.warning(f"  è·³è¿‡: ä¸å¤æƒæ•°æ®ä¸è¶³ ({len(price_df) if price_df is not None else 0} < {MIN_TRAINING_DAYS})")
                failed_symbols.append((symbol, 'data_insufficient'))
                quality_stats['data_insufficient'] += 1
                continue

            std_price_df = _standardize_price_frame(price_df)
            if len(std_price_df) < MIN_TRAINING_DAYS:
                logger.warning(f"  è·³è¿‡: æ ‡å‡†åŒ–åå¯ç”¨æ•°æ®ä¸è¶³ ({len(std_price_df)} < {MIN_TRAINING_DAYS})")
                failed_symbols.append((symbol, 'data_insufficient'))
                quality_stats['data_insufficient'] += 1
                continue

            price_frames[symbol] = std_price_df

            qfq_df = data_access.get_stock_data(
                symbol,
                start_date,
                end_date,
                adjust_mode='qfq'
            )

            if qfq_df is None or len(qfq_df) == 0:
                logger.warning("  è·³è¿‡: å‰å¤æƒæ•°æ®è·å–å¤±è´¥")
                failed_symbols.append((symbol, 'qfq_data_failed'))
                quality_stats['qfq_data_failed'] += 1
                price_frames.pop(symbol, None)
                continue

            qfq_df = qfq_df.copy()
            if 'date' not in qfq_df.columns:
                qfq_df = qfq_df.reset_index()
            if 'date' not in qfq_df.columns:
                logger.warning("  è·³è¿‡: å‰å¤æƒæ•°æ®ç¼ºå°‘æ—¥æœŸåˆ—")
                failed_symbols.append((symbol, 'qfq_data_failed'))
                quality_stats['qfq_data_failed'] += 1
                price_frames.pop(symbol, None)
                continue

            qfq_df['date'] = pd.to_datetime(qfq_df['date'])
            qfq_df.sort_values('date', inplace=True)

            initial_qfq_count = len(qfq_df)

            qfq_negative_mask = (qfq_df['close'] < 0)
            if qfq_negative_mask.sum() > 0:
                logger.warning(f"  å‘ç° {qfq_negative_mask.sum()} æ¡è´Ÿæ•°å‰å¤æƒä»·æ ¼")
                quality_stats['qfq_negative_filtered'] += qfq_negative_mask.sum()
                qfq_df = qfq_df[~qfq_negative_mask]

            qfq_extreme_mask = (qfq_df['close'] > 10000) | (qfq_df['close'] < 0.01)
            if qfq_extreme_mask.sum() > 0:
                logger.warning(f"  å‘ç° {qfq_extreme_mask.sum()} æ¡æç«¯å‰å¤æƒä»·æ ¼")
                quality_stats['qfq_extreme_filtered'] += qfq_extreme_mask.sum()
                qfq_df = qfq_df[~qfq_extreme_mask]

            filtered_count = initial_qfq_count - len(qfq_df)
            if filtered_count > initial_qfq_count * 0.3:
                logger.warning(f"  è·³è¿‡: å‰å¤æƒæ•°æ®è´¨é‡å·® (è¿‡æ»¤{filtered_count}/{initial_qfq_count})")
                failed_symbols.append((symbol, 'qfq_quality_poor'))
                price_frames.pop(symbol, None)
                continue

            # ğŸ”§ å‰å¤æƒæ•°æ®è¶³å¤Ÿæ€§ä¹Ÿä½¿ç”¨ MIN_TRAINING_DAYS
            if len(qfq_df) < MIN_TRAINING_DAYS:
                logger.warning(f"  è·³è¿‡: å‰å¤æƒæ•°æ®ä¸è¶³ (è¿‡æ»¤å{len(qfq_df)} < {MIN_TRAINING_DAYS})")
                failed_symbols.append((symbol, 'qfq_insufficient'))
                price_frames.pop(symbol, None)
                continue

            qfq_frames[symbol] = qfq_df.reset_index(drop=True)

        except Exception as exc:
            logger.error(f"  åŸå§‹æ•°æ®å‡†å¤‡å¤±è´¥: {exc}", exc_info=True)
            failed_symbols.append((symbol, 'exception'))
            price_frames.pop(symbol, None)
            qfq_frames.pop(symbol, None)

    active_symbols = [s for s in symbols if s in price_frames and s in qfq_frames]
    
    logger.info(f"å‡†å¤‡æ„å»ºå¸‚åœºå› å­:")
    logger.info(f"  price_frames æ•°é‡: {len(price_frames)}")
    logger.info(f"  qfq_frames æ•°é‡: {len(qfq_frames)}")
    logger.info(f"  active_symbols æ•°é‡: {len(active_symbols)}")
    logger.info(f"  market_generator: {'å·²åˆå§‹åŒ–' if market_generator is not None else 'æœªåˆå§‹åŒ– âŒ'}")
    logger.info(f"  MIN_STOCKS_FOR_MARKET: {MIN_STOCKS_FOR_MARKET}")

    market_returns = None
    if market_generator is not None and len(price_frames) > 0:
        try:
            logger.info(f"æ„å»ºå…¨å±€å¸‚åœºå› å­ (è¾“å…¥ {len(price_frames)} åªè‚¡ç¥¨)...")
            # ğŸ”§ å…³é”®ä¿®å¤: ä¼ å…¥æ‰€æœ‰è‚¡ç¥¨çš„ä»·æ ¼æ•°æ®ï¼Œè€Œéå•åªè‚¡ç¥¨
            market_returns = market_generator.build_market_returns(
                price_frames,  # ä½¿ç”¨å®Œæ•´çš„ price_frames å­—å…¸
                min_stocks=MIN_STOCKS_FOR_MARKET
            )
            
            if market_returns is not None and len(market_returns) > 0:
                logger.info(f"âœ… å¸‚åœºå› å­æ„å»ºæˆåŠŸ: {len(market_returns)} å¤©, å¹³å‡ {market_returns.get('count', pd.Series([0])).mean():.0f} åªè‚¡ç¥¨/å¤©")
            else:
                logger.warning(f"âš ï¸ å¸‚åœºå› å­ä¸ºç©º (min_stocks={MIN_STOCKS_FOR_MARKET})ï¼Œå°è¯•é™ä½é˜ˆå€¼...")
                # åŠ¨æ€é™ä½é˜ˆå€¼: å–è‚¡ç¥¨æ•°çš„ 10% æˆ–è‡³å°‘ 10 åª
                fallback_threshold = max(10, len(price_frames) // 10)
                logger.info(f"   é‡è¯•ä½¿ç”¨é˜ˆå€¼: {fallback_threshold}")
                market_returns = market_generator.build_market_returns(
                    price_frames,
                    min_stocks=fallback_threshold
                )
                if market_returns is not None and len(market_returns) > 0:
                    logger.info(f"âœ… é™ä½é˜ˆå€¼åæˆåŠŸ: {len(market_returns)} å¤©")
                else:
                    logger.warning("âŒ å³ä½¿é™ä½é˜ˆå€¼ä»æ— æ³•æ„å»ºå¸‚åœºå› å­")
                    market_returns = None
        except Exception as exc:
            logger.error(f"å¸‚åœºå› å­æ„å»ºå¼‚å¸¸: {exc}", exc_info=True)
            market_returns = None

    for j, symbol in enumerate(active_symbols, 1):
        try:
            logger.info(f"[{j}/{len(active_symbols)}] ç”Ÿæˆç‰¹å¾ {symbol}")

            features_df = builder.build_features_from_dataframe(price_frames[symbol], symbol)
            if features_df is None or len(features_df) == 0:
                logger.warning("  è·³è¿‡: ç‰¹å¾æ„å»ºå¤±è´¥")
                failed_symbols.append((symbol, 'feature_build_failed'))
                quality_stats['feature_build_failed'] += 1
                continue

            if market_generator is not None and market_returns is not None:
                try:
                    market_enriched = market_generator.add_market_features(
                        price_frames[symbol].copy(),
                        symbol,
                        market_returns
                    )
                    candidate_cols = ['MKT'] + market_generator.get_feature_names()
                    available_cols: List[str] = []
                    for col in candidate_cols:
                        if col in market_enriched.columns and col not in available_cols:
                            available_cols.append(col)
                    if available_cols:
                        market_slice = market_enriched.reset_index()[['date'] + available_cols]
                        features_df = features_df.merge(market_slice, on='date', how='left')
                except Exception as market_exc:
                    logger.warning(f"  å¸‚åœºç‰¹å¾æ·»åŠ å¤±è´¥: {market_exc}")

            features_df['symbol'] = symbol
            if board_generator is not None:
                try:
                    features_df = board_generator.add_board_feature(features_df, symbol_col='symbol')
                except Exception as board_exc:
                    logger.warning(f"  æ¿å—ç‰¹å¾æ·»åŠ å¤±è´¥: {board_exc}")

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿features_dfä¸­æœ‰dateåˆ—
            if 'date' not in features_df.columns:
                # å¦‚æœdateåœ¨ç´¢å¼•ä¸­ï¼Œè½¬ä¸ºåˆ—
                if features_df.index.name == 'date' or isinstance(features_df.index, pd.DatetimeIndex):
                    features_df = features_df.reset_index()
                    if 'index' in features_df.columns and 'date' not in features_df.columns:
                        features_df.rename(columns={'index': 'date'}, inplace=True)
                    logger.info(f"  â„¹ï¸ dateä»ç´¢å¼•è½¬ä¸ºåˆ—")
                else:
                    logger.error(f"  âŒ æ— æ³•æ‰¾åˆ°dateåˆ—ï¼Œè·³è¿‡æ­¤è‚¡ç¥¨")
                    logger.error(f"     ç´¢å¼•å: {features_df.index.name}, åˆ—: {list(features_df.columns[:10])}")
                    failed_symbols.append((symbol, 'no_date_column'))
                    continue

            # ğŸ”§ ä½¿ç”¨ä¿®æ­£çš„æ ‡ç­¾æ„å»ºå‡½æ•°
            # æ³¨æ„: æ ‡ç­¾é‡‡ç”¨å‰å¤æƒä»·æ ¼ï¼ŒåŸå§‹ä»·æ ¼ç”¨äºå¯¹ç…§
            try:
                # å‡†å¤‡åŸå§‹ä»·æ ¼æ•°æ®ï¼ˆä¸å¤æƒï¼‰
                price_raw = price_frames[symbol].copy()
                if price_raw.index.name == 'date' or isinstance(price_raw.index, pd.DatetimeIndex):
                    price_raw = price_raw.reset_index()
                    if 'index' in price_raw.columns and 'date' not in price_raw.columns:
                        price_raw.rename(columns={'index': 'date'}, inplace=True)

                if 'close' not in price_raw.columns:
                    logger.error(f"  âŒ price_rawä¸­æ²¡æœ‰closeåˆ—: {list(price_raw.columns)}")
                    failed_symbols.append((symbol, 'no_close_column'))
                    continue

                price_raw = price_raw[['date', 'close']].copy()
                price_raw['symbol'] = symbol

                # å‡†å¤‡å‰å¤æƒä»·æ ¼æ•°æ®
                price_adj = qfq_frames[symbol].copy()
                if 'date' not in price_adj.columns:
                    price_adj = price_adj.reset_index()

                if 'close' not in price_adj.columns:
                    logger.error(f"  âŒ å‰å¤æƒæ•°æ®ç¼ºå°‘closeåˆ—: {list(price_adj.columns)}")
                    failed_symbols.append((symbol, 'no_close_column_qfq'))
                    continue

                price_adj['date'] = pd.to_datetime(price_adj['date'])
                price_adj = price_adj[['date', 'close']].copy()
                price_adj['symbol'] = symbol

                # ä½¿ç”¨ä¿®æ­£çš„æ ‡ç­¾æ„å»ºå‡½æ•°
                features_with_labels = add_labels_corrected(
                    features_df=features_df,
                    price_data=price_adj,
                    prediction_period=prediction_period,
                    threshold=CLS_THRESHOLD,  # absolute ç­–ç•¥å…œåº•
                    price_data_raw=price_raw,
                    classification_strategy=classification_strategy,
                    quantile=label_quantile,
                    min_samples_per_date=label_min_samples,
                    negative_quantile=LABEL_NEGATIVE_QUANTILE,
                    enable_neutral_band=ENABLE_LABEL_NEUTRAL_BAND,
                    neutral_quantile=LABEL_NEUTRAL_QUANTILE,
                    market_returns=market_returns,
                    use_market_baseline=LABEL_USE_MARKET_BASELINE,
                    use_industry_neutral=LABEL_USE_INDUSTRY_NEUTRAL
                )
                features_df = features_with_labels
            except Exception as label_exc:
                logger.warning(f"  æ ‡ç­¾æ„å»ºå¤±è´¥: {label_exc}")
                failed_symbols.append((symbol, 'label_build_failed'))
                continue

            if len(features_df) == 0:
                logger.warning("  è·³è¿‡: æ— æœ‰æ•ˆæ ‡ç­¾")
                failed_symbols.append((symbol, 'no_valid_labels'))
                quality_stats['no_valid_labels'] += 1
                continue

            # å·²åœ¨add_labels_correctedä¸­å¤„ç†,è¿™é‡Œå¯é€‰æ‹©æ€§äºŒæ¬¡è¿‡æ»¤
            extreme_return_mask = features_df['label_reg'].abs() > 1.0
            if extreme_return_mask.sum() > 0:
                logger.warning(f"  è¿‡æ»¤ {extreme_return_mask.sum()} æ¡æç«¯æ”¶ç›Šç‡è®°å½•(>100%)")
                features_df = features_df[~extreme_return_mask]

            if len(features_df) == 0:
                logger.warning("  è·³è¿‡: è¿‡æ»¤åæ— æ•°æ®")
                failed_symbols.append((symbol, 'all_filtered'))
                continue

            all_data.append(features_df)
            quality_stats['success'] += 1
            logger.info(f"  âœ“ æˆåŠŸ: {len(features_df)} æ¡è®°å½• (æ­£æ ·æœ¬ç‡: {features_df['label_cls'].mean():.1%})")

        except Exception as exc:
            logger.error(f"  ç‰¹å¾ç”Ÿæˆå¤±è´¥: {exc}", exc_info=True)
            failed_symbols.append((symbol, 'exception'))
    
    # åˆå¹¶æ•°æ®
    if len(all_data) == 0:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œæ‰€æœ‰è‚¡ç¥¨å¤„ç†å¤±è´¥")
    
    df = pd.concat(all_data, ignore_index=True)

    # ç»Ÿä¸€æ‰§è¡Œè¡Œä¸šä¸­æ€§æ®‹å·®è®¡ç®—ï¼Œç¡®ä¿ä½¿ç”¨è·¨è‚¡ç¥¨æˆªé¢ä¿¡æ¯
    if LABEL_USE_INDUSTRY_NEUTRAL:
        base_col = None
        if LABEL_USE_MARKET_BASELINE and 'future_excess_return' in df.columns:
            base_col = 'future_excess_return'
        elif 'future_return' in df.columns:
            base_col = 'future_return'

        if base_col is None:
            logger.warning("è¡Œä¸šä¸­æ€§å¤„ç†è·³è¿‡: æœªæ‰¾åˆ°æœªæ¥æ”¶ç›Šåˆ—")
        elif 'industry' not in df.columns:
            logger.warning("è¡Œä¸šä¸­æ€§å¤„ç†è·³è¿‡: æ•°æ®ç¼ºå°‘industryåˆ—")
        else:
            grouped = df.groupby(['date', 'industry'])[base_col]
            group_counts = grouped.transform('count')
            min_required = max(min(label_min_samples, 3), 2)
            industry_mean = grouped.transform('mean')
            residual_series = df[base_col] - industry_mean
            sufficient_mask = group_counts >= min_required

            df['future_residual_return'] = np.where(sufficient_mask, residual_series, np.nan)

            updated_rows = int(sufficient_mask.sum())
            if updated_rows > 0:
                df.loc[sufficient_mask, 'label_reg'] = df.loc[sufficient_mask, 'future_residual_return']
                logger.info(
                    "è¡Œä¸šä¸­æ€§å·²åº”ç”¨: %d æ¡è®°å½• (é˜ˆå€¼: >=%d åŒæ—¥åŒè¡Œä¸šæ ·æœ¬)",
                    updated_rows,
                    min_required
                )
            else:
                logger.info(
                    "è¡Œä¸šä¸­æ€§æœªåº”ç”¨: æ‰€æœ‰æ—¥æœŸåŒè¡Œä¸šæ ·æœ¬æ•°ä¸è¶³ %d æ¡ï¼Œä¿ç•™åŸå§‹æ”¶ç›Šæ ‡ç­¾",
                    min_required
                )

    # æˆªé¢æ ‡å‡†åŒ–/æ’åºå¢å¼ºç‰¹å¾
    if ENABLE_CROSS_SECTIONAL_ENRICHMENT and 'date' in df.columns:
        logger.info("æ„å»ºæˆªé¢å¢å¼ºç‰¹å¾ (Z-score / Rank)...")
        available_cols = [col for col in CROSS_SECTIONAL_FEATURES if col in df.columns]
        if available_cols:
            grouped = df.groupby('date', group_keys=False)

            def _zscore(series: pd.Series) -> pd.Series:
                mu = series.mean()
                sigma = series.std(ddof=0)
                if np.isnan(mu) or sigma == 0 or np.isnan(sigma):
                    return pd.Series(np.nan, index=series.index)
                return (series - mu) / (sigma + 1e-9)

            for col in available_cols:
                z_col = f'cs_z_{col}'
                rank_col = f'cs_rank_{col}'
                df[z_col] = grouped[col].transform(_zscore)
                df[rank_col] = grouped[col].transform(lambda x: x.rank(pct=True, method='average'))

            logger.info("  æˆªé¢å¢å¼ºåˆ—: %d ä¸ª", len(available_cols) * 2)
        else:
            logger.info("  æˆªé¢å¢å¼ºè·³è¿‡ï¼šæ— å¯ç”¨åŸºç¡€åˆ—")
    
    # ========== è¾“å‡ºæ•°æ®è´¨é‡æŠ¥å‘Š ==========
    logger.info("\n" + "="*80)
    logger.info("æ•°æ®å‡†å¤‡å®Œæˆ - è´¨é‡æŠ¥å‘Š")
    logger.info("="*80)
    logger.info(f"âœ… æˆåŠŸè‚¡ç¥¨: {quality_stats['success']}/{quality_stats['total_processed']} ({quality_stats['success']/quality_stats['total_processed']*100:.1f}%)")
    logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df):,}")
    logger.info(f"ğŸ“… æ—¥æœŸèŒƒå›´: {df['date'].min()} ~ {df['date'].max()}")
    logger.info(f"ğŸ“ˆ æ­£æ ·æœ¬ç‡: {df['label_cls'].mean():.2%}")
    logger.info(f"ğŸ“‰ å¹³å‡æ”¶ç›Š: {df['label_reg'].mean():.4f}")
    logger.info(f"ğŸ“Š æ”¶ç›Šæ ‡å‡†å·®: {df['label_reg'].std():.4f}")
    
    logger.info("\nå¤±è´¥ç»Ÿè®¡:")
    logger.info(f"  æ•°æ®ä¸è¶³: {quality_stats['data_insufficient']}")
    logger.info(f"  ç‰¹å¾æ„å»ºå¤±è´¥: {quality_stats['feature_build_failed']}")
    logger.info(f"  å‰å¤æƒæ•°æ®å¤±è´¥: {quality_stats['qfq_data_failed']}")
    logger.info(f"  æ— æœ‰æ•ˆæ ‡ç­¾: {quality_stats['no_valid_labels']}")
    
    logger.info("\næ•°æ®æ¸…æ´—ç»Ÿè®¡:")
    logger.info(f"  å‰å¤æƒè´Ÿæ•°è¿‡æ»¤: {quality_stats['qfq_negative_filtered']} æ¡")
    logger.info(f"  å‰å¤æƒæç«¯å€¼è¿‡æ»¤: {quality_stats['qfq_extreme_filtered']} æ¡")
    
    if failed_symbols:
        logger.info(f"\nå¤±è´¥è‚¡ç¥¨è¯¦æƒ… (å…±{len(failed_symbols)}åª):")
        failure_reasons = {}
        for symbol, reason in failed_symbols:
            failure_reasons[reason] = failure_reasons.get(reason, []) + [symbol]
        for reason, symbols in failure_reasons.items():
            logger.info(f"  {reason}: {len(symbols)} åª - {symbols[:5]}{'...' if len(symbols) > 5 else ''}")
    
    return df


def add_labels(df: pd.DataFrame, prediction_period: int) -> pd.DataFrame:
    """
    æ·»åŠ åˆ†ç±»å’Œå›å½’æ ‡ç­¾ï¼ˆä½¿ç”¨ä¸å¤æƒä»·æ ¼ï¼Œå·²å¼ƒç”¨ï¼‰
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢« add_labels_with_qfq æ›¿ä»£
    ä¿ç•™ä»…ç”¨äºå‘åå…¼å®¹
    
    Parameters
    ----------
    df : DataFrame
        ç‰¹å¾æ•°æ®
    prediction_period : int
        é¢„æµ‹å‘¨æœŸ
    
    Returns
    -------
    df : DataFrame
        æ·»åŠ æ ‡ç­¾åçš„æ•°æ®
    """
    logger.warning("ä½¿ç”¨äº†å·²å¼ƒç”¨çš„ add_labels å‡½æ•°ï¼Œè¯·æ”¹ç”¨ add_labels_with_qfq")
    
    # è®¡ç®—æœªæ¥æ”¶ç›Š
    df['future_return'] = df['close'].shift(-prediction_period) / df['close'] - 1
    
    # åˆ†ç±»æ ‡ç­¾: æ”¶ç›Š > CLS_THRESHOLD
    df['label_cls'] = (df['future_return'] > CLS_THRESHOLD).astype(int)
    
    # å›å½’æ ‡ç­¾: æ”¶ç›Šç‡
    df['label_reg'] = df['future_return']
    
    return df


def add_labels_with_qfq(
    features_df: pd.DataFrame,
    stock_data_qfq: pd.DataFrame,
    prediction_period: int
) -> pd.DataFrame:
    """
    æ·»åŠ åˆ†ç±»å’Œå›å½’æ ‡ç­¾ï¼ˆä½¿ç”¨å‰å¤æƒä»·æ ¼ï¼‰
    
    å°†å‰å¤æƒä»·æ ¼æ•°æ®ä¸ç‰¹å¾æ•°æ®æŒ‰æ—¥æœŸå¯¹é½ï¼Œè®¡ç®—çœŸå®æŠ•èµ„æ”¶ç›Šç‡æ ‡ç­¾
    
    Parameters
    ----------
    features_df : DataFrame
        ç‰¹å¾æ•°æ®ï¼ˆåŸºäºä¸å¤æƒä»·æ ¼æ„å»ºï¼‰
    stock_data_qfq : DataFrame
        å‰å¤æƒä»·æ ¼æ•°æ®
    prediction_period : int
        é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰
    
    Returns
    -------
    df : DataFrame
        æ·»åŠ æ ‡ç­¾åçš„æ•°æ®
    """
    # ç¡®ä¿ä¸¤ä¸ªDataFrameéƒ½æœ‰dateåˆ—ä¸”ä¸ºdatetimeç±»å‹
    if 'date' not in features_df.columns:
        raise ValueError("features_df ç¼ºå°‘ 'date' åˆ—")
    if 'date' not in stock_data_qfq.columns:
        raise ValueError("stock_data_qfq ç¼ºå°‘ 'date' åˆ—")
    
    features_df = features_df.copy()
    stock_data_qfq = stock_data_qfq.copy()
    
    # ç¡®ä¿dateåˆ—ä¸ºdatetimeç±»å‹
    features_df['date'] = pd.to_datetime(features_df['date'])
    stock_data_qfq['date'] = pd.to_datetime(stock_data_qfq['date'])
    
    # æŒ‰æ—¥æœŸæ’åºå¹¶é‡ç½®ç´¢å¼•
    features_df = features_df.sort_values('date').reset_index(drop=True)
    stock_data_qfq = stock_data_qfq.sort_values('date').reset_index(drop=True)
    
    # æ–¹æ³•1: ç®€å•shiftæ–¹æ³•ï¼ˆå¦‚æœæ•°æ®å®Œå…¨å¯¹é½ï¼‰
    # å…ˆå°è¯•é€šè¿‡æ—¥æœŸmergeå¯¹é½
    # æ³¨æ„: stock_data_qfqå¯èƒ½åŒ…å«close_qfqåˆ—(ä»UnifiedDataAccessLayer)æˆ–åªæœ‰closeåˆ—(æµ‹è¯•mockæ•°æ®)
    close_col = 'close_qfq' if 'close_qfq' in stock_data_qfq.columns else 'close'
    
    qfq_for_merge = stock_data_qfq[['date', close_col]].copy()
    if close_col == 'close':
        qfq_for_merge.rename(columns={'close': 'close_qfq'}, inplace=True)
    
    # åˆ é™¤features_dfä¸­å¯èƒ½å­˜åœ¨çš„å¤æƒåˆ—ï¼Œé¿å…mergeå†²çª
    cols_to_drop = [c for c in features_df.columns if '_qfq' in c or '_hfq' in c]
    if cols_to_drop:
        features_df = features_df.drop(columns=cols_to_drop)
        logger.debug(f"  ä»features_dfåˆ é™¤å¤æƒåˆ—: {cols_to_drop}")
    
    merged = features_df.merge(qfq_for_merge, on='date', how='left')
    
    # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    merged['future_close_qfq'] = merged['close_qfq'].shift(-prediction_period)
    merged['future_return'] = (merged['future_close_qfq'] - merged['close_qfq']) / merged['close_qfq']
    
    # åˆ†ç±»æ ‡ç­¾: æ”¶ç›Š > CLS_THRESHOLD
    merged['label_cls'] = (merged['future_return'] > CLS_THRESHOLD).astype(float)
    
    # å›å½’æ ‡ç­¾: æ”¶ç›Šç‡
    merged['label_reg'] = merged['future_return']
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    result = merged.drop(columns=['close_qfq', 'future_close_qfq', 'future_return'])
    
    return result


def train_models(
    df: pd.DataFrame,
    model_save_dir: str = 'models/v2',
    enable_both_tasks: bool = True,
    classification_strategy: str = LABEL_STRATEGY,
    prediction_period: int = PREDICTION_PERIOD_DAYS
):
    """
    è®­ç»ƒæ¨¡å‹
    
    Parameters
    ----------
    df : DataFrame
        è®­ç»ƒæ•°æ®
    model_save_dir : str
        æ¨¡å‹ä¿å­˜ç›®å½•
    enable_both_tasks : bool
        æ˜¯å¦è®­ç»ƒåˆ†ç±»å’Œå›å½’ä¸¤ä¸ªä»»åŠ¡
    prediction_period : int
        é¢„æµ‹å‘¨æœŸï¼ˆå¤©æ•°ï¼‰ï¼Œç”¨äºæ¨¡å‹æ–‡ä»¶å‘½å
    """
    logger.info("="*80)
    logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹")
    logger.info(f"æ ‡ç­¾ç­–ç•¥: {classification_strategy}")
    logger.info("="*80)
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šè¯†åˆ«å®é™…å­˜åœ¨çš„ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ‡ç­¾ã€å…ƒæ•°æ®å’Œæœªæ¥ä¿¡æ¯ï¼‰
    excluded_cols = {'date', 'symbol', 'label_cls', 'label_reg', 'future_return', 'future_return_raw',
                     'future_excess_return', 'future_residual_return',
                     'open', 'high', 'low', 'close', 'volume', 'amount', 'source',
                     'open_qfq', 'high_qfq', 'low_qfq', 'close_qfq',
                     'open_hfq', 'high_hfq', 'low_hfq', 'close_hfq'}
    
    # è‡ªåŠ¨æ£€æµ‹æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾
    numerical_features = []
    categorical_features = []
    
    for col in df.columns:
        if col in excluded_cols:
            continue
        if df[col].dtype == 'object' or df[col].dtype.name == 'category':
            categorical_features.append(col)
        else:
            numerical_features.append(col)
    
    logger.info(f"æ•°å€¼ç‰¹å¾: {len(numerical_features)} - {numerical_features[:10]}...")
    logger.info(f"ç±»åˆ«ç‰¹å¾: {len(categorical_features)} - {categorical_features}")
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = numerical_features + categorical_features
    X = df[feature_cols].copy()
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = EnhancedTrainerV2(
        numerical_features=numerical_features,
        categorical_features=categorical_features,
        config={
            'use_rolling_cv': True,
            'cv_n_splits': 5
        }
    )
    
    dates_series = pd.to_datetime(df['date']) if 'date' in df.columns else None

    if 'date' in df.columns:
        df = df.copy()
        df['date'] = pd.to_datetime(df['date'])
        logger.info("æ ‡ç­¾è¯Šæ–­: æœˆåº¦æ ·æœ¬ç»Ÿè®¡")
        monthly = df.groupby(df['date'].dt.to_period('M')).agg(
            samples=('label_cls', 'count'),
            pos_rate=('label_cls', 'mean'),
            avg_return=('label_reg', 'mean')
        )
        for period, row in monthly.tail(12).iterrows():
            logger.info(
                "  %s: æ ·æœ¬ %5d, æ­£æ ·æœ¬ç‡ %.2f%%, å¹³å‡æœªæ¥æ”¶ç›Š %.4f",
                period.strftime('%Y-%m'),
                int(row['samples']),
                row['pos_rate'] * 100 if not np.isnan(row['pos_rate']) else float('nan'),
                row['avg_return']
            )
        recent_train = df[df['date'] < df['date'].max() - pd.Timedelta(days=PREDICTION_PERIOD_DAYS)]
        if not recent_train.empty:
            logger.info(
                "è®­ç»ƒæ ·æœ¬æ€»ä½“: %d, æ­£æ ·æœ¬ç‡ %.2f%%, å¹³å‡æœªæ¥æ”¶ç›Š %.4f",
                len(recent_train),
                recent_train['label_cls'].mean() * 100,
                recent_train['label_reg'].mean()
            )
        logger.info(
            "å…¨æ ·æœ¬: %d, æ­£æ ·æœ¬ç‡ %.2f%%, å¹³å‡æœªæ¥æ”¶ç›Š %.4f",
            len(df),
            df['label_cls'].mean() * 100,
            df['label_reg'].mean()
        )

        logger.info("æ ‡ç­¾å¯¹é½è¯Šæ–­: åˆ†ç±»æ ‡ç­¾ä¸æœªæ¥æ”¶ç›Šå…³ç³»")
        corr = df['label_cls'].corr(df['label_reg']) if df['label_reg'].std() > 0 else float('nan')
        logger.info("  ç›¸å…³ç³»æ•°(label_cls vs future_return): %.4f", corr)
        alignment = df.groupby('label_cls')['label_reg'].agg(['count', 'mean', 'median']).rename(index={0.0: 'neg', 1.0: 'pos'})
        neg_stats = alignment.loc['neg'] if 'neg' in alignment.index else None
        pos_stats = alignment.loc['pos'] if 'pos' in alignment.index else None
        if pos_stats is not None:
            logger.info("  æ­£ç±»: æ ·æœ¬ %d, å¹³å‡æœªæ¥æ”¶ç›Š %.4f, ä¸­ä½æ•° %.4f", int(pos_stats['count']), pos_stats['mean'], pos_stats['median'])
        if neg_stats is not None:
            logger.info("  è´Ÿç±»: æ ·æœ¬ %d, å¹³å‡æœªæ¥æ”¶ç›Š %.4f, ä¸­ä½æ•° %.4f", int(neg_stats['count']), neg_stats['mean'], neg_stats['median'])
        if classification_strategy == 'absolute':
            inconsistent = int((df['label_cls'] != (df['label_reg'] > CLS_THRESHOLD).astype(float)).sum())
            if inconsistent:
                logger.warning("  è­¦å‘Š: å‘ç° %d æ¡æ ‡ç­¾ä¸é˜ˆå€¼ä¸ä¸€è‡´çš„è®°å½•ï¼Œéœ€æ£€æŸ¥å¯¹é½é€»è¾‘", inconsistent)
            else:
                logger.info("  æ ‡ç­¾ä¸é˜ˆå€¼é€»è¾‘ä¸€è‡´ï¼Œæœªå‘ç°å¼‚å¸¸")
        else:
            logger.info("  å½“å‰ä½¿ç”¨ quantile ç­–ç•¥ï¼Œè·³è¿‡ç»å¯¹é˜ˆå€¼ä¸€è‡´æ€§æ£€æŸ¥")

    # è®­ç»ƒåˆ†ç±»æ¨¡å‹
    y_cls = df['label_cls'].copy()
    class_counts = y_cls.value_counts(dropna=True).to_dict()
    if enable_both_tasks:
        logger.info("åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: %s", class_counts)
    unique_classes = [cls for cls in y_cls.dropna().unique()]
    train_cls = enable_both_tasks and len(unique_classes) >= 2

    if enable_both_tasks and not train_cls:
        logger.error(
            "åˆ†ç±»æ ‡ç­¾ä»…åŒ…å«å•ä¸€ç±»åˆ« %s (æ ·æœ¬æ•°=%d)ï¼Œè·³è¿‡åˆ†ç±»ä»»åŠ¡",
            unique_classes[0] if unique_classes else 'N/A',
            int(class_counts.get(unique_classes[0], 0)) if unique_classes else 0
        )

    if train_cls:
        logger.info("\n" + "="*80)
        logger.info("è®­ç»ƒåˆ†ç±»ä»»åŠ¡")
        logger.info("="*80)

        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        cls_models = {}
        
        # ğŸ”§ LightGBM (ä½¿ç”¨ä¿å®ˆå‚æ•°)
        logger.info("\nè®­ç»ƒ LightGBM åˆ†ç±»å™¨ (ä¿å®ˆå‚æ•°)...")
        lgbm_params = get_conservative_lgbm_params()
        cls_lgb = trainer.train_classification_model(
            X, y_cls,
            model_type='lightgbm',
            dates=dates_series,
            **lgbm_params
        )
        cls_models['lightgbm'] = cls_lgb
        
        # Logistic Regression (åŸºçº¿)
        logger.info("\nè®­ç»ƒ Logistic åˆ†ç±»å™¨ (åŸºçº¿)...")
        cls_logistic = trainer.train_classification_model(
            X, y_cls,
            model_type='logistic',
            dates=dates_series,
            max_iter=1000
        )
        cls_models['logistic'] = cls_logistic

        # ğŸ”§ XGBoost (ä½¿ç”¨ä¿å®ˆå‚æ•°)
        logger.info("\nè®­ç»ƒ XGBoost åˆ†ç±»å™¨ (ä¿å®ˆå‚æ•°)...")
        xgb_params = get_conservative_xgb_params()
        cls_xgb = trainer.train_classification_model(
            X, y_cls,
            model_type='xgboost',
            dates=dates_series,
            **xgb_params
        )
        cls_models['xgboost'] = cls_xgb
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        best_cls_name = max(cls_models, key=lambda k: cls_models[k]['metrics']['val_auc'])
        best_cls = cls_models[best_cls_name]
        
        logger.info(f"\næœ€ä¼˜åˆ†ç±»æ¨¡å‹: {best_cls_name} (AUC={best_cls['metrics']['val_auc']:.4f})")

        best_val_auc = best_cls['metrics'].get('val_auc', float('nan'))
        if np.isnan(best_val_auc) or best_val_auc < MIN_CLASSIFICATION_AUC:
            raise RuntimeError(
                f"éªŒè¯AUC {best_val_auc:.4f} ä½äºé˜ˆå€¼ {MIN_CLASSIFICATION_AUC:.2f}, è®­ç»ƒæµç¨‹å·²ç»ˆæ­¢"
            )
        
        # ğŸ”§ æœˆåº¦åˆ†å±‚è¯„ä¼°
        try:
            logger.info("\n" + "="*80)
            logger.info("æœˆåº¦åˆ†å±‚è¯„ä¼°")
            logger.info("="*80)
            
            # è·å–æœ€ä¼˜æ¨¡å‹çš„é¢„æµ‹
            best_pipeline = best_cls['pipeline']
            all_pred = best_pipeline.predict_proba(X)[:, 1]

            production_threshold = trainer.config.get('cls_threshold', CLS_THRESHOLD)
            optimal_threshold = best_cls['metrics'].get('optimal_threshold', production_threshold)
            thresholds = OrderedDict([
                ('prod', production_threshold),
                ('opt', optimal_threshold),
                ('0.5', 0.5)
            ])

            monthly_results = evaluate_by_month(
                y_cls,
                all_pred,
                dates_series,
                thresholds=thresholds
            )
            
            if len(monthly_results) > 0:
                auc_std = monthly_results['auc'].std()
                logger.info(f"\nğŸ“Š æ¨¡å‹ç¨³å®šæ€§åˆ†æ:")
                logger.info(f"  å„æœˆä»½AUCæ ‡å‡†å·®: {auc_std:.4f} {'âœ… ç¨³å®š' if auc_std < 0.05 else 'âš ï¸ æ³¢åŠ¨è¾ƒå¤§'}")
        except Exception as eval_exc:
            logger.warning(f"æœˆåº¦è¯„ä¼°å¤±è´¥: {eval_exc}")
        
        # ä¿å­˜æ‰€æœ‰åˆ†ç±»æ¨¡å‹
        for name, model in cls_models.items():
            is_best = (name == best_cls_name)
            filepath = os.path.join(model_save_dir, f'cls_{prediction_period}d_{name}.pkl')
            trainer.save_model(model, filepath, is_best=is_best)
        
        # é¢å¤–ä¿å­˜æœ€ä¼˜æ¨¡å‹
        best_filepath = os.path.join(model_save_dir, f'cls_{prediction_period}d_best.pkl')
        trainer.save_model(best_cls, best_filepath, is_best=True)
    
    # è®­ç»ƒå›å½’æ¨¡å‹
    if enable_both_tasks:
        logger.info("\n" + "="*80)
        logger.info("è®­ç»ƒå›å½’ä»»åŠ¡")
        logger.info("="*80)
        
        y_reg = df['label_reg'].copy()
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        reg_models = {}
        
        # LightGBM
        logger.info("\nè®­ç»ƒ LightGBM å›å½’å™¨...")
        reg_lgb = trainer.train_regression_model(
            X, y_reg,
            model_type='lightgbm',
            dates=dates_series,
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05
        )
        reg_models['lightgbm'] = reg_lgb
        
        # XGBoost
        logger.info("\nè®­ç»ƒ XGBoost å›å½’å™¨...")
        reg_xgb = trainer.train_regression_model(
            X, y_reg,
            model_type='xgboost',
            dates=dates_series,
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05
        )
        reg_models['xgboost'] = reg_xgb
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        best_reg_name = max(reg_models, key=lambda k: reg_models[k]['metrics']['val_r2'])
        best_reg = reg_models[best_reg_name]
        
        logger.info(f"\næœ€ä¼˜å›å½’æ¨¡å‹: {best_reg_name} (RÂ²={best_reg['metrics']['val_r2']:.4f})")

        best_val_r2 = best_reg['metrics'].get('val_r2', float('-inf'))
        if np.isnan(best_val_r2) or best_val_r2 < MIN_REGRESSION_R2:
            raise RuntimeError(
                f"éªŒè¯RÂ² {best_val_r2:.4f} ä½äºé˜ˆå€¼ {MIN_REGRESSION_R2:.2f}, è®­ç»ƒæµç¨‹å·²ç»ˆæ­¢"
            )
        
        # ä¿å­˜æ‰€æœ‰å›å½’æ¨¡å‹
        for name, model in reg_models.items():
            is_best = (name == best_reg_name)
            filepath = os.path.join(model_save_dir, f'reg_{prediction_period}d_{name}.pkl')
            trainer.save_model(model, filepath, is_best=is_best)
        
        # é¢å¤–ä¿å­˜æœ€ä¼˜æ¨¡å‹
        best_filepath = os.path.join(model_save_dir, f'reg_{prediction_period}d_best.pkl')
        trainer.save_model(best_reg, best_filepath, is_best=True)
    
    logger.info("\n" + "="*80)
    logger.info("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")
    logger.info("="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒç»Ÿä¸€é¢„æµ‹æ¨¡å‹ V2')
    parser.add_argument('--symbols', type=str, nargs='+', help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--symbol-file', type=str, help='è‚¡ç¥¨ä»£ç æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
    parser.add_argument('--start-date', type=str, help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--prediction-period', type=int, default=PREDICTION_PERIOD_DAYS,
                        help=f'é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤{PREDICTION_PERIOD_DAYS}')
    parser.add_argument('--model-dir', type=str, default='models/v2',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--classification-only', action='store_true',
                        help='åªè®­ç»ƒåˆ†ç±»æ¨¡å‹')
    parser.add_argument('--regression-only', action='store_true',
                        help='åªè®­ç»ƒå›å½’æ¨¡å‹')
    parser.add_argument('--label-strategy', type=str, choices=['absolute', 'quantile'],
                        default=LABEL_STRATEGY,
                        help='åˆ†ç±»æ ‡ç­¾ç­–ç•¥ï¼Œabsolute æˆ– quantile')
    parser.add_argument('--label-quantile', type=float, default=LABEL_POSITIVE_QUANTILE,
                        help='quantile ç­–ç•¥ä½¿ç”¨çš„ä¸Šåˆ†ä½æ•°ï¼ˆä¾‹å¦‚ 0.7 è¡¨ç¤ºå‰30% ä¸ºæ­£ç±»ï¼‰')
    parser.add_argument('--label-min-samples', type=int, default=LABEL_MIN_SAMPLES_PER_DATE,
                        help='quantile ç­–ç•¥ä¸‹æ¯ä¸ªäº¤æ˜“æ—¥çš„æœ€å°æ ·æœ¬æ•°ï¼Œä½äºè¯¥å€¼å›é€€ absolute')
    parser.add_argument('--enable-fundamental', action='store_true',
                        help='å¯ç”¨åŸºæœ¬é¢ç‰¹å¾ï¼ˆè´¢åŠ¡æ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    if args.symbols:
        symbols = args.symbols
    elif args.symbol_file:
        with open(args.symbol_file, 'r') as f:
            symbols = [line.strip() for line in f if line.strip()]
    else:
        # é»˜è®¤ä½¿ç”¨æ²ªæ·±300æˆåˆ†è‚¡
        logger.info("æœªæŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œä½¿ç”¨æ²ªæ·±300æˆåˆ†è‚¡")
        try:
            import akshare as ak
            df_hs300 = ak.index_stock_cons(symbol="000300")
            symbols = df_hs300['å“ç§ä»£ç '].tolist()
        except Exception as e:
            logger.error(f"è·å–æ²ªæ·±300æˆåˆ†è‚¡å¤±è´¥: {e}")
            symbols = ['000001', '600000', '000002']  # Fallback
    
    # è®¾ç½®æ—¥æœŸèŒƒå›´
    end_date = args.end_date or datetime.now().strftime('%Y-%m-%d')
    start_date = args.start_date or (
        datetime.now() - timedelta(days=LOOKBACK_DAYS + args.prediction_period + 365)
    ).strftime('%Y-%m-%d')
    
    logger.info("è®­ç»ƒé…ç½®:")
    logger.info(f"  è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    logger.info(f"  æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    logger.info(f"  é¢„æµ‹å‘¨æœŸ: {args.prediction_period}å¤©")
    logger.info(f"  æ¨¡å‹ç›®å½•: {args.model_dir}")
    
    # å‡†å¤‡æ•°æ®
    df = prepare_training_data(
        symbols=symbols,
        start_date=start_date,
        end_date=end_date,
        prediction_period=args.prediction_period,
        classification_strategy=args.label_strategy,
        label_quantile=args.label_quantile,
        label_min_samples=args.label_min_samples,
        enable_fundamental=args.enable_fundamental
    )
    
    # è®­ç»ƒæ¨¡å‹
    enable_both = not (args.classification_only or args.regression_only)
    
    train_models(
        df=df,
        model_save_dir=args.model_dir,
        enable_both_tasks=enable_both,
        classification_strategy=args.label_strategy,
        prediction_period=args.prediction_period
    )
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
    logger.info("="*80)


if __name__ == '__main__':
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    main()
