# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ¨¡å‹è®­ç»ƒå…¥å£ V2
æ•´åˆæ–°ç‰¹å¾ä½“ç³»å’Œå¢å¼ºè®­ç»ƒå™¨
"""

import os
import sys
import logging
import argparse
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)

from config.prediction_config import *
from src.ml.features.unified_feature_builder import UnifiedFeatureBuilder
from src.ml.training.enhanced_trainer_v2 import EnhancedTrainerV2
from src.data.unified_data_access import UnifiedDataAccessLayer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/train_v2_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def prepare_training_data(
    symbols: List[str],
    start_date: str,
    end_date: str,
    prediction_period: int = PREDICTION_PERIOD_DAYS
) -> pd.DataFrame:
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®
    
    é‡‡ç”¨æ··åˆä»·æ ¼ç­–ç•¥ï¼š
    - ç‰¹å¾æ„å»ºï¼šä½¿ç”¨ä¸å¤æƒä»·æ ¼ï¼ˆä¿æŒæŠ€æœ¯æŒ‡æ ‡å‡†ç¡®æ€§ï¼‰
    - æ ‡ç­¾è®¡ç®—ï¼šä½¿ç”¨å‰å¤æƒä»·æ ¼ï¼ˆåæ˜ çœŸå®æŠ•èµ„æ”¶ç›Šï¼‰
    
    Parameters
    ----------
    symbols : List[str]
        è‚¡ç¥¨ä»£ç åˆ—è¡¨
    start_date : str
        å¼€å§‹æ—¥æœŸ
    end_date : str
        ç»“æŸæ—¥æœŸ
    prediction_period : int
        é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰
    
    Returns
    -------
    df : DataFrame
        ç‰¹å¾+æ ‡ç­¾æ•°æ®
    """
    logger.info("="*80)
    logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆV2å¢å¼ºç‰ˆï¼‰")
    logger.info("="*80)
    logger.info(f"è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    logger.info(f"é¢„æµ‹å‘¨æœŸ: {prediction_period}å¤©")
    logger.info(f"ä»·æ ¼ç­–ç•¥: ç‰¹å¾ç”¨ä¸å¤æƒ + æ ‡ç­¾ç”¨å‰å¤æƒ")
    
        # åˆå§‹åŒ–æ•°æ®è®¿é—®å±‚ï¼ˆä½¿ç”¨æ•°æ®åº“ï¼‰
    from src.data.unified_data_access import UnifiedDataAccessLayer, DataAccessConfig
    config = DataAccessConfig()
    config.use_cache = True  # âœ… ç¼“å­˜ç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼ˆL1 Redis + L2 Parquetéƒ½å·²éªŒè¯ï¼‰
    config.auto_sync = False  # âœ… è®­ç»ƒæ¨¡å¼ç¦ç”¨å¤–éƒ¨åŒæ­¥,ä»…ä½¿ç”¨æ•°æ®åº“æ•°æ®
    data_access = UnifiedDataAccessLayer(config=config)
    logger.info("âœ… ç¼“å­˜å·²å¯ç”¨, å¤–éƒ¨åŒæ­¥å·²ç¦ç”¨(ä»…ä½¿ç”¨æ•°æ®åº“æ•°æ®)")
    
    # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
    from src.data.db.unified_database_manager import UnifiedDatabaseManager
    db_manager = UnifiedDatabaseManager()
    
    # åˆå§‹åŒ–ç‰¹å¾æ„å»ºå™¨
    builder = UnifiedFeatureBuilder(
        data_access=data_access,
        db_manager=db_manager
    )
    
    all_data = []
    failed_symbols = []
    quality_stats = {
        'total_processed': 0,
        'data_insufficient': 0,
        'feature_build_failed': 0,
        'qfq_data_failed': 0,
        'qfq_negative_filtered': 0,
        'qfq_extreme_filtered': 0,
        'no_valid_labels': 0,
        'success': 0
    }
    
    for i, symbol in enumerate(symbols, 1):
        try:
            logger.info(f"[{i}/{len(symbols)}] å¤„ç† {symbol}")
            quality_stats['total_processed'] += 1
            
            # ========== æ­¥éª¤1: è·å–ä¸å¤æƒæ•°æ®ç”¨äºç‰¹å¾æ„å»º ==========
            stock_data = data_access.get_stock_data(
                symbol,
                start_date,
                end_date,
                adjust_mode='none'  # ä¸å¤æƒ
            )
            
            # ğŸ”§ Debug: æ£€æŸ¥è·å–çš„æ•°æ®ç»“æ„
            if stock_data is not None:
                logger.info(f"  âœ“ è·å–æ•°æ®: {len(stock_data)} rows, index.name={stock_data.index.name}, is_DatetimeIndex={isinstance(stock_data.index, pd.DatetimeIndex)}")
            
            if stock_data is None or len(stock_data) < LOOKBACK_DAYS:
                logger.warning(f"  è·³è¿‡: ä¸å¤æƒæ•°æ®ä¸è¶³ ({len(stock_data) if stock_data is not None else 0} < {LOOKBACK_DAYS})")
                failed_symbols.append((symbol, 'data_insufficient'))
                quality_stats['data_insufficient'] += 1
                continue
            
            # ========== æ­¥éª¤2: æ„å»ºç‰¹å¾ï¼ˆåŸºäºä¸å¤æƒä»·æ ¼ï¼‰ ==========
            features_df = builder.build_features_from_dataframe(stock_data, symbol)
            
            if features_df is None or len(features_df) == 0:
                logger.warning(f"  è·³è¿‡: ç‰¹å¾æ„å»ºå¤±è´¥")
                failed_symbols.append((symbol, 'feature_build_failed'))
                quality_stats['feature_build_failed'] += 1
                continue
            
            # ========== æ­¥éª¤3: è·å–å‰å¤æƒæ•°æ®ç”¨äºæ ‡ç­¾è®¡ç®— ==========
            stock_data_qfq = data_access.get_stock_data(
                symbol,
                start_date,
                end_date,
                adjust_mode='qfq'  # å‰å¤æƒ
            )
            
            # ğŸ”§ Debug: æ£€æŸ¥qfqæ•°æ®ç»“æ„
            if stock_data_qfq is not None:
                logger.info(f"  âœ“ è·å–qfqæ•°æ®: {len(stock_data_qfq)} rows, index.name={stock_data_qfq.index.name}, is_DatetimeIndex={isinstance(stock_data_qfq.index, pd.DatetimeIndex)}")
            
            if stock_data_qfq is None or len(stock_data_qfq) == 0:
                logger.warning(f"  è·³è¿‡: å‰å¤æƒæ•°æ®è·å–å¤±è´¥")
                failed_symbols.append((symbol, 'qfq_data_failed'))
                quality_stats['qfq_data_failed'] += 1
                continue
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†dateç´¢å¼•è½¬ä¸ºåˆ—ï¼ˆadd_labels_with_qfqéœ€è¦dateåˆ—ï¼‰
            if stock_data_qfq.index.name == 'date' or isinstance(stock_data_qfq.index, pd.DatetimeIndex):
                stock_data_qfq = stock_data_qfq.reset_index()
                if 'index' in stock_data_qfq.columns and 'date' not in stock_data_qfq.columns:
                    stock_data_qfq.rename(columns={'index': 'date'}, inplace=True)
                logger.debug(f"  âœ“ qfqæ•°æ®dateç´¢å¼•å·²è½¬ä¸ºåˆ—")
            
            # ğŸ”§ ä¿®å¤ï¼šå°†dateç´¢å¼•è½¬ä¸ºåˆ—ï¼ˆadd_labels_with_qfqéœ€è¦dateåˆ—ï¼‰
            if stock_data_qfq.index.name == 'date' or isinstance(stock_data_qfq.index, pd.DatetimeIndex):
                stock_data_qfq = stock_data_qfq.reset_index()
                if 'index' in stock_data_qfq.columns and 'date' not in stock_data_qfq.columns:
                    stock_data_qfq.rename(columns={'index': 'date'}, inplace=True)
            
            # ========== æ­¥éª¤4: æ•°æ®è´¨é‡è¿‡æ»¤ ==========
            # æ£€æŸ¥å‰å¤æƒä»·æ ¼æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            initial_qfq_count = len(stock_data_qfq)
            
            # è¿‡æ»¤è´Ÿæ•°ä»·æ ¼ï¼ˆæ•°æ®é”™è¯¯ï¼‰
            qfq_negative_mask = (stock_data_qfq['close'] < 0)
            if qfq_negative_mask.sum() > 0:
                logger.warning(f"  å‘ç° {qfq_negative_mask.sum()} æ¡è´Ÿæ•°å‰å¤æƒä»·æ ¼")
                quality_stats['qfq_negative_filtered'] += qfq_negative_mask.sum()
                stock_data_qfq = stock_data_qfq[~qfq_negative_mask]
            
            # è¿‡æ»¤æç«¯å€¼ï¼ˆä»·æ ¼ > 10000 æˆ– < 0.01ï¼Œå¯èƒ½æ˜¯æ•°æ®é”™è¯¯ï¼‰
            qfq_extreme_mask = (stock_data_qfq['close'] > 10000) | (stock_data_qfq['close'] < 0.01)
            if qfq_extreme_mask.sum() > 0:
                logger.warning(f"  å‘ç° {qfq_extreme_mask.sum()} æ¡æç«¯å‰å¤æƒä»·æ ¼")
                quality_stats['qfq_extreme_filtered'] += qfq_extreme_mask.sum()
                stock_data_qfq = stock_data_qfq[~qfq_extreme_mask]
            
            # å¦‚æœè¿‡æ»¤åæ•°æ®å¤ªå°‘ï¼Œè·³è¿‡
            filtered_count = initial_qfq_count - len(stock_data_qfq)
            if filtered_count > initial_qfq_count * 0.3:  # è¶…è¿‡30%è¢«è¿‡æ»¤
                logger.warning(f"  è·³è¿‡: å‰å¤æƒæ•°æ®è´¨é‡å·® (è¿‡æ»¤{filtered_count}/{initial_qfq_count})")
                failed_symbols.append((symbol, 'qfq_quality_poor'))
                continue
            
            if len(stock_data_qfq) < LOOKBACK_DAYS:
                logger.warning(f"  è·³è¿‡: å‰å¤æƒæ•°æ®ä¸è¶³ (è¿‡æ»¤å{len(stock_data_qfq)} < {LOOKBACK_DAYS})")
                failed_symbols.append((symbol, 'qfq_insufficient'))
                continue
            
            # ========== æ­¥éª¤5: æ·»åŠ æ ‡ç­¾ï¼ˆåŸºäºå‰å¤æƒä»·æ ¼ï¼‰ ==========
            features_df = add_labels_with_qfq(features_df, stock_data_qfq, prediction_period)
            
            # åˆ é™¤ç¼ºå¤±æ ‡ç­¾çš„è¡Œ
            features_df = features_df.dropna(subset=['label_cls', 'label_reg'])
            
            if len(features_df) == 0:
                logger.warning(f"  è·³è¿‡: æ— æœ‰æ•ˆæ ‡ç­¾")
                failed_symbols.append((symbol, 'no_valid_labels'))
                quality_stats['no_valid_labels'] += 1
                continue
            
            # ========== æ­¥éª¤6: æ•°æ®è´¨é‡æœ€ç»ˆæ£€æŸ¥ ==========
            # è¿‡æ»¤å¼‚å¸¸æ”¶ç›Šç‡ï¼ˆç»å¯¹å€¼ > 100%ï¼Œå¯èƒ½æ˜¯æ•°æ®é”™è¯¯ï¼‰
            extreme_return_mask = features_df['label_reg'].abs() > 1.0
            if extreme_return_mask.sum() > 0:
                logger.warning(f"  è¿‡æ»¤ {extreme_return_mask.sum()} æ¡æç«¯æ”¶ç›Šç‡è®°å½•")
                features_df = features_df[~extreme_return_mask]
            
            if len(features_df) == 0:
                logger.warning(f"  è·³è¿‡: è¿‡æ»¤åæ— æ•°æ®")
                failed_symbols.append((symbol, 'all_filtered'))
                continue
            
            # æ·»åŠ symbolåˆ—
            features_df['symbol'] = symbol
            
            all_data.append(features_df)
            quality_stats['success'] += 1
            logger.info(f"  âœ“ æˆåŠŸ: {len(features_df)} æ¡è®°å½• (æ­£æ ·æœ¬ç‡: {features_df['label_cls'].mean():.1%})")
            
        except Exception as e:
            logger.error(f"  å¤„ç†å¤±è´¥: {e}", exc_info=True)
            failed_symbols.append((symbol, 'exception'))
    
    # åˆå¹¶æ•°æ®
    if len(all_data) == 0:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œæ‰€æœ‰è‚¡ç¥¨å¤„ç†å¤±è´¥")
    
    df = pd.concat(all_data, ignore_index=True)
    
    # ========== è¾“å‡ºæ•°æ®è´¨é‡æŠ¥å‘Š ==========
    logger.info("\n" + "="*80)
    logger.info("æ•°æ®å‡†å¤‡å®Œæˆ - è´¨é‡æŠ¥å‘Š")
    logger.info("="*80)
    logger.info(f"âœ… æˆåŠŸè‚¡ç¥¨: {quality_stats['success']}/{quality_stats['total_processed']} ({quality_stats['success']/quality_stats['total_processed']*100:.1f}%)")
    logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df):,}")
    logger.info(f"ğŸ“… æ—¥æœŸèŒƒå›´: {df['date'].min()} ~ {df['date'].max()}")
    logger.info(f"ğŸ“ˆ æ­£æ ·æœ¬ç‡: {df['label_cls'].mean():.2%}")
    logger.info(f"ğŸ“‰ å¹³å‡æ”¶ç›Š: {df['label_reg'].mean():.4f}")
    logger.info(f"ğŸ“Š æ”¶ç›Šæ ‡å‡†å·®: {df['label_reg'].std():.4f}")
    
    logger.info("\nå¤±è´¥ç»Ÿè®¡:")
    logger.info(f"  æ•°æ®ä¸è¶³: {quality_stats['data_insufficient']}")
    logger.info(f"  ç‰¹å¾æ„å»ºå¤±è´¥: {quality_stats['feature_build_failed']}")
    logger.info(f"  å‰å¤æƒæ•°æ®å¤±è´¥: {quality_stats['qfq_data_failed']}")
    logger.info(f"  æ— æœ‰æ•ˆæ ‡ç­¾: {quality_stats['no_valid_labels']}")
    
    logger.info("\næ•°æ®æ¸…æ´—ç»Ÿè®¡:")
    logger.info(f"  å‰å¤æƒè´Ÿæ•°è¿‡æ»¤: {quality_stats['qfq_negative_filtered']} æ¡")
    logger.info(f"  å‰å¤æƒæç«¯å€¼è¿‡æ»¤: {quality_stats['qfq_extreme_filtered']} æ¡")
    
    if failed_symbols:
        logger.info(f"\nå¤±è´¥è‚¡ç¥¨è¯¦æƒ… (å…±{len(failed_symbols)}åª):")
        failure_reasons = {}
        for symbol, reason in failed_symbols:
            failure_reasons[reason] = failure_reasons.get(reason, []) + [symbol]
        for reason, symbols in failure_reasons.items():
            logger.info(f"  {reason}: {len(symbols)} åª - {symbols[:5]}{'...' if len(symbols) > 5 else ''}")
    
    return df


def add_labels(df: pd.DataFrame, prediction_period: int) -> pd.DataFrame:
    """
    æ·»åŠ åˆ†ç±»å’Œå›å½’æ ‡ç­¾ï¼ˆä½¿ç”¨ä¸å¤æƒä»·æ ¼ï¼Œå·²å¼ƒç”¨ï¼‰
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢« add_labels_with_qfq æ›¿ä»£
    ä¿ç•™ä»…ç”¨äºå‘åå…¼å®¹
    
    Parameters
    ----------
    df : DataFrame
        ç‰¹å¾æ•°æ®
    prediction_period : int
        é¢„æµ‹å‘¨æœŸ
    
    Returns
    -------
    df : DataFrame
        æ·»åŠ æ ‡ç­¾åçš„æ•°æ®
    """
    logger.warning("ä½¿ç”¨äº†å·²å¼ƒç”¨çš„ add_labels å‡½æ•°ï¼Œè¯·æ”¹ç”¨ add_labels_with_qfq")
    
    # è®¡ç®—æœªæ¥æ”¶ç›Š
    df['future_return'] = df['close'].shift(-prediction_period) / df['close'] - 1
    
    # åˆ†ç±»æ ‡ç­¾: æ”¶ç›Š > CLS_THRESHOLD
    df['label_cls'] = (df['future_return'] > CLS_THRESHOLD).astype(int)
    
    # å›å½’æ ‡ç­¾: æ”¶ç›Šç‡
    df['label_reg'] = df['future_return']
    
    return df


def add_labels_with_qfq(
    features_df: pd.DataFrame,
    stock_data_qfq: pd.DataFrame,
    prediction_period: int
) -> pd.DataFrame:
    """
    æ·»åŠ åˆ†ç±»å’Œå›å½’æ ‡ç­¾ï¼ˆä½¿ç”¨å‰å¤æƒä»·æ ¼ï¼‰
    
    å°†å‰å¤æƒä»·æ ¼æ•°æ®ä¸ç‰¹å¾æ•°æ®æŒ‰æ—¥æœŸå¯¹é½ï¼Œè®¡ç®—çœŸå®æŠ•èµ„æ”¶ç›Šç‡æ ‡ç­¾
    
    Parameters
    ----------
    features_df : DataFrame
        ç‰¹å¾æ•°æ®ï¼ˆåŸºäºä¸å¤æƒä»·æ ¼æ„å»ºï¼‰
    stock_data_qfq : DataFrame
        å‰å¤æƒä»·æ ¼æ•°æ®
    prediction_period : int
        é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰
    
    Returns
    -------
    df : DataFrame
        æ·»åŠ æ ‡ç­¾åçš„æ•°æ®
    """
    # ç¡®ä¿ä¸¤ä¸ªDataFrameéƒ½æœ‰dateåˆ—ä¸”ä¸ºdatetimeç±»å‹
    if 'date' not in features_df.columns:
        raise ValueError("features_df ç¼ºå°‘ 'date' åˆ—")
    if 'date' not in stock_data_qfq.columns:
        raise ValueError("stock_data_qfq ç¼ºå°‘ 'date' åˆ—")
    
    features_df = features_df.copy()
    stock_data_qfq = stock_data_qfq.copy()
    
    # ç¡®ä¿dateåˆ—ä¸ºdatetimeç±»å‹
    features_df['date'] = pd.to_datetime(features_df['date'])
    stock_data_qfq['date'] = pd.to_datetime(stock_data_qfq['date'])
    
    # æŒ‰æ—¥æœŸæ’åºå¹¶é‡ç½®ç´¢å¼•
    features_df = features_df.sort_values('date').reset_index(drop=True)
    stock_data_qfq = stock_data_qfq.sort_values('date').reset_index(drop=True)
    
    # æ–¹æ³•1: ç®€å•shiftæ–¹æ³•ï¼ˆå¦‚æœæ•°æ®å®Œå…¨å¯¹é½ï¼‰
    # å…ˆå°è¯•é€šè¿‡æ—¥æœŸmergeå¯¹é½
    # æ³¨æ„: stock_data_qfqå¯èƒ½åŒ…å«close_qfqåˆ—(ä»UnifiedDataAccessLayer)æˆ–åªæœ‰closeåˆ—(æµ‹è¯•mockæ•°æ®)
    close_col = 'close_qfq' if 'close_qfq' in stock_data_qfq.columns else 'close'
    
    qfq_for_merge = stock_data_qfq[['date', close_col]].copy()
    if close_col == 'close':
        qfq_for_merge.rename(columns={'close': 'close_qfq'}, inplace=True)
    
    # åˆ é™¤features_dfä¸­å¯èƒ½å­˜åœ¨çš„å¤æƒåˆ—ï¼Œé¿å…mergeå†²çª
    cols_to_drop = [c for c in features_df.columns if '_qfq' in c or '_hfq' in c]
    if cols_to_drop:
        features_df = features_df.drop(columns=cols_to_drop)
        logger.debug(f"  ä»features_dfåˆ é™¤å¤æƒåˆ—: {cols_to_drop}")
    
    merged = features_df.merge(qfq_for_merge, on='date', how='left')
    
    # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    merged['future_close_qfq'] = merged['close_qfq'].shift(-prediction_period)
    merged['future_return'] = (merged['future_close_qfq'] - merged['close_qfq']) / merged['close_qfq']
    
    # åˆ†ç±»æ ‡ç­¾: æ”¶ç›Š > CLS_THRESHOLD
    merged['label_cls'] = (merged['future_return'] > CLS_THRESHOLD).astype(float)
    
    # å›å½’æ ‡ç­¾: æ”¶ç›Šç‡
    merged['label_reg'] = merged['future_return']
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    result = merged.drop(columns=['close_qfq', 'future_close_qfq', 'future_return'])
    
    return result


def train_models(
    df: pd.DataFrame,
    model_save_dir: str = 'models/v2',
    enable_both_tasks: bool = True
):
    """
    è®­ç»ƒæ¨¡å‹
    
    Parameters
    ----------
    df : DataFrame
        è®­ç»ƒæ•°æ®
    model_save_dir : str
        æ¨¡å‹ä¿å­˜ç›®å½•
    enable_both_tasks : bool
        æ˜¯å¦è®­ç»ƒåˆ†ç±»å’Œå›å½’ä¸¤ä¸ªä»»åŠ¡
    """
    logger.info("="*80)
    logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹")
    logger.info("="*80)
    
    # è¯†åˆ«å®é™…å­˜åœ¨çš„ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ‡ç­¾å’Œå…ƒæ•°æ®ï¼‰
    excluded_cols = {'date', 'symbol', 'label_cls', 'label_reg', 
                     'open', 'high', 'low', 'close', 'volume', 'amount', 'source',
                     'open_qfq', 'high_qfq', 'low_qfq', 'close_qfq',
                     'open_hfq', 'high_hfq', 'low_hfq', 'close_hfq'}
    
    # è‡ªåŠ¨æ£€æµ‹æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾
    numerical_features = []
    categorical_features = []
    
    for col in df.columns:
        if col in excluded_cols:
            continue
        if df[col].dtype == 'object' or df[col].dtype.name == 'category':
            categorical_features.append(col)
        else:
            numerical_features.append(col)
    
    logger.info(f"æ•°å€¼ç‰¹å¾: {len(numerical_features)} - {numerical_features[:10]}...")
    logger.info(f"ç±»åˆ«ç‰¹å¾: {len(categorical_features)} - {categorical_features}")
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = numerical_features + categorical_features
    X = df[feature_cols].copy()
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = EnhancedTrainerV2(
        numerical_features=numerical_features,
        categorical_features=categorical_features
    )
    
    # è®­ç»ƒåˆ†ç±»æ¨¡å‹
    if enable_both_tasks:
        logger.info("\n" + "="*80)
        logger.info("è®­ç»ƒåˆ†ç±»ä»»åŠ¡")
        logger.info("="*80)
        
        y_cls = df['label_cls'].copy()
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        cls_models = {}
        
        # LightGBM
        logger.info("\nè®­ç»ƒ LightGBM åˆ†ç±»å™¨...")
        cls_lgb = trainer.train_classification_model(
            X, y_cls,
            model_type='lightgbm',
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05
        )
        cls_models['lightgbm'] = cls_lgb
        
        # XGBoost
        logger.info("\nè®­ç»ƒ XGBoost åˆ†ç±»å™¨...")
        cls_xgb = trainer.train_classification_model(
            X, y_cls,
            model_type='xgboost',
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05
        )
        cls_models['xgboost'] = cls_xgb
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        best_cls_name = max(cls_models, key=lambda k: cls_models[k]['metrics']['val_auc'])
        best_cls = cls_models[best_cls_name]
        
        logger.info(f"\næœ€ä¼˜åˆ†ç±»æ¨¡å‹: {best_cls_name} (AUC={best_cls['metrics']['val_auc']:.4f})")
        
        # ä¿å­˜æ‰€æœ‰åˆ†ç±»æ¨¡å‹
        for name, model in cls_models.items():
            is_best = (name == best_cls_name)
            filepath = os.path.join(model_save_dir, f'cls_{PREDICTION_PERIOD_DAYS}d_{name}.pkl')
            trainer.save_model(model, filepath, is_best=is_best)
        
        # é¢å¤–ä¿å­˜æœ€ä¼˜æ¨¡å‹
        best_filepath = os.path.join(model_save_dir, f'cls_{PREDICTION_PERIOD_DAYS}d_best.pkl')
        trainer.save_model(best_cls, best_filepath, is_best=True)
    
    # è®­ç»ƒå›å½’æ¨¡å‹
    if enable_both_tasks:
        logger.info("\n" + "="*80)
        logger.info("è®­ç»ƒå›å½’ä»»åŠ¡")
        logger.info("="*80)
        
        y_reg = df['label_reg'].copy()
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        reg_models = {}
        
        # LightGBM
        logger.info("\nè®­ç»ƒ LightGBM å›å½’å™¨...")
        reg_lgb = trainer.train_regression_model(
            X, y_reg,
            model_type='lightgbm',
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05
        )
        reg_models['lightgbm'] = reg_lgb
        
        # XGBoost
        logger.info("\nè®­ç»ƒ XGBoost å›å½’å™¨...")
        reg_xgb = trainer.train_regression_model(
            X, y_reg,
            model_type='xgboost',
            n_estimators=150,
            max_depth=6,
            learning_rate=0.05
        )
        reg_models['xgboost'] = reg_xgb
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        best_reg_name = max(reg_models, key=lambda k: reg_models[k]['metrics']['val_r2'])
        best_reg = reg_models[best_reg_name]
        
        logger.info(f"\næœ€ä¼˜å›å½’æ¨¡å‹: {best_reg_name} (RÂ²={best_reg['metrics']['val_r2']:.4f})")
        
        # ä¿å­˜æ‰€æœ‰å›å½’æ¨¡å‹
        for name, model in reg_models.items():
            is_best = (name == best_reg_name)
            filepath = os.path.join(model_save_dir, f'reg_{PREDICTION_PERIOD_DAYS}d_{name}.pkl')
            trainer.save_model(model, filepath, is_best=is_best)
        
        # é¢å¤–ä¿å­˜æœ€ä¼˜æ¨¡å‹
        best_filepath = os.path.join(model_save_dir, f'reg_{PREDICTION_PERIOD_DAYS}d_best.pkl')
        trainer.save_model(best_reg, best_filepath, is_best=True)
    
    logger.info("\n" + "="*80)
    logger.info("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")
    logger.info("="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒç»Ÿä¸€é¢„æµ‹æ¨¡å‹ V2')
    parser.add_argument('--symbols', type=str, nargs='+', help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--symbol-file', type=str, help='è‚¡ç¥¨ä»£ç æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
    parser.add_argument('--start-date', type=str, help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--prediction-period', type=int, default=PREDICTION_PERIOD_DAYS,
                        help=f'é¢„æµ‹å‘¨æœŸï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤{PREDICTION_PERIOD_DAYS}')
    parser.add_argument('--model-dir', type=str, default='models/v2',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--classification-only', action='store_true',
                        help='åªè®­ç»ƒåˆ†ç±»æ¨¡å‹')
    parser.add_argument('--regression-only', action='store_true',
                        help='åªè®­ç»ƒå›å½’æ¨¡å‹')
    
    args = parser.parse_args()
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    if args.symbols:
        symbols = args.symbols
    elif args.symbol_file:
        with open(args.symbol_file, 'r') as f:
            symbols = [line.strip() for line in f if line.strip()]
    else:
        # é»˜è®¤ä½¿ç”¨æ²ªæ·±300æˆåˆ†è‚¡
        logger.info("æœªæŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œä½¿ç”¨æ²ªæ·±300æˆåˆ†è‚¡")
        try:
            import akshare as ak
            df_hs300 = ak.index_stock_cons(symbol="000300")
            symbols = df_hs300['å“ç§ä»£ç '].tolist()
        except Exception as e:
            logger.error(f"è·å–æ²ªæ·±300æˆåˆ†è‚¡å¤±è´¥: {e}")
            symbols = ['000001', '600000', '000002']  # Fallback
    
    # è®¾ç½®æ—¥æœŸèŒƒå›´
    end_date = args.end_date or datetime.now().strftime('%Y-%m-%d')
    start_date = args.start_date or (
        datetime.now() - timedelta(days=LOOKBACK_DAYS + args.prediction_period + 365)
    ).strftime('%Y-%m-%d')
    
    logger.info("è®­ç»ƒé…ç½®:")
    logger.info(f"  è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    logger.info(f"  æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    logger.info(f"  é¢„æµ‹å‘¨æœŸ: {args.prediction_period}å¤©")
    logger.info(f"  æ¨¡å‹ç›®å½•: {args.model_dir}")
    
    # å‡†å¤‡æ•°æ®
    df = prepare_training_data(
        symbols=symbols,
        start_date=start_date,
        end_date=end_date,
        prediction_period=args.prediction_period
    )
    
    # è®­ç»ƒæ¨¡å‹
    enable_both = not (args.classification_only or args.regression_only)
    
    train_models(
        df=df,
        model_save_dir=args.model_dir,
        enable_both_tasks=enable_both
    )
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
    logger.info("="*80)


if __name__ == '__main__':
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    main()
